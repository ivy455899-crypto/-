{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivy455899-crypto/-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "YiXnfojtMtF6",
        "outputId": "61aa984c-39c1-4cc8-cf82-8e2599335dec"
      },
      "outputs": [
        {
          "ename": "ConnectionError",
          "evalue": "HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2721965297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# ÁôºÈÄÅ HTTP GET Ë´ãÊ±Ç\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Ê®°Êì¨ÁÄèË¶ΩÂô®Ê®ôÈ†≠\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# „ÄåÁ•ûËæ≤ÂöêÁôæËçâ„ÄçË≥áÊñô‰æÜÊ∫êÔºàÁØÑ‰æãÁî®‰∏ÄÂÄãÂÅáÊÉ≥‰∏≠Ëó•ËçâÁ∂≤Á´ôÔºâ\n",
        "site_name = \"Á•ûËæ≤ÂöêÁôæËçâ\"\n",
        "url = \"https://example-herb-database.com\"  # ÈÄôË£°ÊèõÊàê‰Ω†ÁöÑË≥áÊñô‰æÜÊ∫êÁ∂≤ÂùÄ\n",
        "\n",
        "# ÁôºÈÄÅ HTTP GET Ë´ãÊ±Ç\n",
        "res = requests.get(url, headers=headers)\n",
        "\n",
        "if res.status_code == 200:\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # ÊäìÂèñËó•ËçâÂêçÁ®±ËàáÂäüÊïàÔºàÂÅáË®≠Á∂≤Á´ôÁî® <div class=\"herb\"> ÂåÖÂê´Ë≥áÊñôÔºâ\n",
        "    herbs = soup.select(\".herb\")\n",
        "    herb_dict = {}\n",
        "\n",
        "    for herb in herbs:\n",
        "        name = herb.select_one(\".name\").text.strip()\n",
        "        effect = herb.select_one(\".effect\").text.strip()\n",
        "        herb_dict[name] = effect\n",
        "\n",
        "    # Ê®°Êì¨‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄ\n",
        "    symptom = input(\"Ë´ãËº∏ÂÖ•‰Ω†ÁöÑÁóáÁãÄÔºö\")\n",
        "\n",
        "    # Â∞çÁóáÊêúÂ∞ãÔºàÁ∞°ÂñÆÈóúÈçµÂ≠óÊØîÂ∞çÔºâ\n",
        "    matched_herbs = []\n",
        "    for name, effect in herb_dict.items():\n",
        "        if symptom in effect:\n",
        "            matched_herbs.append((name, effect))\n",
        "\n",
        "    if matched_herbs:\n",
        "        print(f\"üîç Ê†πÊìöÁóáÁãÄ„Äå{symptom}„ÄçÔºåÊé®Ëñ¶‰ª•‰∏ãËó•ËçâÔºö\")\n",
        "        for name, effect in matched_herbs:\n",
        "            print(f\"- {name}Ôºö{effect}\")\n",
        "    else:\n",
        "        print(\"‚ùå Ê≤íÊúâÊâæÂà∞Â∞çÊáâÁöÑËó•ËçâÔºåË´ãÂòóË©¶ÂÖ∂‰ªñÊèèËø∞\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå ÁÑ°Ê≥ïÈÄ£Á∑öÂà∞ {site_name}ÔºåÁãÄÊÖãÁ¢ºÔºö\", res.status_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMuG-7_gO5QB",
        "outputId": "8ad146ef-7bbf-47c5-8fe9-cfc84241ff13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,340 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,942 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,297 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,209 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,783 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [42.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,543 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,617 kB]\n",
            "Fetched 35.1 MB in 4s (10.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-chi-tra\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,586 kB of archives.\n",
            "After this operation, 2,382 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Fetched 1,586 kB in 1s (1,837 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-chi-tra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, pytesseract\n",
            "Successfully installed pytesseract-0.3.13 python-docx-1.2.0\n",
            "ÂÆåÊàê„ÄÇÈÄêÈ†Å DOCX Â∑≤Ëº∏Âá∫Ëá≥Ôºö /content/output\n"
          ]
        }
      ],
      "source": [
        "# ÂÆâË£ù Tesseract ËàáÁπÅ‰∏≠Ë™ûË®ÄÂåÖ + python-docx\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "# Ë∑ØÂæëË®≠ÂÆö\n",
        "zip_path = \"/content/ilovepdf_pages-to-jpg.zip\"  # ‚Üê Â∞á‰Ω†ÁöÑZIP‰∏äÂÇ≥ÂæåÔºåÁ¢∫Ë™çÊ™îÂêç\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Ëß£Â£ì\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# ËíêÈõÜ‰∏¶‰æùÈ†ÅÁ¢ºÊéíÂ∫è\n",
        "imgs = [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")]\n",
        "imgs = sorted(imgs, key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1)))\n",
        "\n",
        "# ÊåáÂÆöÁπÅ‰∏≠Ë™ûË®Ä\n",
        "lang = \"chi_tra\"\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    # ËÆÄÂúñ+ÁÅ∞Èöé\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    # OCRÔºàÂèØË¶ñÈúÄË¶ÅÂä†ÂÖ• config ÂèÉÊï∏ÂæÆË™øÔºâ\n",
        "    text = pytesseract.image_to_string(img, lang=lang).strip()\n",
        "\n",
        "    # Áî¢ÁîüÈÄêÈ†Å DOCX\n",
        "    doc = Document()\n",
        "    try:\n",
        "        doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "        doc.styles['Normal'].font.size = Pt(12)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    doc.add_heading(f\"ÈÄêÂ≠óÁ®øÔºöÁ¨¨ {page_no} È†Å\", level=1)\n",
        "    doc.add_paragraph(\"ÂÇôË®ªÔºöËôïÁêÜÁî® Colab ÈÄ£ÁµêÔºàÁî±‰ΩøÁî®ËÄÖÊèê‰æõÔºâÔºö\\n\"\n",
        "                      \"https://colab.research.google.com/drive/1Iw1gscK2W6UV5QlL__TG2UT_J9UI7F-v\")\n",
        "    doc.add_heading(\"Ëæ®Ë≠òÊñáÂ≠ó\", level=2)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "print(\"ÂÆåÊàê„ÄÇÈÄêÈ†Å DOCX Â∑≤Ëº∏Âá∫Ëá≥Ôºö\", out_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqXm6fynUzD5",
        "outputId": "ba91913a-da54-4fe9-f410-f34931147245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-chi-tra is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "‚úÖ ÂÆåÊàêÔºÅÈÄêÈ†Å DOCX Â∑≤Ëº∏Âá∫Âà∞Ôºö /content/output\n",
            "‚úÖ ‰∏ãËºâ ZIPÔºö /content/ocr_pages_docx.zip\n"
          ]
        }
      ],
      "source": [
        "# ========== ÂÆâË£ù OCR Áí∞Â¢É ==========\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "# ========== Ëß£Â£ìÁ∏Æ‰∏äÂÇ≥ÁöÑ zip ==========\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "zip_path = \"/content/ilovepdf_pages-to-jpg.zip\"  # ‚Üê ‰∏äÂÇ≥ÁöÑÊ™îÊ°àÂêçÁ®±\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# ========== OCR ÈÄêÈ†ÅËΩâ DOCX ==========\n",
        "imgs = [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")]\n",
        "imgs = sorted(imgs, key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1)))\n",
        "\n",
        "lang = \"chi_tra\"  # ‰ΩøÁî®ÁπÅÈ´î‰∏≠Êñá OCR\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    # ÈñãÂïüÂúñÁâáÔºàÁÅ∞ÈöéÂåñ‰ª•ÊèêÈ´òËæ®Ë≠òÁéáÔºâ\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    text = pytesseract.image_to_string(img, lang=lang, config=\"--psm 6\").strip()\n",
        "\n",
        "    # Âª∫Á´ã DOCX\n",
        "    doc = Document()\n",
        "    try:\n",
        "        doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "        doc.styles['Normal'].font.size = Pt(12)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    doc.add_heading(f\"ÈÄêÂ≠óÁ®øÔºöÁ¨¨ {page_no} È†Å\", level=1)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "print(\"‚úÖ ÂÆåÊàêÔºÅÈÄêÈ†Å DOCX Â∑≤Ëº∏Âá∫Âà∞Ôºö\", out_dir)\n",
        "\n",
        "# ========== ÊâìÂåÖÊàê zipÔºåÊñπ‰æø‰∏ãËºâ ==========\n",
        "zip_out = \"/content/ocr_pages_docx.zip\"\n",
        "with zipfile.ZipFile(zip_out, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir(out_dir):\n",
        "        zf.write(os.path.join(out_dir, f), arcname=f)\n",
        "\n",
        "print(\"‚úÖ ‰∏ãËºâ ZIPÔºö\", zip_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "WqhSPrEvU8KR",
        "outputId": "a2143418-b5ac-4834-e274-cdbdce87427d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-chi-tra is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8c1c3d9-f6c6-47d4-929c-1ddf60288bf2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e8c1c3d9-f6c6-47d4-929c-1ddf60288bf2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4274063909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ‰∏äÂÇ≥‰Ω†ÁöÑ ilovepdf_pages-to-jpg.zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# ÂÆâË£ù OCR Áí∞Â¢ÉÔºàÂê´ÁπÅÈ´î‰∏≠ÊñáÔºâ\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "# ‰∏äÂÇ≥‰Ω†ÁöÑ ilovepdf_pages-to-jpg.zip\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# OCR ÈÄêÈ†ÅËΩâ DOCX\n",
        "imgs = sorted(\n",
        "    [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")],\n",
        "    key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1))\n",
        ")\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    text = pytesseract.image_to_string(img, lang=\"chi_tra\", config=\"--psm 6\").strip()\n",
        "\n",
        "    doc = Document()\n",
        "    doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "    doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "    doc.styles['Normal'].font.size = Pt(12)\n",
        "\n",
        "    doc.add_heading(f\"ÈÄêÂ≠óÁ®øÔºöÁ¨¨ {page_no} È†Å\", level=1)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "# ÊâìÂåÖÊàê zipÔºåÊñπ‰æø‰∏ãËºâ\n",
        "zip_out = \"/content/ocr_pages_docx.zip\"\n",
        "with zipfile.ZipFile(zip_out, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir(out_dir):\n",
        "        zf.write(os.path.join(out_dir, f), arcname=f)\n",
        "\n",
        "files.download(zip_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_74GDsyRNJfc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Ê®°Êì¨ÁÄèË¶ΩÂô®Ê®ôÈ†≠\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# ÂÅáÊÉ≥Ë≥áÊñô‰æÜÊ∫êÔºàÈÄôË£°ÊèõÊàê‰Ω†ÁöÑÁúüÂØ¶Á∂≤ÂùÄÔºâ\n",
        "url = \"https://example-herb-database.com\"\n",
        "\n",
        "# ÂÑ≤Â≠òË≥áÊñôÂ∫´Ê™îÊ°à\n",
        "json_file = \"shen_nong_herbs.json\"\n",
        "\n",
        "# Step 1: Áà¨Ëü≤Êî∂ÈõÜËó•ËçâË≥áÊñô‰∏¶ÂÑ≤Â≠òÊàê JSON\n",
        "def scrape_and_save():\n",
        "    print(\"üì° ÈñãÂßãÁà¨ÂèñËó•ËçâË≥áÊñô...\")\n",
        "    res = requests.get(url, headers=headers)\n",
        "    if res.status_code != 200:\n",
        "        print(\"‚ùå ÁÑ°Ê≥ïÈÄ£Á∑öÂà∞Ë≥áÊñô‰æÜÊ∫êÔºåÁãÄÊÖãÁ¢ºÔºö\", res.status_code)\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # ÂÅáË®≠ÊØèÂÄãËó•ËçâË≥áË®äÂú® <div class=\"herb\">\n",
        "    herbs = []\n",
        "    for herb in soup.select(\".herb\"):\n",
        "        name = herb.select_one(\".name\").text.strip()\n",
        "        effect = herb.select_one(\".effect\").text.strip()\n",
        "        usage = herb.select_one(\".usage\").text.strip() if herb.select_one(\".usage\") else \"ÁÑ°Ë≥áÊñô\"\n",
        "        herbs.append({\n",
        "            \"name\": name,\n",
        "            \"effect\": effect,\n",
        "            \"usage\": usage\n",
        "        })\n",
        "\n",
        "    # ÂÑ≤Â≠òÊàêÊú¨Âú∞ JSON\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(herbs, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"‚úÖ Â∑≤ÂÑ≤Â≠ò {len(herbs)} Á≠ÜËó•ËçâË≥áÊñôÂà∞ {json_file}\")\n",
        "\n",
        "# Step 2: ÂæûÊú¨Âú∞ JSON ËºâÂÖ•Ëó•ËçâË≥áÊñô\n",
        "def load_herbs():\n",
        "    if not os.path.exists(json_file):\n",
        "        print(\"‚ö†Ô∏è Êâæ‰∏çÂà∞Êú¨Âú∞Ë≥áÊñôÂ∫´ÔºåË´ãÂÖàÂü∑Ë°åÁà¨Ëü≤Êî∂ÈõÜË≥áÊñô„ÄÇ\")\n",
        "        return []\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Step 3: ‰∫íÂãïÂºèÊúõËÅûÂïèÂàáÂïèÁ≠îÁ≥ªÁµ±\n",
        "def diagnosis_system(herbs):\n",
        "    print(\"üåø Ê≠°Ëøé‰æÜÂà∞Á•ûËæ≤ÂöêÁôæËçâË®∫Êñ∑Á≥ªÁµ± üåø\")\n",
        "    print(\"Ë´ãÂõûÁ≠î‰ª•‰∏ãÂïèÈ°åÔºàÂèØËº∏ÂÖ•Á∞°Áü≠ÊèèËø∞ÔºâÔºö\")\n",
        "\n",
        "    # ÂõõË®∫\n",
        "    symptom = input(\"1Ô∏è‚É£ Ë´ãÊèèËø∞‰∏ªË¶ÅÁóáÁãÄÔºö\")\n",
        "    tongue = input(\"2Ô∏è‚É£ ËàåËãîÈ°èËâ≤/ÁãÄÊÖãÔºàÂ¶ÇÁôΩ„ÄÅÈªÉ„ÄÅÂéö„ÄÅËñÑÔºâÔºö\")\n",
        "    pulse = input(\"3Ô∏è‚É£ ËÑàË±°ÔºàÂ¶ÇÂø´„ÄÅÊÖ¢„ÄÅËôõ„ÄÅÂØ¶ÔºâÔºö\")\n",
        "    extra = input(\"4Ô∏è‚É£ ÂÖ∂‰ªñÁãÄÊ≥ÅÔºàÂèØË∑≥ÈÅéÔºâÔºö\")\n",
        "\n",
        "    # Á∞°ÂñÆÊØîÂ∞çÈóúÈçµÂ≠ó\n",
        "    matched = []\n",
        "    for herb in herbs:\n",
        "        text = herb[\"effect\"] + herb[\"usage\"]\n",
        "        if any(keyword in text for keyword in [symptom, tongue, pulse, extra]):\n",
        "            matched.append(herb)\n",
        "\n",
        "    # È°ØÁ§∫ÁµêÊûú\n",
        "    if matched:\n",
        "        print(\"\\nüîç Ê†πÊìö‰Ω†ÁöÑÊèèËø∞ÔºåÂª∫Ë≠∞‰ª•‰∏ãËó•ËçâÔºö\")\n",
        "        for herb in matched:\n",
        "            print(f\"- {herb['name']}Ôºö{herb['effect']}ÔºàÁî®Ê≥ïÔºö{herb['usage']}Ôºâ\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Ê≤íÊúâÊâæÂà∞Á¨¶ÂêàÊ¢ù‰ª∂ÁöÑËó•ËçâÔºåÂª∫Ë≠∞Êõ¥ÊèõÊèèËø∞ÊàñÂ¢ûÂä†Á¥∞ÁØÄ„ÄÇ\")\n",
        "\n",
        "# Step 4: ‰∏ªÁ®ãÂºèÊµÅÁ®ã\n",
        "if __name__ == \"__main__\":\n",
        "    # Â¶ÇÊûúË≥áÊñôÂ∫´‰∏çÂ≠òÂú®ÔºåÂÖàÁà¨Ëü≤ÊäìË≥áÊñôÔºàÁ§∫ÁØÑÂÅáË≥áÊñôÔºâ\n",
        "    if not os.path.exists(json_file):\n",
        "        print(\"‚ö†Ô∏è Êâæ‰∏çÂà∞Êú¨Âú∞Ë≥áÊñôÂ∫´ÔºåÂ∞á‰ΩøÁî®Á§∫ÁØÑË≥áÊñô...\")\n",
        "        sample_data = [\n",
        "            {\"name\": \"ÁîòËçâ\", \"effect\": \"Ê∏ÖÁÜ±Ëß£ÊØí„ÄÅË™øÂíåË´∏Ëó•\", \"usage\": \"ÁÖéÊπØÊúçÁî® 5-10 ÂÖã\"},\n",
        "            {\"name\": \"ÈªÉËä©\", \"effect\": \"Ê∏ÖÁÜ±Áá•Êøï„ÄÅÁÄâÁÅ´Ëß£ÊØí\", \"usage\": \"ÁÖéÊπØÊúçÁî® 3-9 ÂÖã\"},\n",
        "            {\"name\": \"Áï∂Ê≠∏\", \"effect\": \"Ë£úË°ÄÊ¥ªË°Ä„ÄÅË™øÁ∂ìÊ≠¢Áóõ\", \"usage\": \"ÁÖéÊπØÊúçÁî® 5-15 ÂÖã\"},\n",
        "            {\"name\": \"ËñÑËç∑\", \"effect\": \"ÁñèÈ¢®Êï£ÁÜ±„ÄÅÊ∏ÖÂà©ÂíΩÂñâ\", \"usage\": \"ÁÖéÊπØÊúçÁî® 3-6 ÂÖã\"}\n",
        "        ]\n",
        "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # ËºâÂÖ•Ë≥áÊñôÂ∫´\n",
        "    herbs = load_herbs()\n",
        "\n",
        "    # ÂïüÂãïË®∫Êñ∑Á≥ªÁµ±\n",
        "    diagnosis_system(herbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rNVtYEfNZQr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def fetch_herb(name):\n",
        "    url = \"https://apis.tianapi.com/zhongyao/index\"\n",
        "    res = requests.get(url, params={\"key\": \"‰Ω†ÁöÑAPIKEY\", \"word\": name})\n",
        "    return res.json()  # ÂèØËß£ÊûêÂäüÊïà„ÄÅË≥áÊñôÁ≠â\n",
        "\n",
        "# Á§∫‰æãÔºöÁî®‚ÄúÈáëÈäÄËä±‚Äù„ÄÅËÆäÊàê‰∏ÄÁ≠Ü herb dictÔºåÂ≠ò JSON‚Ä¶ ÂèØÊâπÈáèËº∏ÂÖ•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fg17Fn5Nf3q"
      },
      "outputs": [],
      "source": [
        "# ÂèÉËÄÉ zhongyaocai.com Áà¨Ëü≤Â∞àÊ°àÈÇèËºØÔºàÁ∞°ÂñÆÂàÜÈ°û + ÂàÜÈ†ÅÔºâ\n",
        "# ÂÜçÂØ´Áà¨Ëü≤ÊääË≥áÊñôÊäì‰∏ã‰æÜÔºåÊï¥ÁêÜÊàê dict listÔºåÂ≠òÊàê JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oZBneRmHNi6i"
      },
      "outputs": [],
      "source": [
        "with open(\"shen_nong_herbs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    herbs = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXn4wwYCNmSq"
      },
      "outputs": [],
      "source": [
        "import requests, json, os\n",
        "\n",
        "# ‰Ω†ÂèØ‰ª•ÂÖàÊ±∫ÂÆöÁî®Âì™ÂÄãË≥áÊñôÊ∫êÔºåÊàëÈÄôË£è‰ª• TianAPI ÁÇ∫‰æãÔºö\n",
        "\n",
        "API_KEY = \"‰Ω†ÁöÑAPIKEY\"\n",
        "API_URL = \"https://apis.tianapi.com/zhongyao/index\"\n",
        "\n",
        "def fetch_herb(name):\n",
        "    res = requests.get(API_URL, params={\"key\": API_KEY, \"word\": name})\n",
        "    return res.json().get(\"result\")  # ‰æãÔºöÂèØËÉΩÊúâ title, content Á≠âÊ¨Ñ‰Ωç\n",
        "\n",
        "def build_db(names_list, json_file=\"shen_nong_herbs.json\"):\n",
        "    herbs = []\n",
        "    for nm in names_list:\n",
        "        data = fetch_herb(nm)\n",
        "        if data:\n",
        "            herbs.append({\n",
        "                \"name\": data.get(\"title\", nm),\n",
        "                \"effect\": data.get(\"content\", \"\"),\n",
        "                \"usage\": data.get(\"usage\", \"\")\n",
        "            })\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(herbs, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"ÂÑ≤Â≠ò {len(herbs)} Á≠ÜËó•ËçâË≥áÊñôÂà∞ {json_file}\")\n",
        "\n",
        "def load_herbs(json_file=\"shen_nong_herbs.json\"):\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def diagnosis_system(herbs):\n",
        "    print(\"Á•ûËæ≤ÂöêÁôæËçâË®∫Êñ∑Á≥ªÁµ±\")\n",
        "    symptom = input(\"ÁóáÁãÄÔºö\")\n",
        "    tongue = input(\"ËàåËãîÔºö\")\n",
        "    pulse = input(\"ËÑàË±°Ôºö\")\n",
        "    extra = input(\"ÂÖ∂‰ªñÔºö\")\n",
        "    matched = [h for h in herbs if any(k in (h[\"effect\"]+h.get(\"usage\",\"\")) for k in [symptom, tongue, pulse, extra])]\n",
        "    if matched:\n",
        "        for h in matched:\n",
        "            print(f\"- {h['name']}: {h['effect']} Áî®Ê≥ïÔºö{h.get('usage','')}\")\n",
        "    else:\n",
        "        print(\"Ê≤íÊâæÂà∞ÈÅ©ÂêàÁöÑËó•Ëçâ\")\n",
        "\n",
        "# ÁØÑ‰æãÂïüÂãïÈ†ÜÂ∫è\n",
        "if __name__ == \"__main__\":\n",
        "    names = [\"ÈáëÈäÄËä±\", \"ÈªÉËä©\", \"ÁîòËçâ\"]  # ÂÖàË©¶Ë©¶Áúã\n",
        "    if not os.path.exists(\"shen_nong_herbs.json\"):\n",
        "        build_db(names)\n",
        "    herbs = load_herbs()\n",
        "    diagnosis_system(herbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImR1mqSeNv_i"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"ÁîòËçâ\",\n",
        "  \"effect\": \"Ê∏ÖÁÜ±Ëß£ÊØí„ÄÅË™øÂíåË´∏Ëó•\",\n",
        "  \"usage\": \"ÁÖéÊπØÊúçÁî® 5-10 ÂÖã\",\n",
        "  \"source\": \"TianAPI\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RBTkeToNzAh"
      },
      "outputs": [],
      "source": [
        "- ÁîòËçâÔºà‰æÜÊ∫êÔºöTianAPIÔºâÔºöÊ∏ÖÁÜ±Ëß£ÊØí„ÄÅË™øÂíåË´∏Ëó•ÔºåÁî®Ê≥ïÔºöÁÖéÊπØÊúçÁî® 5-10 ÂÖã\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUk9EZtN9kw"
      },
      "outputs": [],
      "source": [
        "fetch_from_tianapi(name)\n",
        "fetch_from_thinkapi(name)\n",
        "fetch_from_local_spider(name)\n",
        "fetch_from_hkbu(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UwyL0I0oODv4"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Á•ûËæ≤ÂöêÁôæËçâ ¬∑ Â§ö‰æÜÊ∫êÂêà‰ΩµÁâà\n",
        "ÂèØÁõ¥Êé•Âú® Colab / PyCharm Âü∑Ë°å\n",
        "\n",
        "ÂäüËÉΩÁ∏ΩË¶ΩÔºö\n",
        "1) Â§ö‰æÜÊ∫êÊî∂ÈõÜÔºöTianAPI / ThinkAPI / Êú¨Âú∞Áà¨Ëü≤ / HKBU ÂúñÂ∫´ÔºàÊèê‰æõÊì¥ÂÖÖÂáΩÂºè‰ªãÈù¢Ôºâ\n",
        "2) Êú¨Âú∞Ë≥áÊñôÂ∫´Ôºöshen_nong_herbs.jsonÔºàËá™ÂãïÂéªÈáçÂêà‰Ωµ„ÄÅÂê´‰æÜÊ∫êÊ®ôË®òËàáÂúñÁâáÔºâ\n",
        "3) ÊúõËÅûÂïèÂàá‰∫íÂãïÔºöÁóáÁãÄ/ËàåËãî/ËÑàË±°/ÂÖ∂‰ªñ ‚Üí ÈóúÈçµË©ûÊØîÂ∞ç + ÂêåÁæ©Ë©ûÊì¥ÂÖÖ\n",
        "4) Á•ûËæ≤Ê∞èËó•ÊàøÔºöÁ∞°ÂñÆË¶èÂâáÂºïÊìé ‚Üí Êé®Ëñ¶Ëó•Ëçâ + Á§∫ÁØÑÈÖçÊñπÔºàÂèØËá™Ë°åÊì¥ÂÖÖÔºâ\n",
        "\n",
        "‰æùË≥¥Ôºö\n",
        "- requests, beautifulsoup4ÔºàÂè™ÊúâÂú®ÂïüÁî® API/Áà¨Ëü≤ÊôÇÈúÄË¶ÅÔºâ\n",
        "- ÁÑ°Â§ñÁ∂≤ÊôÇÊúÉËá™Âãï‰ΩøÁî®ÂÖßÂª∫Á§∫ÁØÑÊ®£Êú¨\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ========== Âü∫Êú¨Ë®≠ÂÆö ==========\n",
        "\n",
        "DB_FILE = \"shen_nong_herbs.json\"\n",
        "\n",
        "CONFIG = {\n",
        "    \"enable_sources\": {\n",
        "        \"tianapi\": False,   # Êúâ API Key ÂÜçÊâìÈñã\n",
        "        \"thinkapi\": False,  # Êúâ API Key ÂÜçÊâìÈñã\n",
        "        \"local_spider\": False,  # Â∞äÈáçÂ∞çÊñπ robots.txtÔºåÁ¢∫ÂÆöÂÖÅË®±ÂÜçÊâìÈñã\n",
        "        \"hkbu_images\": False    # ÈúÄ‰æùÊéàÊ¨ä/‰ΩøÁî®Ê¢ùÊ¨æ\n",
        "    },\n",
        "    \"api_keys\": {\n",
        "        \"tianapi\": \"PUT_YOUR_TIANAPI_KEY_HERE\",\n",
        "        \"thinkapi\": \"PUT_YOUR_THINKAPI_KEY_HERE\"\n",
        "    },\n",
        "    # Êü•Ë©¢ÊØîÂ∞çÊ¨Ñ‰Ωç\n",
        "    \"match_fields\": [\"effect\", \"usage\", \"props\", \"indications\", \"functions\", \"notes\"],\n",
        "    # Á∞°ÊòìÂêåÁæ©Ë©ûÔºàÂèØËá™Ë°åÊì¥ÂÖÖÔºâ\n",
        "    \"synonyms\": {\n",
        "        \"ÂñâÂö®Áóõ\": [\"ÂíΩÁóõ\", \"ÂíΩÂñâËÖ´Áóõ\", \"ÂíΩ‰πæ\", \"ÂíΩÁô¢\"],\n",
        "        \"ÁôºÁÜ±\": [\"ÁôºÁáí\", \"Ë∫´ÁÜ±\", \"Â£Ø\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TlZDjYgWOJCY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# ÂÅáË£ùÁà¨Âà∞ÁöÑÂ§ö‰æÜÊ∫êË≥áÊñô\n",
        "data_sources = [\n",
        "    {\n",
        "        \"name\": \"‰∫∫ÂèÉ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"Ë£úÊ∞£\", \"ÂÆâÁ•û\", \"ÊäóÁñ≤Âãû\"],\n",
        "        \"source\": \"‰∏≠ÈÜ´Ëó•ÁôæÁßë\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ÈªÉËä™\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"Ë£úÊ∞£Âõ∫Ë°®\", \"Âà©Â∞øÊ∂àËÖ´\", \"‰øÉÈÄ≤ÂÖçÁñ´\"],\n",
        "        \"source\": \"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Êû∏Êùû\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"Ë£úËÇùËÖé\", \"ÁõäÁ≤æÊòéÁõÆ\"],\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ÂÑ≤Â≠òÊàêÊú¨Âú∞ JSON\n",
        "with open(\"shennong.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data_sources, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taXgn9p-ORr_"
      },
      "outputs": [],
      "source": [
        "def search_herb(keyword):\n",
        "    with open(\"shennong.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "\n",
        "    results = [h for h in herbs if keyword in h[\"name\"] or keyword in h[\"pinyin\"]]\n",
        "    return results\n",
        "\n",
        "# Ê∏¨Ë©¶Êü•Ë©¢\n",
        "keyword = \"‰∫∫ÂèÉ\"\n",
        "result = search_herb(keyword)\n",
        "for r in result:\n",
        "    print(f\"ÂêçÁ®±: {r['name']}\")\n",
        "    print(f\"ÂäüÊïà: {', '.join(r['effects'])}\")\n",
        "    print(f\"‰æÜÊ∫ê: {r['source']}\")\n",
        "    print(f\"ÂúñÁâá: {r['image']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lvjk5AyoOWRP"
      },
      "outputs": [],
      "source": [
        "def shennong_bot():\n",
        "    print(\"üåø Ê≠°Ëøé‰æÜÂà∞Á•ûËæ≤ÁôæËçâÂïèÁ≠îÁ≥ªÁµ±ÔºåËº∏ÂÖ•Ëó•ÊùêÂêçÁ®±ÊàñÊãºÈü≥Êü•Ë©¢ÔºåËº∏ÂÖ• q Èõ¢Èñã\")\n",
        "    while True:\n",
        "        q = input(\"Ë´ãËº∏ÂÖ•Ëó•ÊùêÂêçÁ®±: \").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã ÂÜçË¶ãÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "\n",
        "        results = search_herb(q)\n",
        "        if results:\n",
        "            for r in results:\n",
        "                print(f\"üå± {r['name']}Ôºö{', '.join(r['effects'])}Ôºà‰æÜÊ∫ê: {r['source']}Ôºâ\")\n",
        "        else:\n",
        "            print(\"‚ùå Êâæ‰∏çÂà∞Ë©≤Ëó•ÊùêÔºåË´ãÁ¢∫Ë™çÊãºÂØ´„ÄÇ\")\n",
        "\n",
        "# shennong_bot() # Âü∑Ë°å‰∫íÂãïÁ≥ªÁµ±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ytwxk9DOZ0_"
      },
      "outputs": [],
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"zh-Hant\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>Á•ûËæ≤ÁôæËçâÊü•Ë©¢</title>\n",
        "</head>\n",
        "<body>\n",
        "<h1>üåø Á•ûËæ≤ÁôæËçâÊü•Ë©¢Á≥ªÁµ±</h1>\n",
        "<input type=\"text\" id=\"searchInput\" placeholder=\"Ëº∏ÂÖ•Ëó•ÊùêÂêçÁ®±ÊàñÊãºÈü≥\">\n",
        "<button onclick=\"searchHerb()\">Êü•Ë©¢</button>\n",
        "<div id=\"result\"></div>\n",
        "\n",
        "<script>\n",
        "async function searchHerb() {\n",
        "    const keyword = document.getElementById(\"searchInput\").value;\n",
        "    const res = await fetch(\"shennong.json\");\n",
        "    const herbs = await res.json();\n",
        "\n",
        "    const results = herbs.filter(h => h.name.includes(keyword) || h.pinyin.includes(keyword));\n",
        "    let html = \"\";\n",
        "    if (results.length > 0) {\n",
        "        results.forEach(r => {\n",
        "            html += `<h2>${r.name}</h2><p>ÂäüÊïà: ${r.effects.join(\", \")}</p><p>‰æÜÊ∫ê: ${r.source}</p><img src=\"${r.image}\" width=\"150\">`;\n",
        "        });\n",
        "    } else {\n",
        "        html = \"<p>‚ùå Êâæ‰∏çÂà∞Ë©≤Ëó•Êùê</p>\";\n",
        "    }\n",
        "    document.getElementById(\"result\").innerHTML = html;\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD3p3zCAOf83"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"ÂêçÁ®±\": \"ËñÑËç∑\",\n",
        "  \"ÂäüÊïà\": [\"Ê∏ÖÁÜ±\", \"Ëß£ÊØí\", \"ÁñèÈ¢®\"],\n",
        "  \"‰ΩøÁî®ÈÉ®‰Ωç\": \"Ëëâ\",\n",
        "  \"Â∏∏Ë¶ãÈÖçÊñπ\": [\"ËñÑËç∑Ëå∂\", \"ËñÑËç∑Ê≤π\"],\n",
        "  \"‰æÜÊ∫ê\": \"Êú¨ËçâÁ∂±ÁõÆ\",\n",
        "  \"ÊúÄÂæåÊõ¥Êñ∞\": \"2025-08-14\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RJrkwUE5O82V"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "\n",
        "# ======================\n",
        "# Ê≠∑Âè≤ + Áèæ‰ª£Á§∫ÁØÑË≥áÊñô\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\n",
        "        \"name\": \"‰∫∫ÂèÉ\",\n",
        "        \"ancient_name\": \"‰∫∫ÂèÉ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"Ë£úÊ∞£\", \"ÂÆâÁ•û\", \"ÊäóÁñ≤Âãû\"],\n",
        "        \"usage\": \"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\n",
        "        \"source\": \"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ÈªÉËä™\",\n",
        "        \"ancient_name\": \"ÈªÉËÄÜ\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"Ë£úÊ∞£Âõ∫Ë°®\", \"Âà©Â∞øÊ∂àËÖ´\", \"‰øÉÈÄ≤ÂÖçÁñ´\"],\n",
        "        \"usage\": \"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\n",
        "        \"source\": \"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Êû∏Êùû\",\n",
        "        \"ancient_name\": \"Êû∏ÊùûÂ≠ê\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"Ë£úËÇùËÖé\", \"ÁõäÁ≤æÊòéÁõÆ\"],\n",
        "        \"usage\": \"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ======================\n",
        "# ÂÑ≤Â≠òÊú¨Âú∞ JSON Ë≥áÊñôÂ∫´\n",
        "# ======================\n",
        "DB_FILE = \"shennong_ai.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Â∑≤Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´Ôºö{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# Êü•Ë©¢ÂáΩÂºè\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in h.get(k,\"\") for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±ÔºàÊúõËÅûÂïèÂàáÊ®°Êì¨Ôºâ\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Ê≠°Ëøé‰æÜÂà∞Áèæ‰ª£ AI Á•ûËæ≤Ê∞èÁ≥ªÁµ±ÔºÅËº∏ÂÖ• q Èõ¢Èñã\")\n",
        "    while True:\n",
        "        q = input(\"Ë´ãËº∏ÂÖ•ÁóáÁãÄ„ÄÅËçâËó•ÂêçÁ®±ÊàñÊãºÈü≥Ôºö\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã ÂÜçË¶ãÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "        matches = search_herb(q)\n",
        "        if matches:\n",
        "            for m in matches:\n",
        "                print(f\"\\nüå± {m['name']}ÔºàÂè§ÂêçÔºö{m['ancient_name']}Ôºâ\")\n",
        "                print(f\"ÂäüÊïà: {', '.join(m['effects'])}\")\n",
        "                print(f\"Áî®Ê≥ï: {m['usage']}\")\n",
        "                print(f\"‰æÜÊ∫ê: {m['source']}\")\n",
        "                print(f\"ÂúñÁâá: {m['image']}\")\n",
        "        else:\n",
        "            print(\"‚ùå Êâæ‰∏çÂà∞Áõ∏ÈóúËó•ÊùêÔºåË´ãÁ¢∫Ë™çËº∏ÂÖ•ÊàñÊèõÂÄãÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãïÁ≥ªÁµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KylqzB8LPHqu"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\n",
        "        \"name\": \"‰∫∫ÂèÉ\",\n",
        "        \"ancient_name\": \"‰∫∫ÂèÉ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"Ë£úÊ∞£\", \"ÂÆâÁ•û\", \"ÊäóÁñ≤Âãû\"],\n",
        "        \"usage\": \"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\n",
        "        \"source\": \"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ÈªÉËä™\",\n",
        "        \"ancient_name\": \"ÈªÉËÄÜ\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"Ë£úÊ∞£Âõ∫Ë°®\", \"Âà©Â∞øÊ∂àËÖ´\", \"‰øÉÈÄ≤ÂÖçÁñ´\"],\n",
        "        \"usage\": \"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\n",
        "        \"source\": \"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Êû∏Êùû\",\n",
        "        \"ancient_name\": \"Êû∏ÊùûÂ≠ê\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"Ë£úËÇùËÖé\", \"ÁõäÁ≤æÊòéÁõÆ\"],\n",
        "        \"usage\": \"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ËñÑËç∑\",\n",
        "        \"ancient_name\": \"ËñÑËç∑\",\n",
        "        \"pinyin\": \"bohe\",\n",
        "        \"effects\": [\"ÁñèÈ¢®Ê∏ÖÁÜ±\", \"Âà©ÂíΩ\", \"È†≠Áóõ\"],\n",
        "        \"usage\": \"Ê≥°Ëå∂ÊàñÂÖ•Êñπ 3-6 ÂÖã\",\n",
        "        \"source\": \"Êú¨ËçâÁ∂±ÁõÆ\",\n",
        "        \"image\": \"https://example.com/bohe.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Êú¨Âú∞Ë≥áÊñôÂ∫´Â∑≤Âª∫Á´ãÔºö{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2Ô∏è‚É£ Êü•Ë©¢ÂäüËÉΩ\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3Ô∏è‚É£ AI ÈÖçÊñπÁîüÊàêÁ§∫ÁØÑ\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    \"\"\"\n",
        "    Ëº∏ÂÖ•ÁóáÁãÄÂàóË°® ‚Üí Êé®Ëñ¶ËçâËó•ÁµÑÂêà\n",
        "    Á∞°ÂñÆË¶èÂâáÔºöÁóáÁãÄËàáËçâËó•ÂäüÊïàÂåπÈÖç\n",
        "    \"\"\"\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for symptom in symptoms:\n",
        "        matched = [h for h in herbs if symptom in h[\"effects\"]]\n",
        "        if matched:\n",
        "            chosen = random.choice(matched)  # Ëã•Â§öÂÄãÂåπÈÖçÔºåÈö®Ê©üÈÅ∏‰∏ÄÂÄã\n",
        "            formula.append(chosen)\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 4Ô∏è‚É£ ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±ÔºàÊúõËÅûÂïèÂàáÊ®°Êì¨Ôºâ\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Ê≠°Ëøé‰æÜÂà∞Áï∂‰ª£ AI Á•ûËæ≤Ê∞èÁ≥ªÁµ±ÔºÅËº∏ÂÖ• q Èõ¢Èñã\")\n",
        "    print(\"‰Ω†ÂèØ‰ª•Ëº∏ÂÖ•ÁóáÁãÄ„ÄÅËçâËó•ÂêçÁ®±ÊàñÊãºÈü≥‰æÜÊü•Ë©¢ËçâËó•ÔºåÊàñËº∏ÂÖ• 'ÈÖçÊñπ' ÁîüÊàêËçâËó•ÁµÑÂêà\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nË´ãËº∏ÂÖ•Ôºö\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã ÂÜçË¶ãÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "        elif q.lower() == \"ÈÖçÊñπ\":\n",
        "            symptoms_input = input(\"Ë´ãËº∏ÂÖ•ÁóáÁãÄÔºàÈÄóËôüÂàÜÈöîÔºå‰æãÂ¶ÇÔºöË£úÊ∞£,ÂÆâÁ•ûÔºâÔºö\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nüíä Êé®Ëñ¶ËçâËó•ÁµÑÂêàÔºö\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, Áî®Ê≥ï: {f['usage']}, ‰æÜÊ∫ê: {f['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Á¨¶ÂêàÁóáÁãÄÁöÑËçâËó•\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nüå± ÊâæÂà∞ {len(matches)} Á≠ÜË≥áÊñôÔºö\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  ÂäüÊïà: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  Áî®Ê≥ï: {m['usage']}\")\n",
        "                    print(f\"  ‰æÜÊ∫ê: {m['source']}\")\n",
        "                    print(f\"  ÂúñÁâá: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Áõ∏ÈóúËó•ÊùêÔºåË´ãÁ¢∫Ë™çËº∏ÂÖ•ÊàñÊèõÂÄãÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãïÁ≥ªÁµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3QtnuZTPMx9"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"‰∫∫ÂèÉ\",\n",
        "  \"ancient_name\": \"‰∫∫ÂèÉ\",\n",
        "  \"pinyin\": \"renshen\",\n",
        "  \"effects\": [\"Ë£úÊ∞£\", \"ÂÆâÁ•û\", \"ÊäóÁñ≤Âãû\"],\n",
        "  \"usage\": \"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\n",
        "  \"source\": \"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\n",
        "  \"image\": \"https://example.com/renshen.jpg\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaQhIiK2PRdd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()  # ÊØèÂÄãËçâËó•ÁöÑÊé®Ëñ¶Ê¶ÇÁéá\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hidIa2yWPaRc"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖÁóáÁãÄ ‚Üí ÁâπÂæµÂêëÈáè ‚Üí Á•ûÁ∂ìÁ∂≤Ë∑Ø ‚Üí ËçâËó•Ê¶ÇÁéá ‚Üí ÈÖçÊñπÁîüÊàê ‚Üí È°ØÁ§∫ÂäüÊïà + ‰æÜÊ∫ê + ÂúñÁâá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jttn8aaRPf-k"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"Êû∏Êùû\",\n",
        "  \"ancient_name\": \"Êû∏ÊùûÂ≠ê\",\n",
        "  \"pinyin\": \"gouqi\",\n",
        "  \"effects\": [\"Ë£úËÇùËÖé\", \"ÁõäÁ≤æÊòéÁõÆ\"],\n",
        "  \"usage\": \"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\n",
        "  \"source\": \"TianAPI\",\n",
        "  \"image\": \"https://example.com/gouqi.jpg\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf9q0z5SPnZs"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖÁóáÁãÄ/ÁÖßÁâá ‚Üí ÂêëÈáèÂåñ ‚Üí Á•ûÁ∂ìÁ∂≤Ë∑ØÂàÜÊûê ‚Üí Êé®Ëñ¶ËçâËó• + ÈÖçÊñπ ‚Üí È°ØÁ§∫ÂäüÊïà + ÂúñÁâá + Ë≥áÊñô‰æÜÊ∫ê\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8d-Had4lPxt8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"‰∫∫ÂèÉ\",\"ancient_name\":\"‰∫∫ÂèÉ\",\"pinyin\":\"renshen\",\"effects\":[\"Ë£úÊ∞£\",\"ÂÆâÁ•û\",\"ÊäóÁñ≤Âãû\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\"source\":\"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"ÈªÉËä™\",\"ancient_name\":\"ÈªÉËÄÜ\",\"pinyin\":\"huangqi\",\"effects\":[\"Ë£úÊ∞£Âõ∫Ë°®\",\"Âà©Â∞øÊ∂àËÖ´\",\"‰øÉÈÄ≤ÂÖçÁñ´\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\"source\":\"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"Êû∏Êùû\",\"ancient_name\":\"Êû∏ÊùûÂ≠ê\",\"pinyin\":\"gouqi\",\"effects\":[\"Ë£úËÇùËÖé\",\"ÁõäÁ≤æÊòéÁõÆ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"ËñÑËç∑\",\"ancient_name\":\"ËñÑËç∑\",\"pinyin\":\"bohe\",\"effects\":[\"ÁñèÈ¢®Ê∏ÖÁÜ±\",\"Âà©ÂíΩ\",\"È†≠Áóõ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÂÖ•Êñπ 3-6 ÂÖã\",\"source\":\"Êú¨ËçâÁ∂±ÁõÆ\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Êú¨Âú∞Ë≥áÊñôÂ∫´Â∑≤Âª∫Á´ãÔºö{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2Ô∏è‚É£ Êü•Ë©¢ÂäüËÉΩ\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3Ô∏è‚É£ Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØÁ§∫ÁØÑÔºàÊé®Ëñ¶ËçâËó•Ôºâ\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# Á∞°ÂñÆÁ§∫ÁØÑÔºöËº∏ÂÖ•ÁóáÁãÄÂêëÈáè(Èï∑Â∫¶4) ‚Üí Ëº∏Âá∫ËçâËó•Ê¶ÇÁéá(Èï∑Â∫¶4)\n",
        "input_size = 4\n",
        "hidden_size = 8\n",
        "output_size = 4\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4Ô∏è‚É£ AI ÈÖçÊñπÁîüÊàêÁ§∫ÁØÑ\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    # Â∞áÁóáÁãÄÁ∞°ÂñÆÂêëÈáèÂåñÔºöÊØèÂÄãÁóáÁãÄÂ∞çÊáâ0Êàñ1\n",
        "    symptom_dict = {\"Ë£úÊ∞£\":0,\"ÂÆâÁ•û\":1,\"ÊäóÁñ≤Âãû\":2,\"Ë£úÊ∞£Âõ∫Ë°®\":3}\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()  # Ê®°ÂûãËº∏Âá∫ËçâËó•Êé®Ëñ¶Ê¶ÇÁéá\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # Á∞°ÂñÆÈñæÂÄº\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5Ô∏è‚É£ ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±ÔºàÊúõËÅûÂïèÂàáÊ®°Êì¨Ôºâ\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Ê≠°Ëøé‰æÜÂà∞Â§öÂÖÉÂåñÁï∂‰ª£ AI Á•ûËæ≤Ê∞èÁ≥ªÁµ±ÔºÅËº∏ÂÖ• q Èõ¢Èñã\")\n",
        "    print(\"Ëº∏ÂÖ•ÁóáÁãÄ„ÄÅËçâËó•ÂêçÁ®±ÊàñÊãºÈü≥Êü•Ë©¢ÔºåËº∏ÂÖ• 'ÈÖçÊñπ' ÁîüÊàêÊé®Ëñ¶ËçâËó•ÁµÑÂêà\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nË´ãËº∏ÂÖ•Ôºö\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã ÂÜçË¶ãÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "        elif q.lower() == \"ÈÖçÊñπ\":\n",
        "            symptoms_input = input(\"Ë´ãËº∏ÂÖ•ÁóáÁãÄÔºàÈÄóËôüÂàÜÈöîÔºå‰æãÂ¶ÇÔºöË£úÊ∞£,ÂÆâÁ•ûÔºâÔºö\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nüíä Êé®Ëñ¶ËçâËó•ÁµÑÂêàÔºö\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, Áî®Ê≥ï: {f['usage']}, ‰æÜÊ∫ê: {f['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Á¨¶ÂêàÁóáÁãÄÁöÑËçâËó•\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nüå± ÊâæÂà∞ {len(matches)} Á≠ÜË≥áÊñôÔºö\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  ÂäüÊïà: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  Áî®Ê≥ï: {m['usage']}\")\n",
        "                    print(f\"  ‰æÜÊ∫ê: {m['source']}\")\n",
        "                    print(f\"  ÂúñÁâá: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Áõ∏ÈóúËó•ÊùêÔºåË´ãÁ¢∫Ë™çËº∏ÂÖ•ÊàñÊèõÂÄãÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãïÁ≥ªÁµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K6RT8DoP7ST"
      },
      "outputs": [],
      "source": [
        "üåø Áï∂‰ª£Â§öÂÖÉ AI Á•ûËæ≤Ê∞èÁèæË∫´ÔºÅ\n",
        "üìú Âè§‰ªäÊô∫ÊÖßÁõ°Êî∂ÁúºÂ∫ï\n",
        "üíä ËçâËó•Êé®Ëñ¶Âç≥ÊôÇÁîüÊàê\n",
        "üß† AI ÈÖçÊñπÊô∫ËÉΩÂåπÈÖç\n",
        "üì∏ ÂúñÁâáËàá‰æÜÊ∫êÊ∏ÖÊ•öÂëàÁèæ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duL1bpcLP85b"
      },
      "source": [
        "üåø Áï∂‰ª£Â§öÂÖÉ AI Á•ûËæ≤Ê∞èÁèæË∫´ÔºÅ\n",
        "üìú Âè§‰ªäÊô∫ÊÖßÁõ°Êî∂ÁúºÂ∫ï\n",
        "üíä ËçâËó•Êé®Ëñ¶Âç≥ÊôÇÁîüÊàê\n",
        "üß† AI ÈÖçÊñπÊô∫ËÉΩÂåπÈÖç\n",
        "üì∏ ÂúñÁâáËàá‰æÜÊ∫êÊ∏ÖÊ•öÂëàÁèæ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5OClf9LOQH6z"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"‰∫∫ÂèÉ\",\"ancient_name\":\"‰∫∫ÂèÉ\",\"pinyin\":\"renshen\",\"effects\":[\"Ë£úÊ∞£\",\"ÂÆâÁ•û\",\"ÊäóÁñ≤Âãû\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\"source\":\"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"ÈªÉËä™\",\"ancient_name\":\"ÈªÉËÄÜ\",\"pinyin\":\"huangqi\",\"effects\":[\"Ë£úÊ∞£Âõ∫Ë°®\",\"Âà©Â∞øÊ∂àËÖ´\",\"‰øÉÈÄ≤ÂÖçÁñ´\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\"source\":\"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"Êû∏Êùû\",\"ancient_name\":\"Êû∏ÊùûÂ≠ê\",\"pinyin\":\"gouqi\",\"effects\":[\"Ë£úËÇùËÖé\",\"ÁõäÁ≤æÊòéÁõÆ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"ËñÑËç∑\",\"ancient_name\":\"ËñÑËç∑\",\"pinyin\":\"bohe\",\"effects\":[\"ÁñèÈ¢®Ê∏ÖÁÜ±\",\"Âà©ÂíΩ\",\"È†≠Áóõ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÂÖ•Êñπ 3-6 ÂÖã\",\"source\":\"Êú¨ËçâÁ∂±ÁõÆ\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Êú¨Âú∞Ë≥áÊñôÂ∫´Â∑≤Âª∫Á´ãÔºö{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2Ô∏è‚É£ Êü•Ë©¢ÂäüËÉΩ\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3Ô∏è‚É£ Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØÁ§∫ÁØÑÔºàËçâËó•Êé®Ëñ¶Ôºâ\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4  # Á§∫‰æãÁóáÁãÄÂêëÈáèÈï∑Â∫¶\n",
        "hidden_size = 8\n",
        "output_size = 4  # ËçâËó•Êï∏Èáè\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4Ô∏è‚É£ AI ÈÖçÊñπÁîüÊàêÁ§∫ÁØÑÔºàÂÆâÂÖ®ÁØÑÂúçÔºâ\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    symptom_dict = {\"Ë£úÊ∞£\":0,\"ÂÆâÁ•û\":1,\"ÊäóÁñ≤Âãû\":2,\"Ë£úÊ∞£Âõ∫Ë°®\":3}  # Á∞°ÂñÆÁ§∫ÁØÑ\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # Ê¶ÇÁéáÈñæÂÄº\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5Ô∏è‚É£ ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±ÔºàÊúõËÅûÂïèÂàáÊ®°Êì¨Ôºâ\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Á•ûËæ≤Ê∞èÈôçËá®ÔºÅÁï∂‰ª£Â§öÂÖÉ AI Á•ûËæ≤Ê∞èÁèæË∫´\")\n",
        "    print(\"Ëº∏ÂÖ•ÁóáÁãÄ„ÄÅËçâËó•ÂêçÁ®±ÊàñÊãºÈü≥Êü•Ë©¢ÔºåËº∏ÂÖ• 'ÈÖçÊñπ' ÁîüÊàêÊé®Ëñ¶ËçâËó•ÁµÑÂêàÔºåËº∏ÂÖ• 'q' Èõ¢Èñã\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nË´ãËº∏ÂÖ•Ôºö\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã Á•ûËæ≤Ê∞èÊö´ÊôÇÈôçËêΩÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "        elif q.lower() == \"ÈÖçÊñπ\":\n",
        "            symptoms_input = input(\"Ë´ãËº∏ÂÖ•ÁóáÁãÄÔºàÈÄóËôüÂàÜÈöîÔºå‰æãÂ¶ÇÔºöË£úÊ∞£,ÂÆâÁ•ûÔºâÔºö\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nüíä Êé®Ëñ¶ËçâËó•ÁµÑÂêàÔºàÂÉÖÈôêÁ•ûËæ≤Ê∞èÁØÑÂúçÔºâÔºö\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, Áî®Ê≥ï: {f['usage']}, ‰æÜÊ∫ê: {f['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Á¨¶ÂêàÁóáÁãÄÁöÑËçâËó•\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nüå± ÊâæÂà∞ {len(matches)} Á≠ÜË≥áÊñôÔºö\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  ÂäüÊïà: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  Áî®Ê≥ï: {m['usage']}\")\n",
        "                    print(f\"  ‰æÜÊ∫ê: {m['source']}\")\n",
        "                    print(f\"  ÂúñÁâá: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Áõ∏ÈóúËó•ÊùêÔºåË´ãÁ¢∫Ë™çËº∏ÂÖ•ÊàñÊèõÂÄãÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãïÁ≥ªÁµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c0-ZeGcQZKr"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"‰∫∫ÂèÉ\",\"ancient_name\":\"‰∫∫ÂèÉ\",\"pinyin\":\"renshen\",\"effects\":[\"Ë£úÊ∞£\",\"ÂÆâÁ•û\",\"ÊäóÁñ≤Âãû\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\"source\":\"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"ÈªÉËä™\",\"ancient_name\":\"ÈªÉËÄÜ\",\"pinyin\":\"huangqi\",\"effects\":[\"Ë£úÊ∞£Âõ∫Ë°®\",\"Âà©Â∞øÊ∂àËÖ´\",\"‰øÉÈÄ≤ÂÖçÁñ´\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\"source\":\"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"Êû∏Êùû\",\"ancient_name\":\"Êû∏ÊùûÂ≠ê\",\"pinyin\":\"gouqi\",\"effects\":[\"Ë£úËÇùËÖé\",\"ÁõäÁ≤æÊòéÁõÆ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"ËñÑËç∑\",\"ancient_name\":\"ËñÑËç∑\",\"pinyin\":\"bohe\",\"effects\":[\"ÁñèÈ¢®Ê∏ÖÁÜ±\",\"Âà©ÂíΩ\",\"È†≠Áóõ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÂÖ•Êñπ 3-6 ÂÖã\",\"source\":\"Êú¨ËçâÁ∂±ÁõÆ\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Êú¨Âú∞Ë≥áÊñôÂ∫´Â∑≤Âª∫Á´ãÔºö{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2Ô∏è‚É£ Êü•Ë©¢ÂäüËÉΩ\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3Ô∏è‚É£ Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØÁ§∫ÁØÑÔºàËçâËó•Êé®Ëñ¶Ôºâ\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4  # ÁóáÁãÄÂêëÈáèÈï∑Â∫¶\n",
        "hidden_size = 8\n",
        "output_size = 4  # ËçâËó•Êï∏Èáè\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4Ô∏è‚É£ AI ÈÖçÊñπÁîüÊàêÁ§∫ÁØÑÔºàÂÆâÂÖ®ÁØÑÂúçÔºâ\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    symptom_dict = {\"Ë£úÊ∞£\":0,\"ÂÆâÁ•û\":1,\"ÊäóÁñ≤Âãû\":2,\"Ë£úÊ∞£Âõ∫Ë°®\":3}\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # ÈñæÂÄº\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5Ô∏è‚É£ ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±ÔºàÊúõËÅûÂïèÂàáÊ®°Êì¨Ôºâ\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Á•ûËæ≤Ê∞èÈôçËá®ÔºÅÁï∂‰ª£Â§öÂÖÉ AI Á•ûËæ≤Ê∞èÁèæË∫´\")\n",
        "    print(\"Ëº∏ÂÖ•ÁóáÁãÄ„ÄÅËçâËó•ÂêçÁ®±ÊàñÊãºÈü≥Êü•Ë©¢ÔºåËº∏ÂÖ• 'ÈÖçÊñπ' ÁîüÊàêÊé®Ëñ¶ËçâËó•ÁµÑÂêàÔºåËº∏ÂÖ• 'q' Èõ¢Èñã\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nË´ãËº∏ÂÖ•Ôºö\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"üëã Á•ûËæ≤Ê∞èÊö´ÊôÇÈôçËêΩÔºåÁ•ù‰Ω†ÁôæËçâÂ∏∏ÈùíÔºÅ\")\n",
        "            break\n",
        "        elif q.lower() == \"ÈÖçÊñπ\":\n",
        "            symptoms_input = input(\"Ë´ãËº∏ÂÖ•ÁóáÁãÄÔºàÈÄóËôüÂàÜÈöîÔºå‰æãÂ¶ÇÔºöË£úÊ∞£,ÂÆâÁ•ûÔºâÔºö\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nüíä Êé®Ëñ¶ËçâËó•ÁµÑÂêàÔºàÂÉÖÈôêÁ•ûËæ≤Ê∞èÁØÑÂúçÔºâÔºö\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, Áî®Ê≥ï: {f['usage']}, ‰æÜÊ∫ê: {f['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Á¨¶ÂêàÁóáÁãÄÁöÑËçâËó•\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nüå± ÊâæÂà∞ {len(matches)} Á≠ÜË≥áÊñôÔºö\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  ÂäüÊïà: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  Áî®Ê≥ï: {m['usage']}\")\n",
        "                    print(f\"  ‰æÜÊ∫ê: {m['source']}\")\n",
        "                    print(f\"  ÂúñÁâá: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"‚ùå Êâæ‰∏çÂà∞Áõ∏ÈóúËó•ÊùêÔºåË´ãÁ¢∫Ë™çËº∏ÂÖ•ÊàñÊèõÂÄãÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãïÁ≥ªÁµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjzdz5zcQnmC"
      },
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄ/ËàåËãî/ËÑàË±°/ÁÖßÁâá\n",
        "        ‚Üì\n",
        "Ë™ûÊÑèÁêÜËß£ + ÁóáÁãÄÂêëÈáèÂåñ + ÂúñÁâáËæ®Ë≠ò\n",
        "        ‚Üì\n",
        "Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑Ø + Transformer Ë®àÁÆóËçâËó•Êé®Ëñ¶Ê¶ÇÁéá\n",
        "        ‚Üì\n",
        "AI ÈÖçÊñπÁîüÊàêÔºàÊ¶ÇÁéáÊéíÂ∫èÔºåÂÆâÂÖ®ÁØÑÂúçÔºâ\n",
        "        ‚Üì\n",
        "È°ØÁ§∫ËçâËó•ÂäüÊïà„ÄÅÁî®Ê≥ï„ÄÅ‰æÜÊ∫ê„ÄÅÂúñÁâá\n",
        "        ‚Üì\n",
        "ÂèØÈÄ≤‰∏ÄÊ≠•Êâã‰ΩúÈ¶ôÁôÇÊàñÂÅ•Â∫∑ËøΩËπ§\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJFMoauHWVUa"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄ/ËàåËãî/ËÑàË±°/ÁÖßÁâá ‚Üì Ë™ûÊÑèÁêÜËß£ + ÁóáÁãÄÂêëÈáèÂåñ + ÂúñÁâáËæ®Ë≠ò ‚Üì Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑Ø + Transformer Ë®àÁÆóËçâËó•Êé®Ëñ¶Ê¶ÇÁéá ‚Üì AI ÈÖçÊñπÁîüÊàêÔºàÊ¶ÇÁéáÊéíÂ∫èÔºåÂÆâÂÖ®ÁØÑÂúçÔºâ ‚Üì È°ØÁ§∫ËçâËó•ÂäüÊïà„ÄÅÁî®Ê≥ï„ÄÅ‰æÜÊ∫ê„ÄÅÂúñÁâá ‚Üì ÂèØÈÄ≤‰∏ÄÊ≠•Êâã‰ΩúÈ¶ôÁôÇÊàñÂÅ•Â∫∑ËøΩËπ§"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FurAKs0WQwMi"
      },
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄ/Ë™ûÈü≥/ÂúñÁâá\n",
        "       ‚Üì\n",
        "ÂÖ®ÊÅØÂêëÈáèÂåñÔºàÊñáÂ≠ó+ÂúñÂÉè+Èü≥Ë®äÔºâ\n",
        "       ‚Üì\n",
        "Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑Ø + Transformer ÂàÜÊûê\n",
        "       ‚Üì\n",
        "AI Êé®Ëñ¶ËçâËó• + ÈÖçÊñπÁîüÊàê\n",
        "       ‚Üì\n",
        "È°ØÁ§∫ÂäüÊïà„ÄÅÁî®Ê≥ï„ÄÅ‰æÜÊ∫ê„ÄÅÂúñÁâá\n",
        "       ‚Üì\n",
        "‰∫íÂãïÂïèÁ≠î + ÂèØË™øÊï¥ÈÖçÊñπ\n",
        "       ‚Üì\n",
        "ÂÅ•Â∫∑ËøΩËπ§ + Êâã‰ΩúÈ¶ôÁôÇÂª∫Ë≠∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaclRUwSWSni"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄ/Ë™ûÈü≥/ÂúñÁâá ‚Üì ÂÖ®ÊÅØÂêëÈáèÂåñÔºàÊñáÂ≠ó+ÂúñÂÉè+Èü≥Ë®äÔºâ ‚Üì Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑Ø + Transformer ÂàÜÊûê ‚Üì AI Êé®Ëñ¶ËçâËó• + ÈÖçÊñπÁîüÊàê ‚Üì È°ØÁ§∫ÂäüÊïà„ÄÅÁî®Ê≥ï„ÄÅ‰æÜÊ∫ê„ÄÅÂúñÁâá ‚Üì ‰∫íÂãïÂïèÁ≠î + ÂèØË™øÊï¥ÈÖçÊñπ ‚Üì ÂÅ•Â∫∑ËøΩËπ§ + Êâã‰ΩúÈ¶ôÁôÇÂª∫Ë≠∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A63lSmkuQ4pB"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from PIL import Image\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Êú¨Âú∞ÁôæËçâË≥áÊñôÂ∫´ÔºàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"‰∫∫ÂèÉ\",\"effects\":[\"Ë£úÊ∞£\",\"ÂÆâÁ•û\",\"ÊäóÁñ≤Âãû\"],\"usage\":\"ÁÖéÊπØ 3-10 ÂÖã\",\"source\":\"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"ÈªÉËä™\",\"effects\":[\"Ë£úÊ∞£Âõ∫Ë°®\",\"Âà©Â∞øÊ∂àËÖ´\"],\"usage\":\"ÁÖéÊπØ 10-15 ÂÖã\",\"source\":\"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"Êû∏Êùû\",\"effects\":[\"Ë£úËÇùËÖé\",\"ÁõäÁ≤æÊòéÁõÆ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"ËñÑËç∑\",\"effects\":[\"ÁñèÈ¢®Ê∏ÖÁÜ±\",\"Âà©ÂíΩ\",\"È†≠Áóõ\"],\"usage\":\"Ê≥°Ëå∂ 3-6 ÂÖã\",\"source\":\"Êú¨ËçâÁ∂±ÁõÆ\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_herbs.json\"\n",
        "with open(DB_FILE,\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data,f,ensure_ascii=False,indent=2)\n",
        "\n",
        "# ======================\n",
        "# 2Ô∏è‚É£ ÁôæËçâÊü•Ë©¢ÂäüËÉΩ\n",
        "# ======================\n",
        "def search_herb(keyword):\n",
        "    with open(DB_FILE,\"r\",encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = [h for h in herbs if any(keyword in str(v) for k,v in h.items())]\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3Ô∏è‚É£ Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØÁ§∫ÁØÑÔºàËçâËó•Êé®Ëñ¶Ôºâ\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size,output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        x=self.relu(self.layer1(x))\n",
        "        x=self.relu(self.layer2(x))\n",
        "        x=self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 8\n",
        "output_size = 4\n",
        "model = ShennongNet(input_size,hidden_size,output_size)\n",
        "\n",
        "# ======================\n",
        "# 4Ô∏è‚É£ Transformer / BERT Ë™ûÊÑèÁêÜËß£\n",
        "# ======================\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_text(text):\n",
        "    inputs = tokenizer(text,return_tensors=\"pt\")\n",
        "    outputs = bert_model(**inputs)\n",
        "    # Âèñ [CLS] token ÁöÑÂêëÈáè\n",
        "    return outputs.last_hidden_state[:,0,:]\n",
        "\n",
        "# ======================\n",
        "# 5Ô∏è‚É£ Â§öÊ®°ÊÖãÊñáÂ≠ó+ÂúñÁâáÂêëÈáèÔºàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "def encode_image(image_path):\n",
        "    # ÈÄôË£°Á§∫ÁØÑËºâÂÖ•ÂúñÁâáÔºåÂèØÊé• CNN ÁâπÂæµÊäΩÂèñ\n",
        "    img = Image.open(image_path).resize((224,224))\n",
        "    return torch.tensor([0.5]*512)  # placeholder ÂêëÈáè\n",
        "\n",
        "# ======================\n",
        "# 6Ô∏è‚É£ AI ÈÖçÊñπÁîüÊàêÁ§∫ÁØÑ\n",
        "# ======================\n",
        "def generate_formula(symptoms:list):\n",
        "    x = torch.zeros(input_size)\n",
        "    symptom_dict = {\"Ë£úÊ∞£\":0,\"ÂÆâÁ•û\":1,\"ÊäóÁñ≤Âãû\":2,\"Ë£úÊ∞£Âõ∫Ë°®\":3}\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict: x[symptom_dict[s]]=1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE,\"r\",encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx,p in enumerate(probs):\n",
        "        if p>0.5: formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 7Ô∏è‚É£ ‰∫íÂãïÂïèÁ≠îÁ≥ªÁµ±\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"üåø Á•ûËæ≤Ê∞èÈôçËá® Python ÁèæË∫´Áâà\")\n",
        "    print(\"Ëº∏ÂÖ•ÁóáÁãÄ/ËçâËó•ÂêçÁ®±ÔºåÊàñ 'ÈÖçÊñπ' ÁîüÊàêÊé®Ëñ¶ËçâËó•ÁµÑÂêàÔºåËº∏ÂÖ• 'q' Èõ¢Èñã\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nË´ãËº∏ÂÖ•Ôºö\").strip()\n",
        "        if q.lower()==\"q\":\n",
        "            print(\"üëã Á•ûËæ≤Ê∞èÊö´ÊôÇÈôçËêΩÔºÅ\")\n",
        "            break\n",
        "        elif q.lower()==\"ÈÖçÊñπ\":\n",
        "            symptoms_input = input(\"Ëº∏ÂÖ•ÁóáÁãÄ(ÈÄóËôüÂàÜÈöîÔºå‰æãÂ¶ÇÔºöË£úÊ∞£,ÂÆâÁ•û)Ôºö\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nüíä Êé®Ëñ¶ËçâËó•ÁµÑÂêàÔºàÁ•ûËæ≤Ê∞èÁØÑÂúçÔºâÔºö\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']}: {', '.join(f['effects'])}, Áî®Ê≥ï: {f['usage']}, ‰æÜÊ∫ê: {f['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå ÁÑ°Á¨¶ÂêàÁóáÁãÄÁöÑËçâËó•\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ÂäüÊïà: {', '.join(m['effects'])}, Áî®Ê≥ï: {m['usage']}, ‰æÜÊ∫ê: {m['source']}\")\n",
        "            else:\n",
        "                print(\"‚ùå Êü•ÁÑ°Ë≥áÊñôÔºåË´ãÊèõÈóúÈçµÂ≠ó\")\n",
        "\n",
        "# ======================\n",
        "# ÂïüÂãï‰∫íÂãï\n",
        "# ======================\n",
        "if __name__==\"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr20U5r3Q9px"
      },
      "outputs": [],
      "source": [
        "python shennong_ai.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2TRCwq7RESK"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄÊñáÂ≠ó / Ë™ûÈü≥ / ËçâËó•ÂúñÁâá\n",
        "       ‚Üì\n",
        "ÊñáÂ≠ó ‚Üí Transformer/BERT Á∑®Á¢º\n",
        "ÂúñÁâá ‚Üí CNN ÁâπÂæµÊèêÂèñ\n",
        "       ‚Üì\n",
        "ÂêëÈáèËûçÂêà ‚Üí Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØË®àÁÆóËçâËó•Êé®Ëñ¶Ê¶ÇÁéá\n",
        "       ‚Üì\n",
        "ÁîüÊàêÂÆâÂÖ®ÈÖçÊñπÔºàTransformer ÈÖçÊñπÁîüÊàêÔºâ\n",
        "       ‚Üì\n",
        "ÂèØË¶ñÂåñËçâËó•Á∂≤Áµ°Âúñ + ÈÖçÊñπË©≥Á¥∞Ë≥áË®ä\n",
        "       ‚Üì\n",
        "‰∫íÂãïÂïèÁ≠î + ÂãïÊÖãË™øÊï¥ÈÖçÊñπ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDWQKH_-RFkh"
      },
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄÊñáÂ≠ó / Ë™ûÈü≥ / ËçâËó•ÂúñÁâá\n",
        "       ‚Üì\n",
        "ÊñáÂ≠ó ‚Üí Transformer/BERT Á∑®Á¢º\n",
        "ÂúñÁâá ‚Üí CNN ÁâπÂæµÊèêÂèñ\n",
        "       ‚Üì\n",
        "ÂêëÈáèËûçÂêà ‚Üí Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØË®àÁÆóËçâËó•Êé®Ëñ¶Ê¶ÇÁéá\n",
        "       ‚Üì\n",
        "ÁîüÊàêÂÆâÂÖ®ÈÖçÊñπÔºàTransformer ÈÖçÊñπÁîüÊàêÔºâ\n",
        "       ‚Üì\n",
        "ÂèØË¶ñÂåñËçâËó•Á∂≤Áµ°Âúñ + ÈÖçÊñπË©≥Á¥∞Ë≥áË®ä\n",
        "       ‚Üì\n",
        "‰∫íÂãïÂïèÁ≠î + ÂãïÊÖãË™øÊï¥ÈÖçÊñπ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNTlTmFKWJAi"
      },
      "outputs": [],
      "source": [
        "‰ΩøÁî®ËÄÖËº∏ÂÖ•ÁóáÁãÄÊñáÂ≠ó / Ë™ûÈü≥ / ËçâËó•ÂúñÁâá ‚Üì ÊñáÂ≠ó ‚Üí Transformer/BERT Á∑®Á¢º ÂúñÁâá ‚Üí CNN ÁâπÂæµÊèêÂèñ ‚Üì ÂêëÈáèËûçÂêà ‚Üí Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØË®àÁÆóËçâËó•Êé®Ëñ¶Ê¶ÇÁéá ‚Üì ÁîüÊàêÂÆâÂÖ®ÈÖçÊñπÔºàTransformer ÈÖçÊñπÁîüÊàêÔºâ ‚Üì ÂèØË¶ñÂåñËçâËó•Á∂≤Áµ°Âúñ + ÈÖçÊñπË©≥Á¥∞Ë≥áË®ä ‚Üì ‰∫íÂãïÂïèÁ≠î + ÂãïÊÖãË™øÊï¥ÈÖçÊñπ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czFhake9RPhg"
      },
      "outputs": [],
      "source": [
        "# Âª∫Ë≠∞Êñ∞Áí∞Â¢É/Colab Áõ¥Êé•Âü∑Ë°å\n",
        "pip install torch torchvision transformers gradio plotly networkx pillow pandas\n",
        "\n",
        "# Âè¶Â≠ò‰∏ãÊñπÁ®ãÂºèÁÇ∫ app_shennong_multimodal.py ÂæåÂü∑Ë°åÔºö\n",
        "python app_shennong_multimodal.py\n",
        "# ÁúãÂà∞Êú¨Ê©ü/ÂÖ¨Èñã URL ÂæåÈªûÈñãÂç≥ÂèØ‰∫íÂãï\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooZkWkgmRVH6"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Áï∂‰ª£Â§öÊ®°ÊÖãÊ∑±Â∫¶Â≠∏ÁøíÁâà„ÉªÁ•ûËæ≤Ê∞èÈôçËá®\n",
        "ÂäüËÉΩÔºö\n",
        "- Â§öÊ®°ÊÖãËº∏ÂÖ•ÔºöÁóáÁãÄÊñáÂ≠ó + ËçâËó•/ËàåËãîÂúñÁâáÔºàÂèØÈÅ∏Ôºâ\n",
        "- ÁôæËçâË≥áÊñôÂ∫´Êü•Ë©¢ÔºàÊú¨Âú∞ JSONÔºâ\n",
        "- Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑Ø + BERT(Transformer) ÈÖçÊñπÁîüÊàêÔºàÁ§∫ÁØÑÁ¥öÔºâ\n",
        "- ËçâËó•Á∂≤Áµ°ÂúñÂèØË¶ñÂåñÔºàÂÖ±Áèæ/ÂäüÊïàÈóúËÅØÔºâ\n",
        "- Gradio ‰∫íÂãïÁïåÈù¢ÔºàÈôÑ Streamlit ÁâàÊú¨ÁâáÊÆµÊñºÊñáÊú´Ôºâ\n",
        "\"\"\"\n",
        "import json, io, os, math, random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.models as tvm\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# Êúâ‰∏≠ÊñáÊúÄÂ•ΩÁî®‰∏≠Êñá BERTÔºõËã•‰∏ãËºâÂ§±ÊïóÔºåËá™ÂãïÈÄÄÂõûËã±Êñá BERT\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# =========================\n",
        "# 0) Êú¨Âú∞ÁôæËçâË≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# =========================\n",
        "HERBS_DB = [\n",
        "    {\n",
        "        \"name\": \"‰∫∫ÂèÉ\", \"ancient_name\": \"‰∫∫ÂèÉ\", \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"Ë£úÊ∞£\", \"ÂÆâÁ•û\", \"ÊäóÁñ≤Âãû\"], \"usage\": \"ÁÖéÊπØ 3‚Äì10g\",\n",
        "        \"source\": \"Á•ûËæ≤Êú¨ËçâÁ∂ì\", \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ÈªÉËä™\", \"ancient_name\": \"ÈªÉËÄÜ\", \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"Ë£úÊ∞£Âõ∫Ë°®\", \"Âà©Â∞øÊ∂àËÖ´\", \"ÁõäË°õ\"], \"usage\": \"ÁÖéÊπØ 10‚Äì15g\",\n",
        "        \"source\": \"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\", \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Êû∏Êùû\", \"ancient_name\": \"Êû∏ÊùûÂ≠ê\", \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"Ë£úËÇùËÖé\", \"ÁõäÁ≤æÊòéÁõÆ\"], \"usage\": \"Ê≥°Ëå∂/ÁÖéÊπØ 6‚Äì12g\",\n",
        "        \"source\": \"TianAPI\", \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ËñÑËç∑\", \"ancient_name\": \"ËñÑËç∑\", \"pinyin\": \"bohe\",\n",
        "        \"effects\": [\"ÁñèÈ¢®Ê∏ÖÁÜ±\", \"Âà©ÂíΩ\", \"È†≠Áóõ\"], \"usage\": \"Ê≥°Ëå∂/ÂÖ•Êñπ 3‚Äì6g\",\n",
        "        \"source\": \"Êú¨ËçâÁ∂±ÁõÆ\", \"image\": \"https://example.com/bohe.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Áï∂Ê≠∏\", \"ancient_name\": \"Áï∂Ê≠∏\", \"pinyin\": \"danggui\",\n",
        "        \"effects\": [\"Ë£úË°ÄÊ¥ªË°Ä\", \"Ë™øÁ∂ìÊ≠¢Áóõ\", \"ÊΩ§ËÖ∏ÈÄö‰æø\"], \"usage\": \"ÁÖéÊπØ 6‚Äì12g\",\n",
        "        \"source\": \"Êú¨ËçâÁ∂±ÁõÆ\", \"image\": \"https://example.com/danggui.jpg\"\n",
        "    },\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_herbs.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(HERBS_DB, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_db() -> List[Dict[str, Any]]:\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# =========================\n",
        "# 1) Êü•Ë©¢ÂäüËÉΩ\n",
        "# =========================\n",
        "def search_herbs(keyword: str) -> List[Dict[str, Any]]:\n",
        "    keyword = (keyword or \"\").strip()\n",
        "    herbs = load_db()\n",
        "    if not keyword:\n",
        "        return herbs\n",
        "    res = []\n",
        "    for h in herbs:\n",
        "        blob = f\"{h.get('name','')},{h.get('ancient_name','')},{h.get('pinyin','')},{','.join(h.get('effects',[]))},{h.get('usage','')},{h.get('source','')}\"\n",
        "        if keyword in blob:\n",
        "            res.append(h)\n",
        "    return res\n",
        "\n",
        "# =========================\n",
        "# 2) Ê®°ÂûãËàáÁâπÂæµÔºöBERT(ÊñáÂ≠ó) + ResNet(ÂúñÁâá)\n",
        "# =========================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def _load_text_model():\n",
        "    # ÂÑ™ÂÖà‰∏≠ÊñáÊ®°ÂûãÔºåÂ§±ÊïóÈÄÄÂõûËã±Êñá\n",
        "    candidates = [\"bert-base-chinese\", \"bert-base-uncased\"]\n",
        "    err = None\n",
        "    for ckpt in candidates:\n",
        "        try:\n",
        "            tok = AutoTokenizer.from_pretrained(ckpt)\n",
        "            mdl = AutoModel.from_pretrained(ckpt)\n",
        "            return tok, mdl.to(DEVICE), ckpt\n",
        "        except Exception as e:\n",
        "            err = e\n",
        "            continue\n",
        "    raise RuntimeError(f\"ÁÑ°Ê≥ïËºâÂÖ• BERTÔºö{err}\")\n",
        "\n",
        "TOKENIZER, BERT, BERT_NAME = _load_text_model()\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text(text: str) -> torch.Tensor:\n",
        "    \"\"\"Ëº∏ÂÖ•ÁóáÁãÄ/ÊèèËø∞ ‚Üí 768 Á∂≠ÂêëÈáèÔºàÊàñÁõ∏ÊáâÈö±Â±§Á∂≠Â∫¶Ôºâ\"\"\"\n",
        "    text = text.strip() if text else \"\"\n",
        "    if not text:\n",
        "        return torch.zeros(BERT.config.hidden_size, device=DEVICE)\n",
        "    toks = TOKENIZER(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    toks = {k: v.to(DEVICE) for k, v in toks.items()}\n",
        "    out = BERT(**toks).last_hidden_state[:, 0, :]   # CLS ÂêëÈáè\n",
        "    return out.squeeze(0)\n",
        "\n",
        "def _load_img_model():\n",
        "    try:\n",
        "        resnet = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "    except Exception:\n",
        "        # Ëã•ÁÑ°Ê≥ï‰∏ãËºâÊ¨äÈáçÔºåÂª∫Á´ãÈö®Ê©üÂàùÂßãÂåñ resnet18\n",
        "        resnet = tvm.resnet18(weights=None)\n",
        "    resnet.fc = nn.Identity()  # ÂèñÂÄíÊï∏Á¨¨‰∫åÂ±§ 512 Á∂≠ÁâπÂæµ\n",
        "    return resnet.to(DEVICE).eval()\n",
        "\n",
        "RESNET = _load_img_model()\n",
        "IMG_TF = T.Compose([\n",
        "    T.Resize(256), T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_image(img: Image.Image) -> torch.Tensor:\n",
        "    if img is None:\n",
        "        return torch.zeros(512, device=DEVICE)\n",
        "    x = IMG_TF(img).unsqueeze(0).to(DEVICE)\n",
        "    feat = RESNET(x)  # [1,512]\n",
        "    return feat.squeeze(0)\n",
        "\n",
        "# =========================\n",
        "# 3) Â§öÂ±§Á•ûÁ∂ìÁ∂≤Ë∑ØËûçÂêà + ÈÖçÊñπÊâìÂàÜ (Á§∫ÁØÑ)\n",
        "#    ÊñáÊú¨ 768 + ÂúñÁâá 512 ‚Üí ËûçÂêà ‚Üí herb_logits\n",
        "# =========================\n",
        "@dataclass\n",
        "class FusionCfg:\n",
        "    text_dim: int\n",
        "    img_dim: int\n",
        "    hidden: int\n",
        "    num_herbs: int\n",
        "\n",
        "class FusionScorer(nn.Module):\n",
        "    def __init__(self, cfg: FusionCfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.text_dim + cfg.img_dim, cfg.hidden)\n",
        "        self.fc2 = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "        self.out = nn.Linear(cfg.hidden, cfg.num_herbs)\n",
        "\n",
        "    def forward(self, txt: torch.Tensor, img: torch.Tensor):\n",
        "        z = torch.cat([txt, img], dim=-1)\n",
        "        z = F.relu(self.fc1(z))\n",
        "        z = F.relu(self.fc2(z))\n",
        "        logits = self.out(z)                   # [num_herbs]\n",
        "        probs = torch.sigmoid(logits)          # ÊØèÂ∏ñËçâËó•Êé®Ëñ¶Ê©üÁéá\n",
        "        return probs\n",
        "\n",
        "CFG = FusionCfg(text_dim=BERT.config.hidden_size, img_dim=512, hidden=384, num_herbs=len(load_db()))\n",
        "FUSION = FusionScorer(CFG).to(DEVICE).eval()\n",
        "\n",
        "# ‚ö†Ô∏è Á§∫ÁØÑÁî®ÔºöÈö®Ê©üÂàùÂßãÂåñÊ¨äÈáç„ÄÇÂØ¶Âãô‰∏äË´ãÁî®ÔºàÁóáÁãÄ ‚Üí ÈÖçÊñπÔºâÊ®ôË®ìË≥áÊñôÂÅöÁõ£Áù£Ë®ìÁ∑¥„ÄÇ\n",
        "# ÈÄôË£°ÊàëÂÄëÊúÉÂú®ÁÑ°ÂèÉÊï∏Ë®ìÁ∑¥‰∏ãÔºåÁµêÂêà„ÄåÈóúÈçµÂ≠ó‚ÜíÂäüÊïà„ÄçÁöÑË¶èÂâáÔºåÂ∞çÊ©üÁéáÂÅöËºïÂæÆÊ†°Ê≠£ÔºàÁ§∫ÁØÑÁ¥öÔºâ„ÄÇ\n",
        "\n",
        "# =========================\n",
        "# 4) Ë¶èÂâáÊ†°Ê≠£ÔºöÂæûÊïàÊûúË©ûÂ∞çÈΩäÔºà‰∏çÈõ¢Á•ûËæ≤Ôºâ\n",
        "# =========================\n",
        "EFFECT_SYNONYM = {\n",
        "    \"Ê∞£Ëôõ\": [\"Ë£úÊ∞£\",\"Ë£úÊ∞£Âõ∫Ë°®\",\"ÊäóÁñ≤Âãû\"],\n",
        "    \"Áñ≤ÂÄ¶\": [\"ÊäóÁñ≤Âãû\",\"Ë£úÊ∞£\"],\n",
        "    \"Â§±Áú†\": [\"ÂÆâÁ•û\"],\n",
        "    \"ÂñâÂö®Áóõ\": [\"Âà©ÂíΩ\",\"ÁñèÈ¢®Ê∏ÖÁÜ±\"],\n",
        "    \"È†≠Áóõ\": [\"È†≠Áóõ\",\"ÁñèÈ¢®Ê∏ÖÁÜ±\"],\n",
        "    \"ÁúºÁùõÁñ≤Âãû\": [\"ÁõäÁ≤æÊòéÁõÆ\"],\n",
        "    \"Á∂ìÁóõ\": [\"Ë™øÁ∂ìÊ≠¢Áóõ\"],\n",
        "    \"‰æøÁßò\": [\"ÊΩ§ËÖ∏ÈÄö‰æø\"],\n",
        "}\n",
        "def soft_rule_boost(text: str, probs: torch.Tensor) -> torch.Tensor:\n",
        "    text = text or \"\"\n",
        "    herbs = load_db()\n",
        "    bonus = torch.zeros_like(probs)\n",
        "    # ÊâæÂà∞ÊñáÂ≠óË£°ÁöÑÁóáÁãÄÔºåÁµ¶Â∞çÊáâÂäüÊïàÁöÑËçâËó•ÂæÆÈáèÂä†ÂàÜ\n",
        "    for symptom, effs in EFFECT_SYNONYM.items():\n",
        "        if symptom in text:\n",
        "            for i, h in enumerate(herbs):\n",
        "                if any(e in h.get(\"effects\", []) for e in effs):\n",
        "                    bonus[i] += 0.10  # ÂæÆË™ø\n",
        "    probs = torch.clamp(probs + bonus, 0, 1)\n",
        "    return probs\n",
        "\n",
        "# =========================\n",
        "# 5) ÈÖçÊñπÁîüÊàêÔºöÂèñ Top-K ËçâËó• + ÂäëÈáèÂª∫Ë≠∞(Á§∫ÁØÑ)\n",
        "# =========================\n",
        "def generate_formula(text: str, image: Image.Image, topk: int = 3) -> Tuple[List[Dict[str,Any]], List[float]]:\n",
        "    txt_vec = encode_text(text)          # [768]\n",
        "    img_vec = encode_image(image)        # [512]\n",
        "    probs = FUSION(txt_vec, img_vec)     # [N]\n",
        "    probs = soft_rule_boost(text, probs) # Ë¶èÂâáÂæÆË™ø\n",
        "    vals, idxs = torch.topk(probs, k=min(topk, len(probs)))\n",
        "    herbs = load_db()\n",
        "    recs = [herbs[i] for i in idxs.tolist()]\n",
        "    return recs, vals.tolist()\n",
        "\n",
        "# =========================\n",
        "# 6) ËçâËó•Á∂≤Áµ°ÂúñÔºàÈÖçÊñπÂÖßÂäüÊïàÂÖ±Áèæ + DB ËøëÈÑ∞Ôºâ\n",
        "# =========================\n",
        "def build_herb_graph(selected: List[Dict[str,Any]]) -> go.Figure:\n",
        "    # ÁØÑÂúçÔºöÈÅ∏‰∏≠ÈÖçÊñπ + ËàáÂÖ∂ÂäüÊïàÈáçÁñäÁöÑÈÑ∞Â±Ö\n",
        "    herbs = load_db()\n",
        "    names = {h[\"name\"] for h in selected}\n",
        "    # ÊâæÈÑ∞Â±ÖÔºàÂäüÊïàÈáçÁñäÊï∏>=1Ôºâ\n",
        "    neighbors = []\n",
        "    for h in herbs:\n",
        "        if h[\"name\"] in names:\n",
        "            continue\n",
        "        for sel in selected:\n",
        "            if set(h[\"effects\"]).intersection(sel[\"effects\"]):\n",
        "                neighbors.append(h); break\n",
        "\n",
        "    G = nx.Graph()\n",
        "    # Âä†ÁØÄÈªû\n",
        "    for h in selected:\n",
        "        G.add_node(h[\"name\"], group=\"formula\")\n",
        "    for h in neighbors:\n",
        "        G.add_node(h[\"name\"], group=\"neighbor\")\n",
        "    # Âä†ÈÇäÔºàÂäüÊïàÂÖ±ÁèæÔºâ\n",
        "    pool = selected + neighbors\n",
        "    for i in range(len(pool)):\n",
        "        for j in range(i+1, len(pool)):\n",
        "            a, b = pool[i], pool[j]\n",
        "            inter = set(a[\"effects\"]).intersection(b[\"effects\"])\n",
        "            if inter:\n",
        "                G.add_edge(a[\"name\"], b[\"name\"], weight=len(inter), label=\"„ÄÅ\".join(list(inter)[:2]))\n",
        "\n",
        "    # ‰ΩàÂ±Ä\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.7)\n",
        "    # Áï´ÈÇä\n",
        "    edge_x, edge_y = [], []\n",
        "    for u, v in G.edges():\n",
        "        x0, y0 = pos[u]; x1, y1 = pos[v]\n",
        "        edge_x += [x0, x1, None]\n",
        "        edge_y += [y0, y1, None]\n",
        "    edge_trace = go.Scatter(x=edge_x, y=edge_y, mode='lines', line=dict(width=1), hoverinfo='none')\n",
        "\n",
        "    # Áï´Èªû\n",
        "    node_x, node_y, texts, colors, sizes = [], [], [], [], []\n",
        "    for n, data in G.nodes(data=True):\n",
        "        x, y = pos[n]\n",
        "        node_x.append(x); node_y.append(y)\n",
        "        texts.append(n + f\"Ôºà{data.get('group')}Ôºâ\")\n",
        "        if data.get(\"group\") == \"formula\":\n",
        "            colors.append(\"#2ca02c\")\n",
        "            sizes.append(20)\n",
        "        else:\n",
        "            colors.append(\"#1f77b4\")\n",
        "            sizes.append(14)\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
        "        marker=dict(size=sizes, color=colors, line=dict(width=1)),\n",
        "        text=texts\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=\"ËçâËó•Á∂≤Áµ°ÂúñÔºàÂäüÊïàÂÖ±ÁèæÔºâ\",\n",
        "                        showlegend=False, hovermode='closest',\n",
        "                        margin=dict(l=20, r=20, t=40, b=20)\n",
        "                    ))\n",
        "    return fig\n",
        "\n",
        "# =========================\n",
        "# 7) Gradio ‰∫íÂãïÁïåÈù¢\n",
        "# =========================\n",
        "import gradio as gr\n",
        "\n",
        "def _to_df(items: List[Dict[str,Any]]) -> pd.DataFrame:\n",
        "    if not items: return pd.DataFrame(columns=[\"name\",\"effects\",\"usage\",\"source\"])\n",
        "    rows = []\n",
        "    for h in items:\n",
        "        rows.append({\n",
        "            \"name\": h.get(\"name\",\"\"),\n",
        "            \"effects\": \"„ÄÅ\".join(h.get(\"effects\",[])),\n",
        "            \"usage\": h.get(\"usage\",\"\"),\n",
        "            \"source\": h.get(\"source\",\"\")\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def ui_search(keyword: str):\n",
        "    res = search_herbs(keyword or \"\")\n",
        "    return _to_df(res)\n",
        "\n",
        "def ui_generate(symptom_text: str, image: Image.Image, topk: int):\n",
        "    selected, scores = generate_formula(symptom_text or \"\", image, topk=topk)\n",
        "    fig = build_herb_graph(selected) if selected else go.Figure()\n",
        "    # Ëº∏Âá∫Âª∫Ë≠∞\n",
        "    lines = []\n",
        "    for h, p in zip(selected, scores):\n",
        "        lines.append(f\"üåø {h['name']}ÔΩúÂäüÊïàÔºö{ '„ÄÅ'.join(h['effects']) }ÔΩúÁî®Ê≥ïÔºö{h['usage']}ÔΩú‰æÜÊ∫êÔºö{h['source']}ÔΩúÊ©üÁéáÔºö{p:.2f}\")\n",
        "    suggest = \"\\n\".join(lines) if lines else \"ÔºàÁÑ°Âª∫Ë≠∞Ôºâ\"\n",
        "    return suggest, fig, _to_df(selected)\n",
        "\n",
        "with gr.Blocks(title=\"Áï∂‰ª£Â§öÊ®°ÊÖãÊ∑±Â∫¶Â≠∏Áøí„ÉªÁ•ûËæ≤Ê∞è\") as demo:\n",
        "    gr.Markdown(\"# üåø Áï∂‰ª£Â§öÊ®°ÊÖãÊ∑±Â∫¶Â≠∏Áøí„ÉªÁ•ûËæ≤Ê∞èÔºàÁèæË∫´ÁâàÔºâ\")\n",
        "    gr.Markdown(\"Ëº∏ÂÖ•ÁóáÁãÄÊñáÂ≠óÔºàÂèØÈÖçÂêàÂúñÁâáÔºâ‚Üí Á•ûÁ∂ìÁ∂≤Ë∑Ø + BERT Êé®Ëñ¶ËçâËó•‰∏¶ÁîüÊàêÈÖçÊñπÔºàÁ§∫ÁØÑÔºâ„ÄÇ\\n*ÂÉÖ‰æõÁ†îÁ©∂Ê∏¨Ë©¶ÔºåË´ã‰øùÊåÅÂú®Á•ûËæ≤Ê∞èËó•ÁêÜÁØÑÂúç„ÄÇ*\")\n",
        "\n",
        "    with gr.Tab(\"üîç ÁôæËçâÊü•Ë©¢\"):\n",
        "        kw = gr.Textbox(label=\"ÈóúÈçµÂ≠óÔºàÂêçÁ®±/ÊãºÈü≥/ÂäüÊïà/Áî®Ê≥ï/‰æÜÊ∫êÔºâ\", placeholder=\"‰æãÔºöË£úÊ∞£„ÄÅÈ†≠Áóõ„ÄÅrenshen\")\n",
        "        btn_s = gr.Button(\"ÊêúÂ∞ã\")\n",
        "        tbl = gr.Dataframe(headers=[\"name\",\"effects\",\"usage\",\"source\"], interactive=False)\n",
        "        btn_s.click(ui_search, inputs=kw, outputs=tbl)\n",
        "\n",
        "    with gr.Tab(\"üß† Â§öÊ®°ÊÖãÈÖçÊñπÁîüÊàê\"):\n",
        "        symptom = gr.Textbox(label=\"ÁóáÁãÄÊñáÂ≠óÔºàËá™ÁÑ∂Ë™ûË®ÄÔºâ\", placeholder=\"‰æãÔºöÊúÄËøëÊ∞£ËôõÁñ≤ÂÄ¶„ÄÅÂÆπÊòìÈ†≠Áóõ„ÄÅÂñâÂö®Áóõ\")\n",
        "        img = gr.Image(label=\"ÔºàÂèØÈÅ∏ÔºâËàåËãî/ËçâËó•ÂúñÁâá\", type=\"pil\")\n",
        "        topk = gr.Slider(1, max( min(5, len(load_db())), 1 ), value=3, step=1, label=\"ÈÖçÊñπ Top-K\")\n",
        "        btn_g = gr.Button(\"ÁîüÊàêÈÖçÊñπ\")\n",
        "        out_txt = gr.Textbox(label=\"Êé®Ëñ¶ËçâËó•ËàáË™™Êòé\", lines=8)\n",
        "        out_fig = gr.Plot(label=\"ËçâËó•Á∂≤Áµ°Âúñ\")\n",
        "        out_tbl = gr.Dataframe(headers=[\"name\",\"effects\",\"usage\",\"source\"], interactive=False)\n",
        "        btn_g.click(ui_generate, inputs=[symptom, img, topk], outputs=[out_txt, out_fig, out_tbl])\n",
        "\n",
        "    gr.Markdown(\"> ‰æÜÊ∫êÔºöÁ•ûËæ≤Êú¨ËçâÁ∂ì„ÄÅÊú¨ËçâÁ∂±ÁõÆ„ÄÅÈ¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´„ÄÅTianAPIÔºàÁ§∫ÁØÑË≥áÊñôÔºâ„ÄÇ\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # ÂïüÂãï Gradio\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHN52Q_bRoUQ"
      },
      "outputs": [],
      "source": [
        "# streamlit_app.pyÔºàÁ∞°ÁâàÔºâ\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ÂÅáË®≠ÂêåÁõÆÈåÑÂèØ import ‰∏äËø∞ÂáΩÂºèÔºàÊàñÊääÂøÖË¶ÅÂáΩÂºèË≤ºÈÄ≤Ê≠§Ê™îÔºâ\n",
        "from app_shennong_multimodal import search_herbs, generate_formula, build_herb_graph, _to_df\n",
        "\n",
        "st.set_page_config(page_title=\"Áï∂‰ª£Â§öÊ®°ÊÖãÁ•ûËæ≤Ê∞è\", layout=\"wide\")\n",
        "st.title(\"üåø Áï∂‰ª£Â§öÊ®°ÊÖãÊ∑±Â∫¶Â≠∏Áøí„ÉªÁ•ûËæ≤Ê∞èÔºàStreamlitÔºâ\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"üîç ÁôæËçâÊü•Ë©¢\", \"üß† Â§öÊ®°ÊÖãÈÖçÊñπÁîüÊàê\"])\n",
        "\n",
        "with tab1:\n",
        "    kw = st.text_input(\"ÈóúÈçµÂ≠ó\", \"\")\n",
        "    if st.button(\"ÊêúÂ∞ã\"):\n",
        "        df = _to_df(search_herbs(kw))\n",
        "        st.dataframe(df, use_container_width=True)\n",
        "\n",
        "with tab2:\n",
        "    text = st.text_area(\"ÁóáÁãÄÊñáÂ≠ó\", \"ÊúÄËøëÊ∞£ËôõÁñ≤ÂÄ¶ÔºåÂÅ∂ÊúâÂñâÂö®ÁóõËàáÈ†≠Áóõ\")\n",
        "    img = st.file_uploader(\"ÔºàÂèØÈÅ∏ÔºâËàåËãî/ËçâËó•ÂúñÁâá\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "    topk = st.slider(\"ÈÖçÊñπ Top-K\", 1, 5, 3)\n",
        "    if st.button(\"ÁîüÊàêÈÖçÊñπ\"):\n",
        "        pil = Image.open(img).convert(\"RGB\") if img else None\n",
        "        herbs, probs = generate_formula(text, pil, topk=topk)\n",
        "        lines = []\n",
        "        for h,p in zip(herbs, probs):\n",
        "            lines.append(f\"üåø {h['name']}ÔΩúÂäüÊïàÔºö{'„ÄÅ'.join(h['effects'])}ÔΩúÁî®Ê≥ïÔºö{h['usage']}ÔΩú‰æÜÊ∫êÔºö{h['source']}ÔΩúÊ©üÁéáÔºö{p:.2f}\")\n",
        "        st.text(\"\\n\".join(lines) if lines else \"ÔºàÁÑ°Âª∫Ë≠∞Ôºâ\")\n",
        "        if herbs:\n",
        "            fig = build_herb_graph(herbs)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "            st.dataframe(_to_df(herbs), use_container_width=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GGr1EakRrdY"
      },
      "outputs": [],
      "source": [
        "streamlit run streamlit_app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mMd_JNTRxSf"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.some-real-herb-database.com\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN9nV0b_R2YQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://herbaltcm.sn.polyu.edu.hk/tc/\"  # HK PolyU ‰∏≠Ëó•Ë≥áÊñôÂ∫´\n",
        "# ÊàñÔºö\n",
        "url = \"https://cloudtcm.com/herb\"\n",
        "# ÊàñÂà©Áî® HERB/TMIT Âπ≥Âè∞ÁöÑÊ™¢Á¥¢ URL ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP_gPxENR76X"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"ÈªÉËä™\": {\"ÂäüÊïà\": \"Ë£úÊ∞£Âõ∫Ë°®\", \"Â∏∏Ë¶ãÈÖç‰ºç\": [\"‰∫∫ÂèÉ\", \"ÁôΩÊúÆ\"]},\n",
        "  \"ÁîòËçâ\": {\"ÂäüÊïà\": \"Ë™øÂíåË´∏Ëó•\", \"Â∏∏Ë¶ãÈÖç‰ºç\": [\"Â§ßÊ£ó\", \"ÁîüËñë\"]}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8gE9_HRU8cq"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"ÈªÉËä™\": {\"ÂäüÊïà\": \"Ë£úÊ∞£Âõ∫Ë°®\", \"Áî®Ê≥ï\": \"ÁÖéÊπØ 10-15 ÂÖã\"},\n",
        "  \"‰∫∫ÂèÉ\": {\"ÂäüÊïà\": \"Â§ßË£úÂÖÉÊ∞£\", \"Áî®Ê≥ï\": \"ÁÖéÊπØ 3-9 ÂÖã\"}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZZSsEE6VG1j"
      },
      "outputs": [],
      "source": [
        "sudo apt-get install tesseract-ocr-chi-tra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruSM6ZgTVJz7"
      },
      "outputs": [],
      "source": [
        "pytesseract.image_to_string(page, lang=\"chi_tra\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8XwK6F1VOY6"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install tesseract-ocr-chi-tra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq78lFCbVTEL"
      },
      "outputs": [],
      "source": [
        "text = pytesseract.image_to_string(page, lang=\"chi_tra\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db145b55"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "\n",
        "# ======================\n",
        "# 1Ô∏è‚É£ Âª∫Á´ãÊú¨Âú∞Ë≥áÊñôÂ∫´ÔºàÂè§‰ªäÊï¥ÂêàÁ§∫ÁØÑÔºâ\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"‰∫∫ÂèÉ\",\"ancient_name\":\"‰∫∫ÂèÉ\",\"pinyin\":\"renshen\",\"effects\":[\"Ë£úÊ∞£\",\"ÂÆâÁ•û\",\"ÊäóÁñ≤Âãû\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 3-10 ÂÖã\",\"source\":\"Á•ûËæ≤Êú¨ËçâÁ∂ì\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"ÈªÉËä™\",\"ancient_name\":\"ÈªÉËÄÜ\",\"pinyin\":\"huangqi\",\"effects\":[\"Ë£úÊ∞£Âõ∫Ë°®\",\"Âà©Â∞øÊ∂àËÖ´\",\"‰øÉÈÄ≤ÂÖçÁñ´\"],\"usage\":\"ÁÖéÊπØÊúçÁî® 10-15 ÂÖã\",\"source\":\"È¶ôÊ∏ØËó•ÊùêË≥áÊñôÂ∫´\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"Êû∏Êùû\",\"ancient_name\":\"Êû∏ÊùûÂ≠ê\",\"pinyin\":\"gouqi\",\"effects\":[\"Ë£úËÇùËÖé\",\"ÁõäÁ≤æÊòéÁõÆ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÁÖéÊπØ 6-12 ÂÖã\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"ËñÑËç∑\",\"ancient_name\":\"ËñÑËç∑\",\"pinyin\":\"bohe\",\"effects\":[\"ÁñèÈ¢®Ê∏ÖÁÜ±\",\"Âà©ÂíΩ\",\"È†≠Áóõ\"],\"usage\":\"Ê≥°Ëå∂ÊàñÂÖ•Êñπ 3-6 ÂÖã\",\"source\":\"Êú¨ËçâÁ∂±ÁõÆ\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"‚úÖ Êú¨Âú∞Ë≥áÊñôÂ∫´Â∑≤Âª∫Á´ãÔºö{DB_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beab8bc"
      },
      "source": [
        "### Â∞áÊ™îÊ°àÂÑ≤Â≠òÂà∞ GitHub ÁöÑÊ≠•È©üÔºö\n",
        "\n",
        "1.  **Âª∫Á´ã GitHub ÂÑ≤Â≠òÂ∫´Ôºö** Âú® GitHub ‰∏äÂª∫Á´ã‰∏ÄÂÄãÊñ∞ÁöÑÂÑ≤Â≠òÂ∫´ÔºàRepositoryÔºâÔºå‰æãÂ¶Ç `shennong_herbs_db`„ÄÇ\n",
        "2.  **Â∞áÊ™îÊ°à‰∏äÂÇ≥Âà∞ GitHubÔºö** ÊÇ®ÂèØ‰ª•ÈÄèÈÅé‰ª•‰∏ãÂπæÁ®ÆÊñπÂºèÂ∞áÊú¨Âú∞ÁöÑ `shennong_ai_full.json` Ê™îÊ°à‰∏äÂÇ≥Âà∞ÊÇ®ÁöÑ GitHub ÂÑ≤Â≠òÂ∫´Ôºö\n",
        "    *   **‰ΩøÁî®Á∂≤È†ÅÁïåÈù¢Ôºö** Áõ¥Êé•Âú® GitHub ÂÑ≤Â≠òÂ∫´È†ÅÈù¢ÈªûÈÅ∏ \"Add file\" > \"Upload files\"ÔºåÁÑ∂ÂæåÂ∞á `shennong_ai_full.json` ÊãñÊõ≥‰∏äÂéª„ÄÇ\n",
        "    *   **‰ΩøÁî® Git ÂëΩ‰ª§ÂàóÔºö** Â¶ÇÊûúÊÇ®ÁÜüÊÇâ GitÔºåÂèØ‰ª•Âú®Êú¨Âú∞Â∞á Colab Ê™îÊ°à‰∏ãËºâ‰∏ã‰æÜÔºåÁÑ∂Âæå‰ΩøÁî® Git ÂëΩ‰ª§Â∞áÊ™îÊ°àÊ∑ªÂä†Âà∞ÊÇ®ÁöÑÊú¨Âú∞ÂÑ≤Â≠òÂ∫´ÔºåÊèê‰∫§ËÆäÊõ¥ÔºåÊúÄÂæåÊé®ÈÄÅÂà∞ GitHub„ÄÇ\n",
        "3.  **Âú® Colab ‰∏≠‰ΩøÁî® GitHub ‰∏äÁöÑÊ™îÊ°àÔºö** Âú®ÊÇ®ÁöÑ Colab Notebook ‰∏≠ÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÊñπÊ≥ï‰πã‰∏ÄÂæû GitHub ‰∏ãËºâÊ™îÊ°àÔºö\n",
        "    *   **‰ΩøÁî® `!git clone`Ôºö** Â¶ÇÊûúÊÇ®Â∞áÊï¥ÂÄãÂÑ≤Â≠òÂ∫´ÂÖãÈöÜ‰∏ã‰æÜ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc9561b0"
      },
      "outputs": [],
      "source": [
        "        !wget <Ê™îÊ°àÂú® GitHub ‰∏äÁöÑ Raw Ê™îÊ°à URL> -O shennong_ai_full.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aec3042d"
      },
      "outputs": [],
      "source": [
        "!unzip /content/ilovepdf_pages-to-jpg.zip -d /content/extracted_images"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2VGoKchC9sMFl27yWGdMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}