{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivy455899-crypto/-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YiXnfojtMtF6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "61aa984c-39c1-4cc8-cf82-8e2599335dec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2721965297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# ç™¼é€ HTTP GET è«‹æ±‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='example-herb-database.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f0add55f1a0>: Failed to resolve 'example-herb-database.com' ([Errno -2] Name or service not known)\"))"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# æ¨¡æ“¬ç€è¦½å™¨æ¨™é ­\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# ã€Œç¥è¾²åšç™¾è‰ã€è³‡æ–™ä¾†æºï¼ˆç¯„ä¾‹ç”¨ä¸€å€‹å‡æƒ³ä¸­è—¥è‰ç¶²ç«™ï¼‰\n",
        "site_name = \"ç¥è¾²åšç™¾è‰\"\n",
        "url = \"https://example-herb-database.com\"  # é€™è£¡æ›æˆä½ çš„è³‡æ–™ä¾†æºç¶²å€\n",
        "\n",
        "# ç™¼é€ HTTP GET è«‹æ±‚\n",
        "res = requests.get(url, headers=headers)\n",
        "\n",
        "if res.status_code == 200:\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # æŠ“å–è—¥è‰åç¨±èˆ‡åŠŸæ•ˆï¼ˆå‡è¨­ç¶²ç«™ç”¨ <div class=\"herb\"> åŒ…å«è³‡æ–™ï¼‰\n",
        "    herbs = soup.select(\".herb\")\n",
        "    herb_dict = {}\n",
        "\n",
        "    for herb in herbs:\n",
        "        name = herb.select_one(\".name\").text.strip()\n",
        "        effect = herb.select_one(\".effect\").text.strip()\n",
        "        herb_dict[name] = effect\n",
        "\n",
        "    # æ¨¡æ“¬ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€\n",
        "    symptom = input(\"è«‹è¼¸å…¥ä½ çš„ç—‡ç‹€ï¼š\")\n",
        "\n",
        "    # å°ç—‡æœå°‹ï¼ˆç°¡å–®é—œéµå­—æ¯”å°ï¼‰\n",
        "    matched_herbs = []\n",
        "    for name, effect in herb_dict.items():\n",
        "        if symptom in effect:\n",
        "            matched_herbs.append((name, effect))\n",
        "\n",
        "    if matched_herbs:\n",
        "        print(f\"ğŸ” æ ¹æ“šç—‡ç‹€ã€Œ{symptom}ã€ï¼Œæ¨è–¦ä»¥ä¸‹è—¥è‰ï¼š\")\n",
        "        for name, effect in matched_herbs:\n",
        "            print(f\"- {name}ï¼š{effect}\")\n",
        "    else:\n",
        "        print(\"âŒ æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„è—¥è‰ï¼Œè«‹å˜—è©¦å…¶ä»–æè¿°\")\n",
        "\n",
        "else:\n",
        "    print(f\"âŒ ç„¡æ³•é€£ç·šåˆ° {site_name}ï¼Œç‹€æ…‹ç¢¼ï¼š\", res.status_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£ Tesseract èˆ‡ç¹ä¸­èªè¨€åŒ… + python-docx\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "# è·¯å¾‘è¨­å®š\n",
        "zip_path = \"/content/ilovepdf_pages-to-jpg.zip\"  # â† å°‡ä½ çš„ZIPä¸Šå‚³å¾Œï¼Œç¢ºèªæª”å\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# è§£å£“\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# è’é›†ä¸¦ä¾é ç¢¼æ’åº\n",
        "imgs = [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")]\n",
        "imgs = sorted(imgs, key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1)))\n",
        "\n",
        "# æŒ‡å®šç¹ä¸­èªè¨€\n",
        "lang = \"chi_tra\"\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    # è®€åœ–+ç°éš\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    # OCRï¼ˆå¯è¦–éœ€è¦åŠ å…¥ config åƒæ•¸å¾®èª¿ï¼‰\n",
        "    text = pytesseract.image_to_string(img, lang=lang).strip()\n",
        "\n",
        "    # ç”¢ç”Ÿé€é  DOCX\n",
        "    doc = Document()\n",
        "    try:\n",
        "        doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "        doc.styles['Normal'].font.size = Pt(12)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    doc.add_heading(f\"é€å­—ç¨¿ï¼šç¬¬ {page_no} é \", level=1)\n",
        "    doc.add_paragraph(\"å‚™è¨»ï¼šè™•ç†ç”¨ Colab é€£çµï¼ˆç”±ä½¿ç”¨è€…æä¾›ï¼‰ï¼š\\n\"\n",
        "                      \"https://colab.research.google.com/drive/1Iw1gscK2W6UV5QlL__TG2UT_J9UI7F-v\")\n",
        "    doc.add_heading(\"è¾¨è­˜æ–‡å­—\", level=2)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "print(\"å®Œæˆã€‚é€é  DOCX å·²è¼¸å‡ºè‡³ï¼š\", out_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMuG-7_gO5QB",
        "outputId": "8ad146ef-7bbf-47c5-8fe9-cfc84241ff13"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,340 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,942 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,272 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,297 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [80.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,209 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,783 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [42.7 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,576 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,543 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,617 kB]\n",
            "Fetched 35.1 MB in 4s (10.0 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-chi-tra\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 1,586 kB of archives.\n",
            "After this operation, 2,382 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-tra all 1:4.00~git30-7274cfa-1.1 [1,586 kB]\n",
            "Fetched 1,586 kB in 1s (1,837 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-chi-tra.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-chi-tra_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-chi-tra (1:4.00~git30-7274cfa-1.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, pytesseract\n",
            "Successfully installed pytesseract-0.3.13 python-docx-1.2.0\n",
            "å®Œæˆã€‚é€é  DOCX å·²è¼¸å‡ºè‡³ï¼š /content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== å®‰è£ OCR ç’°å¢ƒ ==========\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "# ========== è§£å£“ç¸®ä¸Šå‚³çš„ zip ==========\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "zip_path = \"/content/ilovepdf_pages-to-jpg.zip\"  # â† ä¸Šå‚³çš„æª”æ¡ˆåç¨±\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# ========== OCR é€é è½‰ DOCX ==========\n",
        "imgs = [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")]\n",
        "imgs = sorted(imgs, key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1)))\n",
        "\n",
        "lang = \"chi_tra\"  # ä½¿ç”¨ç¹é«”ä¸­æ–‡ OCR\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    # é–‹å•Ÿåœ–ç‰‡ï¼ˆç°éšåŒ–ä»¥æé«˜è¾¨è­˜ç‡ï¼‰\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    text = pytesseract.image_to_string(img, lang=lang, config=\"--psm 6\").strip()\n",
        "\n",
        "    # å»ºç«‹ DOCX\n",
        "    doc = Document()\n",
        "    try:\n",
        "        doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "        doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "        doc.styles['Normal'].font.size = Pt(12)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    doc.add_heading(f\"é€å­—ç¨¿ï¼šç¬¬ {page_no} é \", level=1)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "print(\"âœ… å®Œæˆï¼é€é  DOCX å·²è¼¸å‡ºåˆ°ï¼š\", out_dir)\n",
        "\n",
        "# ========== æ‰“åŒ…æˆ zipï¼Œæ–¹ä¾¿ä¸‹è¼‰ ==========\n",
        "zip_out = \"/content/ocr_pages_docx.zip\"\n",
        "with zipfile.ZipFile(zip_out, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir(out_dir):\n",
        "        zf.write(os.path.join(out_dir, f), arcname=f)\n",
        "\n",
        "print(\"âœ… ä¸‹è¼‰ ZIPï¼š\", zip_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqXm6fynUzD5",
        "outputId": "ba91913a-da54-4fe9-f410-f34931147245"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-chi-tra is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "âœ… å®Œæˆï¼é€é  DOCX å·²è¼¸å‡ºåˆ°ï¼š /content/output\n",
            "âœ… ä¸‹è¼‰ ZIPï¼š /content/ocr_pages_docx.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å®‰è£ OCR ç’°å¢ƒï¼ˆå«ç¹é«”ä¸­æ–‡ï¼‰\n",
        "!apt-get update -y\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-chi-tra\n",
        "!pip install pytesseract python-docx Pillow\n",
        "\n",
        "# ä¸Šå‚³ä½ çš„ ilovepdf_pages-to-jpg.zip\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os, re, zipfile\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "extract_dir = \"/content/input\"\n",
        "out_dir = \"/content/output\"\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "    zf.extractall(extract_dir)\n",
        "\n",
        "# OCR é€é è½‰ DOCX\n",
        "imgs = sorted(\n",
        "    [f for f in os.listdir(extract_dir) if f.lower().endswith(\".jpg\")],\n",
        "    key=lambda x: int(re.search(r'page-(\\d+)\\.jpg$', x).group(1))\n",
        ")\n",
        "\n",
        "for f in imgs:\n",
        "    page_no = int(re.search(r'page-(\\d+)\\.jpg$', f).group(1))\n",
        "    img_path = os.path.join(extract_dir, f)\n",
        "\n",
        "    img = Image.open(img_path).convert(\"L\")\n",
        "    text = pytesseract.image_to_string(img, lang=\"chi_tra\", config=\"--psm 6\").strip()\n",
        "\n",
        "    doc = Document()\n",
        "    doc.styles['Normal'].font.name = 'PMingLiU'\n",
        "    doc.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), 'PMingLiU')\n",
        "    doc.styles['Normal'].font.size = Pt(12)\n",
        "\n",
        "    doc.add_heading(f\"é€å­—ç¨¿ï¼šç¬¬ {page_no} é \", level=1)\n",
        "    doc.add_paragraph(text)\n",
        "\n",
        "    out_path = os.path.join(out_dir, f\"page-{page_no:04d}.docx\")\n",
        "    doc.save(out_path)\n",
        "\n",
        "# æ‰“åŒ…æˆ zipï¼Œæ–¹ä¾¿ä¸‹è¼‰\n",
        "zip_out = \"/content/ocr_pages_docx.zip\"\n",
        "with zipfile.ZipFile(zip_out, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "    for f in os.listdir(out_dir):\n",
        "        zf.write(os.path.join(out_dir, f), arcname=f)\n",
        "\n",
        "files.download(zip_out)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "WqhSPrEvU8KR",
        "outputId": "66c41160-2d4c-4da3-8066-1425674e0bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r            \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.82)] [Connected to cloud.r-pr\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "tesseract-ocr-chi-tra is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8c1c3d9-f6c6-47d4-929c-1ddf60288bf2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e8c1c3d9-f6c6-47d4-929c-1ddf60288bf2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_74GDsyRNJfc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "\n",
        "# æ¨¡æ“¬ç€è¦½å™¨æ¨™é ­\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# å‡æƒ³è³‡æ–™ä¾†æºï¼ˆé€™è£¡æ›æˆä½ çš„çœŸå¯¦ç¶²å€ï¼‰\n",
        "url = \"https://example-herb-database.com\"\n",
        "\n",
        "# å„²å­˜è³‡æ–™åº«æª”æ¡ˆ\n",
        "json_file = \"shen_nong_herbs.json\"\n",
        "\n",
        "# Step 1: çˆ¬èŸ²æ”¶é›†è—¥è‰è³‡æ–™ä¸¦å„²å­˜æˆ JSON\n",
        "def scrape_and_save():\n",
        "    print(\"ğŸ“¡ é–‹å§‹çˆ¬å–è—¥è‰è³‡æ–™...\")\n",
        "    res = requests.get(url, headers=headers)\n",
        "    if res.status_code != 200:\n",
        "        print(\"âŒ ç„¡æ³•é€£ç·šåˆ°è³‡æ–™ä¾†æºï¼Œç‹€æ…‹ç¢¼ï¼š\", res.status_code)\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "    # å‡è¨­æ¯å€‹è—¥è‰è³‡è¨Šåœ¨ <div class=\"herb\">\n",
        "    herbs = []\n",
        "    for herb in soup.select(\".herb\"):\n",
        "        name = herb.select_one(\".name\").text.strip()\n",
        "        effect = herb.select_one(\".effect\").text.strip()\n",
        "        usage = herb.select_one(\".usage\").text.strip() if herb.select_one(\".usage\") else \"ç„¡è³‡æ–™\"\n",
        "        herbs.append({\n",
        "            \"name\": name,\n",
        "            \"effect\": effect,\n",
        "            \"usage\": usage\n",
        "        })\n",
        "\n",
        "    # å„²å­˜æˆæœ¬åœ° JSON\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(herbs, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"âœ… å·²å„²å­˜ {len(herbs)} ç­†è—¥è‰è³‡æ–™åˆ° {json_file}\")\n",
        "\n",
        "# Step 2: å¾æœ¬åœ° JSON è¼‰å…¥è—¥è‰è³‡æ–™\n",
        "def load_herbs():\n",
        "    if not os.path.exists(json_file):\n",
        "        print(\"âš ï¸ æ‰¾ä¸åˆ°æœ¬åœ°è³‡æ–™åº«ï¼Œè«‹å…ˆåŸ·è¡Œçˆ¬èŸ²æ”¶é›†è³‡æ–™ã€‚\")\n",
        "        return []\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Step 3: äº’å‹•å¼æœ›èå•åˆ‡å•ç­”ç³»çµ±\n",
        "def diagnosis_system(herbs):\n",
        "    print(\"ğŸŒ¿ æ­¡è¿ä¾†åˆ°ç¥è¾²åšç™¾è‰è¨ºæ–·ç³»çµ± ğŸŒ¿\")\n",
        "    print(\"è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆå¯è¼¸å…¥ç°¡çŸ­æè¿°ï¼‰ï¼š\")\n",
        "\n",
        "    # å››è¨º\n",
        "    symptom = input(\"1ï¸âƒ£ è«‹æè¿°ä¸»è¦ç—‡ç‹€ï¼š\")\n",
        "    tongue = input(\"2ï¸âƒ£ èˆŒè‹”é¡è‰²/ç‹€æ…‹ï¼ˆå¦‚ç™½ã€é»ƒã€åšã€è–„ï¼‰ï¼š\")\n",
        "    pulse = input(\"3ï¸âƒ£ è„ˆè±¡ï¼ˆå¦‚å¿«ã€æ…¢ã€è™›ã€å¯¦ï¼‰ï¼š\")\n",
        "    extra = input(\"4ï¸âƒ£ å…¶ä»–ç‹€æ³ï¼ˆå¯è·³éï¼‰ï¼š\")\n",
        "\n",
        "    # ç°¡å–®æ¯”å°é—œéµå­—\n",
        "    matched = []\n",
        "    for herb in herbs:\n",
        "        text = herb[\"effect\"] + herb[\"usage\"]\n",
        "        if any(keyword in text for keyword in [symptom, tongue, pulse, extra]):\n",
        "            matched.append(herb)\n",
        "\n",
        "    # é¡¯ç¤ºçµæœ\n",
        "    if matched:\n",
        "        print(\"\\nğŸ” æ ¹æ“šä½ çš„æè¿°ï¼Œå»ºè­°ä»¥ä¸‹è—¥è‰ï¼š\")\n",
        "        for herb in matched:\n",
        "            print(f\"- {herb['name']}ï¼š{herb['effect']}ï¼ˆç”¨æ³•ï¼š{herb['usage']}ï¼‰\")\n",
        "    else:\n",
        "        print(\"\\nâŒ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è—¥è‰ï¼Œå»ºè­°æ›´æ›æè¿°æˆ–å¢åŠ ç´°ç¯€ã€‚\")\n",
        "\n",
        "# Step 4: ä¸»ç¨‹å¼æµç¨‹\n",
        "if __name__ == \"__main__\":\n",
        "    # å¦‚æœè³‡æ–™åº«ä¸å­˜åœ¨ï¼Œå…ˆçˆ¬èŸ²æŠ“è³‡æ–™ï¼ˆç¤ºç¯„å‡è³‡æ–™ï¼‰\n",
        "    if not os.path.exists(json_file):\n",
        "        print(\"âš ï¸ æ‰¾ä¸åˆ°æœ¬åœ°è³‡æ–™åº«ï¼Œå°‡ä½¿ç”¨ç¤ºç¯„è³‡æ–™...\")\n",
        "        sample_data = [\n",
        "            {\"name\": \"ç”˜è‰\", \"effect\": \"æ¸…ç†±è§£æ¯’ã€èª¿å’Œè«¸è—¥\", \"usage\": \"ç…æ¹¯æœç”¨ 5-10 å…‹\"},\n",
        "            {\"name\": \"é»ƒèŠ©\", \"effect\": \"æ¸…ç†±ç‡¥æ¿•ã€ç€‰ç«è§£æ¯’\", \"usage\": \"ç…æ¹¯æœç”¨ 3-9 å…‹\"},\n",
        "            {\"name\": \"ç•¶æ­¸\", \"effect\": \"è£œè¡€æ´»è¡€ã€èª¿ç¶“æ­¢ç—›\", \"usage\": \"ç…æ¹¯æœç”¨ 5-15 å…‹\"},\n",
        "            {\"name\": \"è–„è·\", \"effect\": \"ç–é¢¨æ•£ç†±ã€æ¸…åˆ©å’½å–‰\", \"usage\": \"ç…æ¹¯æœç”¨ 3-6 å…‹\"}\n",
        "        ]\n",
        "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # è¼‰å…¥è³‡æ–™åº«\n",
        "    herbs = load_herbs()\n",
        "\n",
        "    # å•Ÿå‹•è¨ºæ–·ç³»çµ±\n",
        "    diagnosis_system(herbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rNVtYEfNZQr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "def fetch_herb(name):\n",
        "    url = \"https://apis.tianapi.com/zhongyao/index\"\n",
        "    res = requests.get(url, params={\"key\": \"ä½ çš„APIKEY\", \"word\": name})\n",
        "    return res.json()  # å¯è§£æåŠŸæ•ˆã€è³‡æ–™ç­‰\n",
        "\n",
        "# ç¤ºä¾‹ï¼šç”¨â€œé‡‘éŠ€èŠ±â€ã€è®Šæˆä¸€ç­† herb dictï¼Œå­˜ JSONâ€¦ å¯æ‰¹é‡è¼¸å…¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fg17Fn5Nf3q"
      },
      "outputs": [],
      "source": [
        "# åƒè€ƒ zhongyaocai.com çˆ¬èŸ²å°ˆæ¡ˆé‚è¼¯ï¼ˆç°¡å–®åˆ†é¡ + åˆ†é ï¼‰\n",
        "# å†å¯«çˆ¬èŸ²æŠŠè³‡æ–™æŠ“ä¸‹ä¾†ï¼Œæ•´ç†æˆ dict listï¼Œå­˜æˆ JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZBneRmHNi6i"
      },
      "outputs": [],
      "source": [
        "with open(\"shen_nong_herbs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    herbs = json.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXn4wwYCNmSq"
      },
      "outputs": [],
      "source": [
        "import requests, json, os\n",
        "\n",
        "# ä½ å¯ä»¥å…ˆæ±ºå®šç”¨å“ªå€‹è³‡æ–™æºï¼Œæˆ‘é€™è£ä»¥ TianAPI ç‚ºä¾‹ï¼š\n",
        "\n",
        "API_KEY = \"ä½ çš„APIKEY\"\n",
        "API_URL = \"https://apis.tianapi.com/zhongyao/index\"\n",
        "\n",
        "def fetch_herb(name):\n",
        "    res = requests.get(API_URL, params={\"key\": API_KEY, \"word\": name})\n",
        "    return res.json().get(\"result\")  # ä¾‹ï¼šå¯èƒ½æœ‰ title, content ç­‰æ¬„ä½\n",
        "\n",
        "def build_db(names_list, json_file=\"shen_nong_herbs.json\"):\n",
        "    herbs = []\n",
        "    for nm in names_list:\n",
        "        data = fetch_herb(nm)\n",
        "        if data:\n",
        "            herbs.append({\n",
        "                \"name\": data.get(\"title\", nm),\n",
        "                \"effect\": data.get(\"content\", \"\"),\n",
        "                \"usage\": data.get(\"usage\", \"\")\n",
        "            })\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(herbs, f, ensure_ascii=False, indent=2)\n",
        "    print(f\"å„²å­˜ {len(herbs)} ç­†è—¥è‰è³‡æ–™åˆ° {json_file}\")\n",
        "\n",
        "def load_herbs(json_file=\"shen_nong_herbs.json\"):\n",
        "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def diagnosis_system(herbs):\n",
        "    print(\"ç¥è¾²åšç™¾è‰è¨ºæ–·ç³»çµ±\")\n",
        "    symptom = input(\"ç—‡ç‹€ï¼š\")\n",
        "    tongue = input(\"èˆŒè‹”ï¼š\")\n",
        "    pulse = input(\"è„ˆè±¡ï¼š\")\n",
        "    extra = input(\"å…¶ä»–ï¼š\")\n",
        "    matched = [h for h in herbs if any(k in (h[\"effect\"]+h.get(\"usage\",\"\")) for k in [symptom, tongue, pulse, extra])]\n",
        "    if matched:\n",
        "        for h in matched:\n",
        "            print(f\"- {h['name']}: {h['effect']} ç”¨æ³•ï¼š{h.get('usage','')}\")\n",
        "    else:\n",
        "        print(\"æ²’æ‰¾åˆ°é©åˆçš„è—¥è‰\")\n",
        "\n",
        "# ç¯„ä¾‹å•Ÿå‹•é †åº\n",
        "if __name__ == \"__main__\":\n",
        "    names = [\"é‡‘éŠ€èŠ±\", \"é»ƒèŠ©\", \"ç”˜è‰\"]  # å…ˆè©¦è©¦çœ‹\n",
        "    if not os.path.exists(\"shen_nong_herbs.json\"):\n",
        "        build_db(names)\n",
        "    herbs = load_herbs()\n",
        "    diagnosis_system(herbs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImR1mqSeNv_i"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"ç”˜è‰\",\n",
        "  \"effect\": \"æ¸…ç†±è§£æ¯’ã€èª¿å’Œè«¸è—¥\",\n",
        "  \"usage\": \"ç…æ¹¯æœç”¨ 5-10 å…‹\",\n",
        "  \"source\": \"TianAPI\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RBTkeToNzAh"
      },
      "outputs": [],
      "source": [
        "- ç”˜è‰ï¼ˆä¾†æºï¼šTianAPIï¼‰ï¼šæ¸…ç†±è§£æ¯’ã€èª¿å’Œè«¸è—¥ï¼Œç”¨æ³•ï¼šç…æ¹¯æœç”¨ 5-10 å…‹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWUk9EZtN9kw"
      },
      "outputs": [],
      "source": [
        "fetch_from_tianapi(name)\n",
        "fetch_from_thinkapi(name)\n",
        "fetch_from_local_spider(name)\n",
        "fetch_from_hkbu(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwyL0I0oODv4"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ç¥è¾²åšç™¾è‰ Â· å¤šä¾†æºåˆä½µç‰ˆ\n",
        "å¯ç›´æ¥åœ¨ Colab / PyCharm åŸ·è¡Œ\n",
        "\n",
        "åŠŸèƒ½ç¸½è¦½ï¼š\n",
        "1) å¤šä¾†æºæ”¶é›†ï¼šTianAPI / ThinkAPI / æœ¬åœ°çˆ¬èŸ² / HKBU åœ–åº«ï¼ˆæä¾›æ“´å……å‡½å¼ä»‹é¢ï¼‰\n",
        "2) æœ¬åœ°è³‡æ–™åº«ï¼šshen_nong_herbs.jsonï¼ˆè‡ªå‹•å»é‡åˆä½µã€å«ä¾†æºæ¨™è¨˜èˆ‡åœ–ç‰‡ï¼‰\n",
        "3) æœ›èå•åˆ‡äº’å‹•ï¼šç—‡ç‹€/èˆŒè‹”/è„ˆè±¡/å…¶ä»– â†’ é—œéµè©æ¯”å° + åŒç¾©è©æ“´å……\n",
        "4) ç¥è¾²æ°è—¥æˆ¿ï¼šç°¡å–®è¦å‰‡å¼•æ“ â†’ æ¨è–¦è—¥è‰ + ç¤ºç¯„é…æ–¹ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰\n",
        "\n",
        "ä¾è³´ï¼š\n",
        "- requests, beautifulsoup4ï¼ˆåªæœ‰åœ¨å•Ÿç”¨ API/çˆ¬èŸ²æ™‚éœ€è¦ï¼‰\n",
        "- ç„¡å¤–ç¶²æ™‚æœƒè‡ªå‹•ä½¿ç”¨å…§å»ºç¤ºç¯„æ¨£æœ¬\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "\n",
        "# ========== åŸºæœ¬è¨­å®š ==========\n",
        "\n",
        "DB_FILE = \"shen_nong_herbs.json\"\n",
        "\n",
        "CONFIG = {\n",
        "    \"enable_sources\": {\n",
        "        \"tianapi\": False,   # æœ‰ API Key å†æ‰“é–‹\n",
        "        \"thinkapi\": False,  # æœ‰ API Key å†æ‰“é–‹\n",
        "        \"local_spider\": False,  # å°Šé‡å°æ–¹ robots.txtï¼Œç¢ºå®šå…è¨±å†æ‰“é–‹\n",
        "        \"hkbu_images\": False    # éœ€ä¾æˆæ¬Š/ä½¿ç”¨æ¢æ¬¾\n",
        "    },\n",
        "    \"api_keys\": {\n",
        "        \"tianapi\": \"PUT_YOUR_TIANAPI_KEY_HERE\",\n",
        "        \"thinkapi\": \"PUT_YOUR_THINKAPI_KEY_HERE\"\n",
        "    },\n",
        "    # æŸ¥è©¢æ¯”å°æ¬„ä½\n",
        "    \"match_fields\": [\"effect\", \"usage\", \"props\", \"indications\", \"functions\", \"notes\"],\n",
        "    # ç°¡æ˜“åŒç¾©è©ï¼ˆå¯è‡ªè¡Œæ“´å……ï¼‰\n",
        "    \"synonyms\": {\n",
        "        \"å–‰åš¨ç—›\": [\"å’½ç—›\", \"å’½å–‰è…«ç—›\", \"å’½ä¹¾\", \"å’½ç™¢\"],\n",
        "        \"ç™¼ç†±\": [\"ç™¼ç‡’\", \"èº«ç†±\", \"å£¯\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlZDjYgWOJCY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# å‡è£çˆ¬åˆ°çš„å¤šä¾†æºè³‡æ–™\n",
        "data_sources = [\n",
        "    {\n",
        "        \"name\": \"äººåƒ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"è£œæ°£\", \"å®‰ç¥\", \"æŠ—ç–²å‹\"],\n",
        "        \"source\": \"ä¸­é†«è—¥ç™¾ç§‘\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"é»ƒèŠª\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"è£œæ°£å›ºè¡¨\", \"åˆ©å°¿æ¶ˆè…«\", \"ä¿ƒé€²å…ç–«\"],\n",
        "        \"source\": \"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"æ¸æ\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"è£œè‚è…\", \"ç›Šç²¾æ˜ç›®\"],\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# å„²å­˜æˆæœ¬åœ° JSON\n",
        "with open(\"shennong.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(data_sources, f, ensure_ascii=False, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taXgn9p-ORr_"
      },
      "outputs": [],
      "source": [
        "def search_herb(keyword):\n",
        "    with open(\"shennong.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "\n",
        "    results = [h for h in herbs if keyword in h[\"name\"] or keyword in h[\"pinyin\"]]\n",
        "    return results\n",
        "\n",
        "# æ¸¬è©¦æŸ¥è©¢\n",
        "keyword = \"äººåƒ\"\n",
        "result = search_herb(keyword)\n",
        "for r in result:\n",
        "    print(f\"åç¨±: {r['name']}\")\n",
        "    print(f\"åŠŸæ•ˆ: {', '.join(r['effects'])}\")\n",
        "    print(f\"ä¾†æº: {r['source']}\")\n",
        "    print(f\"åœ–ç‰‡: {r['image']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvjk5AyoOWRP"
      },
      "outputs": [],
      "source": [
        "def shennong_bot():\n",
        "    print(\"ğŸŒ¿ æ­¡è¿ä¾†åˆ°ç¥è¾²ç™¾è‰å•ç­”ç³»çµ±ï¼Œè¼¸å…¥è—¥æåç¨±æˆ–æ‹¼éŸ³æŸ¥è©¢ï¼Œè¼¸å…¥ q é›¢é–‹\")\n",
        "    while True:\n",
        "        q = input(\"è«‹è¼¸å…¥è—¥æåç¨±: \").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ å†è¦‹ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "\n",
        "        results = search_herb(q)\n",
        "        if results:\n",
        "            for r in results:\n",
        "                print(f\"ğŸŒ± {r['name']}ï¼š{', '.join(r['effects'])}ï¼ˆä¾†æº: {r['source']}ï¼‰\")\n",
        "        else:\n",
        "            print(\"âŒ æ‰¾ä¸åˆ°è©²è—¥æï¼Œè«‹ç¢ºèªæ‹¼å¯«ã€‚\")\n",
        "\n",
        "# shennong_bot() # åŸ·è¡Œäº’å‹•ç³»çµ±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ytwxk9DOZ0_"
      },
      "outputs": [],
      "source": [
        "<!DOCTYPE html>\n",
        "<html lang=\"zh-Hant\">\n",
        "<head>\n",
        "<meta charset=\"UTF-8\">\n",
        "<title>ç¥è¾²ç™¾è‰æŸ¥è©¢</title>\n",
        "</head>\n",
        "<body>\n",
        "<h1>ğŸŒ¿ ç¥è¾²ç™¾è‰æŸ¥è©¢ç³»çµ±</h1>\n",
        "<input type=\"text\" id=\"searchInput\" placeholder=\"è¼¸å…¥è—¥æåç¨±æˆ–æ‹¼éŸ³\">\n",
        "<button onclick=\"searchHerb()\">æŸ¥è©¢</button>\n",
        "<div id=\"result\"></div>\n",
        "\n",
        "<script>\n",
        "async function searchHerb() {\n",
        "    const keyword = document.getElementById(\"searchInput\").value;\n",
        "    const res = await fetch(\"shennong.json\");\n",
        "    const herbs = await res.json();\n",
        "\n",
        "    const results = herbs.filter(h => h.name.includes(keyword) || h.pinyin.includes(keyword));\n",
        "    let html = \"\";\n",
        "    if (results.length > 0) {\n",
        "        results.forEach(r => {\n",
        "            html += `<h2>${r.name}</h2><p>åŠŸæ•ˆ: ${r.effects.join(\", \")}</p><p>ä¾†æº: ${r.source}</p><img src=\"${r.image}\" width=\"150\">`;\n",
        "        });\n",
        "    } else {\n",
        "        html = \"<p>âŒ æ‰¾ä¸åˆ°è©²è—¥æ</p>\";\n",
        "    }\n",
        "    document.getElementById(\"result\").innerHTML = html;\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD3p3zCAOf83"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"åç¨±\": \"è–„è·\",\n",
        "  \"åŠŸæ•ˆ\": [\"æ¸…ç†±\", \"è§£æ¯’\", \"ç–é¢¨\"],\n",
        "  \"ä½¿ç”¨éƒ¨ä½\": \"è‘‰\",\n",
        "  \"å¸¸è¦‹é…æ–¹\": [\"è–„è·èŒ¶\", \"è–„è·æ²¹\"],\n",
        "  \"ä¾†æº\": \"æœ¬è‰ç¶±ç›®\",\n",
        "  \"æœ€å¾Œæ›´æ–°\": \"2025-08-14\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJrkwUE5O82V"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "\n",
        "# ======================\n",
        "# æ­·å² + ç¾ä»£ç¤ºç¯„è³‡æ–™\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\n",
        "        \"name\": \"äººåƒ\",\n",
        "        \"ancient_name\": \"äººåƒ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"è£œæ°£\", \"å®‰ç¥\", \"æŠ—ç–²å‹\"],\n",
        "        \"usage\": \"ç…æ¹¯æœç”¨ 3-10 å…‹\",\n",
        "        \"source\": \"ç¥è¾²æœ¬è‰ç¶“\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"é»ƒèŠª\",\n",
        "        \"ancient_name\": \"é»ƒè€†\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"è£œæ°£å›ºè¡¨\", \"åˆ©å°¿æ¶ˆè…«\", \"ä¿ƒé€²å…ç–«\"],\n",
        "        \"usage\": \"ç…æ¹¯æœç”¨ 10-15 å…‹\",\n",
        "        \"source\": \"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"æ¸æ\",\n",
        "        \"ancient_name\": \"æ¸æå­\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"è£œè‚è…\", \"ç›Šç²¾æ˜ç›®\"],\n",
        "        \"usage\": \"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# ======================\n",
        "# å„²å­˜æœ¬åœ° JSON è³‡æ–™åº«\n",
        "# ======================\n",
        "DB_FILE = \"shennong_ai.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… å·²å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼š{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# æŸ¥è©¢å‡½å¼\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in h.get(k,\"\") for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# äº’å‹•å•ç­”ç³»çµ±ï¼ˆæœ›èå•åˆ‡æ¨¡æ“¬ï¼‰\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ æ­¡è¿ä¾†åˆ°ç¾ä»£ AI ç¥è¾²æ°ç³»çµ±ï¼è¼¸å…¥ q é›¢é–‹\")\n",
        "    while True:\n",
        "        q = input(\"è«‹è¼¸å…¥ç—‡ç‹€ã€è‰è—¥åç¨±æˆ–æ‹¼éŸ³ï¼š\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ å†è¦‹ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "        matches = search_herb(q)\n",
        "        if matches:\n",
        "            for m in matches:\n",
        "                print(f\"\\nğŸŒ± {m['name']}ï¼ˆå¤åï¼š{m['ancient_name']}ï¼‰\")\n",
        "                print(f\"åŠŸæ•ˆ: {', '.join(m['effects'])}\")\n",
        "                print(f\"ç”¨æ³•: {m['usage']}\")\n",
        "                print(f\"ä¾†æº: {m['source']}\")\n",
        "                print(f\"åœ–ç‰‡: {m['image']}\")\n",
        "        else:\n",
        "            print(\"âŒ æ‰¾ä¸åˆ°ç›¸é—œè—¥æï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–æ›å€‹é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•ç³»çµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KylqzB8LPHqu"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\n",
        "        \"name\": \"äººåƒ\",\n",
        "        \"ancient_name\": \"äººåƒ\",\n",
        "        \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"è£œæ°£\", \"å®‰ç¥\", \"æŠ—ç–²å‹\"],\n",
        "        \"usage\": \"ç…æ¹¯æœç”¨ 3-10 å…‹\",\n",
        "        \"source\": \"ç¥è¾²æœ¬è‰ç¶“\",\n",
        "        \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"é»ƒèŠª\",\n",
        "        \"ancient_name\": \"é»ƒè€†\",\n",
        "        \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"è£œæ°£å›ºè¡¨\", \"åˆ©å°¿æ¶ˆè…«\", \"ä¿ƒé€²å…ç–«\"],\n",
        "        \"usage\": \"ç…æ¹¯æœç”¨ 10-15 å…‹\",\n",
        "        \"source\": \"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\n",
        "        \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"æ¸æ\",\n",
        "        \"ancient_name\": \"æ¸æå­\",\n",
        "        \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"è£œè‚è…\", \"ç›Šç²¾æ˜ç›®\"],\n",
        "        \"usage\": \"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\n",
        "        \"source\": \"TianAPI\",\n",
        "        \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"è–„è·\",\n",
        "        \"ancient_name\": \"è–„è·\",\n",
        "        \"pinyin\": \"bohe\",\n",
        "        \"effects\": [\"ç–é¢¨æ¸…ç†±\", \"åˆ©å’½\", \"é ­ç—›\"],\n",
        "        \"usage\": \"æ³¡èŒ¶æˆ–å…¥æ–¹ 3-6 å…‹\",\n",
        "        \"source\": \"æœ¬è‰ç¶±ç›®\",\n",
        "        \"image\": \"https://example.com/bohe.jpg\"\n",
        "    }\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æœ¬åœ°è³‡æ–™åº«å·²å»ºç«‹ï¼š{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2ï¸âƒ£ æŸ¥è©¢åŠŸèƒ½\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3ï¸âƒ£ AI é…æ–¹ç”Ÿæˆç¤ºç¯„\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    \"\"\"\n",
        "    è¼¸å…¥ç—‡ç‹€åˆ—è¡¨ â†’ æ¨è–¦è‰è—¥çµ„åˆ\n",
        "    ç°¡å–®è¦å‰‡ï¼šç—‡ç‹€èˆ‡è‰è—¥åŠŸæ•ˆåŒ¹é…\n",
        "    \"\"\"\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for symptom in symptoms:\n",
        "        matched = [h for h in herbs if symptom in h[\"effects\"]]\n",
        "        if matched:\n",
        "            chosen = random.choice(matched)  # è‹¥å¤šå€‹åŒ¹é…ï¼Œéš¨æ©Ÿé¸ä¸€å€‹\n",
        "            formula.append(chosen)\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 4ï¸âƒ£ äº’å‹•å•ç­”ç³»çµ±ï¼ˆæœ›èå•åˆ‡æ¨¡æ“¬ï¼‰\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ æ­¡è¿ä¾†åˆ°ç•¶ä»£ AI ç¥è¾²æ°ç³»çµ±ï¼è¼¸å…¥ q é›¢é–‹\")\n",
        "    print(\"ä½ å¯ä»¥è¼¸å…¥ç—‡ç‹€ã€è‰è—¥åç¨±æˆ–æ‹¼éŸ³ä¾†æŸ¥è©¢è‰è—¥ï¼Œæˆ–è¼¸å…¥ 'é…æ–¹' ç”Ÿæˆè‰è—¥çµ„åˆ\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nè«‹è¼¸å…¥ï¼š\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ å†è¦‹ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "        elif q.lower() == \"é…æ–¹\":\n",
        "            symptoms_input = input(\"è«‹è¼¸å…¥ç—‡ç‹€ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè£œæ°£,å®‰ç¥ï¼‰ï¼š\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nğŸ’Š æ¨è–¦è‰è—¥çµ„åˆï¼š\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, ç”¨æ³•: {f['usage']}, ä¾†æº: {f['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç¬¦åˆç—‡ç‹€çš„è‰è—¥\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nğŸŒ± æ‰¾åˆ° {len(matches)} ç­†è³‡æ–™ï¼š\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  åŠŸæ•ˆ: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  ç”¨æ³•: {m['usage']}\")\n",
        "                    print(f\"  ä¾†æº: {m['source']}\")\n",
        "                    print(f\"  åœ–ç‰‡: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç›¸é—œè—¥æï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–æ›å€‹é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•ç³»çµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3QtnuZTPMx9"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"äººåƒ\",\n",
        "  \"ancient_name\": \"äººåƒ\",\n",
        "  \"pinyin\": \"renshen\",\n",
        "  \"effects\": [\"è£œæ°£\", \"å®‰ç¥\", \"æŠ—ç–²å‹\"],\n",
        "  \"usage\": \"ç…æ¹¯æœç”¨ 3-10 å…‹\",\n",
        "  \"source\": \"ç¥è¾²æœ¬è‰ç¶“\",\n",
        "  \"image\": \"https://example.com/renshen.jpg\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaQhIiK2PRdd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()  # æ¯å€‹è‰è—¥çš„æ¨è–¦æ¦‚ç‡\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hidIa2yWPaRc"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…ç—‡ç‹€ â†’ ç‰¹å¾µå‘é‡ â†’ ç¥ç¶“ç¶²è·¯ â†’ è‰è—¥æ¦‚ç‡ â†’ é…æ–¹ç”Ÿæˆ â†’ é¡¯ç¤ºåŠŸæ•ˆ + ä¾†æº + åœ–ç‰‡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jttn8aaRPf-k"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"name\": \"æ¸æ\",\n",
        "  \"ancient_name\": \"æ¸æå­\",\n",
        "  \"pinyin\": \"gouqi\",\n",
        "  \"effects\": [\"è£œè‚è…\", \"ç›Šç²¾æ˜ç›®\"],\n",
        "  \"usage\": \"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\n",
        "  \"source\": \"TianAPI\",\n",
        "  \"image\": \"https://example.com/gouqi.jpg\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf9q0z5SPnZs"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…ç—‡ç‹€/ç…§ç‰‡ â†’ å‘é‡åŒ– â†’ ç¥ç¶“ç¶²è·¯åˆ†æ â†’ æ¨è–¦è‰è—¥ + é…æ–¹ â†’ é¡¯ç¤ºåŠŸæ•ˆ + åœ–ç‰‡ + è³‡æ–™ä¾†æº\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d-Had4lPxt8"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"äººåƒ\",\"ancient_name\":\"äººåƒ\",\"pinyin\":\"renshen\",\"effects\":[\"è£œæ°£\",\"å®‰ç¥\",\"æŠ—ç–²å‹\"],\"usage\":\"ç…æ¹¯æœç”¨ 3-10 å…‹\",\"source\":\"ç¥è¾²æœ¬è‰ç¶“\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"é»ƒèŠª\",\"ancient_name\":\"é»ƒè€†\",\"pinyin\":\"huangqi\",\"effects\":[\"è£œæ°£å›ºè¡¨\",\"åˆ©å°¿æ¶ˆè…«\",\"ä¿ƒé€²å…ç–«\"],\"usage\":\"ç…æ¹¯æœç”¨ 10-15 å…‹\",\"source\":\"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"æ¸æ\",\"ancient_name\":\"æ¸æå­\",\"pinyin\":\"gouqi\",\"effects\":[\"è£œè‚è…\",\"ç›Šç²¾æ˜ç›®\"],\"usage\":\"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"è–„è·\",\"ancient_name\":\"è–„è·\",\"pinyin\":\"bohe\",\"effects\":[\"ç–é¢¨æ¸…ç†±\",\"åˆ©å’½\",\"é ­ç—›\"],\"usage\":\"æ³¡èŒ¶æˆ–å…¥æ–¹ 3-6 å…‹\",\"source\":\"æœ¬è‰ç¶±ç›®\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æœ¬åœ°è³‡æ–™åº«å·²å»ºç«‹ï¼š{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2ï¸âƒ£ æŸ¥è©¢åŠŸèƒ½\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3ï¸âƒ£ å¤šå±¤ç¥ç¶“ç¶²è·¯ç¤ºç¯„ï¼ˆæ¨è–¦è‰è—¥ï¼‰\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# ç°¡å–®ç¤ºç¯„ï¼šè¼¸å…¥ç—‡ç‹€å‘é‡(é•·åº¦4) â†’ è¼¸å‡ºè‰è—¥æ¦‚ç‡(é•·åº¦4)\n",
        "input_size = 4\n",
        "hidden_size = 8\n",
        "output_size = 4\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4ï¸âƒ£ AI é…æ–¹ç”Ÿæˆç¤ºç¯„\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    # å°‡ç—‡ç‹€ç°¡å–®å‘é‡åŒ–ï¼šæ¯å€‹ç—‡ç‹€å°æ‡‰0æˆ–1\n",
        "    symptom_dict = {\"è£œæ°£\":0,\"å®‰ç¥\":1,\"æŠ—ç–²å‹\":2,\"è£œæ°£å›ºè¡¨\":3}\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()  # æ¨¡å‹è¼¸å‡ºè‰è—¥æ¨è–¦æ¦‚ç‡\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # ç°¡å–®é–¾å€¼\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5ï¸âƒ£ äº’å‹•å•ç­”ç³»çµ±ï¼ˆæœ›èå•åˆ‡æ¨¡æ“¬ï¼‰\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ æ­¡è¿ä¾†åˆ°å¤šå…ƒåŒ–ç•¶ä»£ AI ç¥è¾²æ°ç³»çµ±ï¼è¼¸å…¥ q é›¢é–‹\")\n",
        "    print(\"è¼¸å…¥ç—‡ç‹€ã€è‰è—¥åç¨±æˆ–æ‹¼éŸ³æŸ¥è©¢ï¼Œè¼¸å…¥ 'é…æ–¹' ç”Ÿæˆæ¨è–¦è‰è—¥çµ„åˆ\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nè«‹è¼¸å…¥ï¼š\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ å†è¦‹ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "        elif q.lower() == \"é…æ–¹\":\n",
        "            symptoms_input = input(\"è«‹è¼¸å…¥ç—‡ç‹€ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè£œæ°£,å®‰ç¥ï¼‰ï¼š\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nğŸ’Š æ¨è–¦è‰è—¥çµ„åˆï¼š\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, ç”¨æ³•: {f['usage']}, ä¾†æº: {f['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç¬¦åˆç—‡ç‹€çš„è‰è—¥\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nğŸŒ± æ‰¾åˆ° {len(matches)} ç­†è³‡æ–™ï¼š\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  åŠŸæ•ˆ: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  ç”¨æ³•: {m['usage']}\")\n",
        "                    print(f\"  ä¾†æº: {m['source']}\")\n",
        "                    print(f\"  åœ–ç‰‡: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç›¸é—œè—¥æï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–æ›å€‹é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•ç³»çµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K6RT8DoP7ST"
      },
      "outputs": [],
      "source": [
        "ğŸŒ¿ ç•¶ä»£å¤šå…ƒ AI ç¥è¾²æ°ç¾èº«ï¼\n",
        "ğŸ“œ å¤ä»Šæ™ºæ…§ç›¡æ”¶çœ¼åº•\n",
        "ğŸ’Š è‰è—¥æ¨è–¦å³æ™‚ç”Ÿæˆ\n",
        "ğŸ§  AI é…æ–¹æ™ºèƒ½åŒ¹é…\n",
        "ğŸ“¸ åœ–ç‰‡èˆ‡ä¾†æºæ¸…æ¥šå‘ˆç¾\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duL1bpcLP85b"
      },
      "source": [
        "ğŸŒ¿ ç•¶ä»£å¤šå…ƒ AI ç¥è¾²æ°ç¾èº«ï¼\n",
        "ğŸ“œ å¤ä»Šæ™ºæ…§ç›¡æ”¶çœ¼åº•\n",
        "ğŸ’Š è‰è—¥æ¨è–¦å³æ™‚ç”Ÿæˆ\n",
        "ğŸ§  AI é…æ–¹æ™ºèƒ½åŒ¹é…\n",
        "ğŸ“¸ åœ–ç‰‡èˆ‡ä¾†æºæ¸…æ¥šå‘ˆç¾\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OClf9LOQH6z"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"äººåƒ\",\"ancient_name\":\"äººåƒ\",\"pinyin\":\"renshen\",\"effects\":[\"è£œæ°£\",\"å®‰ç¥\",\"æŠ—ç–²å‹\"],\"usage\":\"ç…æ¹¯æœç”¨ 3-10 å…‹\",\"source\":\"ç¥è¾²æœ¬è‰ç¶“\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"é»ƒèŠª\",\"ancient_name\":\"é»ƒè€†\",\"pinyin\":\"huangqi\",\"effects\":[\"è£œæ°£å›ºè¡¨\",\"åˆ©å°¿æ¶ˆè…«\",\"ä¿ƒé€²å…ç–«\"],\"usage\":\"ç…æ¹¯æœç”¨ 10-15 å…‹\",\"source\":\"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"æ¸æ\",\"ancient_name\":\"æ¸æå­\",\"pinyin\":\"gouqi\",\"effects\":[\"è£œè‚è…\",\"ç›Šç²¾æ˜ç›®\"],\"usage\":\"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"è–„è·\",\"ancient_name\":\"è–„è·\",\"pinyin\":\"bohe\",\"effects\":[\"ç–é¢¨æ¸…ç†±\",\"åˆ©å’½\",\"é ­ç—›\"],\"usage\":\"æ³¡èŒ¶æˆ–å…¥æ–¹ 3-6 å…‹\",\"source\":\"æœ¬è‰ç¶±ç›®\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æœ¬åœ°è³‡æ–™åº«å·²å»ºç«‹ï¼š{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2ï¸âƒ£ æŸ¥è©¢åŠŸèƒ½\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3ï¸âƒ£ å¤šå±¤ç¥ç¶“ç¶²è·¯ç¤ºç¯„ï¼ˆè‰è—¥æ¨è–¦ï¼‰\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4  # ç¤ºä¾‹ç—‡ç‹€å‘é‡é•·åº¦\n",
        "hidden_size = 8\n",
        "output_size = 4  # è‰è—¥æ•¸é‡\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4ï¸âƒ£ AI é…æ–¹ç”Ÿæˆç¤ºç¯„ï¼ˆå®‰å…¨ç¯„åœï¼‰\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    symptom_dict = {\"è£œæ°£\":0,\"å®‰ç¥\":1,\"æŠ—ç–²å‹\":2,\"è£œæ°£å›ºè¡¨\":3}  # ç°¡å–®ç¤ºç¯„\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # æ¦‚ç‡é–¾å€¼\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5ï¸âƒ£ äº’å‹•å•ç­”ç³»çµ±ï¼ˆæœ›èå•åˆ‡æ¨¡æ“¬ï¼‰\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ ç¥è¾²æ°é™è‡¨ï¼ç•¶ä»£å¤šå…ƒ AI ç¥è¾²æ°ç¾èº«\")\n",
        "    print(\"è¼¸å…¥ç—‡ç‹€ã€è‰è—¥åç¨±æˆ–æ‹¼éŸ³æŸ¥è©¢ï¼Œè¼¸å…¥ 'é…æ–¹' ç”Ÿæˆæ¨è–¦è‰è—¥çµ„åˆï¼Œè¼¸å…¥ 'q' é›¢é–‹\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nè«‹è¼¸å…¥ï¼š\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ ç¥è¾²æ°æš«æ™‚é™è½ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "        elif q.lower() == \"é…æ–¹\":\n",
        "            symptoms_input = input(\"è«‹è¼¸å…¥ç—‡ç‹€ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè£œæ°£,å®‰ç¥ï¼‰ï¼š\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nğŸ’Š æ¨è–¦è‰è—¥çµ„åˆï¼ˆåƒ…é™ç¥è¾²æ°ç¯„åœï¼‰ï¼š\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, ç”¨æ³•: {f['usage']}, ä¾†æº: {f['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç¬¦åˆç—‡ç‹€çš„è‰è—¥\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nğŸŒ± æ‰¾åˆ° {len(matches)} ç­†è³‡æ–™ï¼š\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  åŠŸæ•ˆ: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  ç”¨æ³•: {m['usage']}\")\n",
        "                    print(f\"  ä¾†æº: {m['source']}\")\n",
        "                    print(f\"  åœ–ç‰‡: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç›¸é—œè—¥æï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–æ›å€‹é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•ç³»çµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c0-ZeGcQZKr"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"äººåƒ\",\"ancient_name\":\"äººåƒ\",\"pinyin\":\"renshen\",\"effects\":[\"è£œæ°£\",\"å®‰ç¥\",\"æŠ—ç–²å‹\"],\"usage\":\"ç…æ¹¯æœç”¨ 3-10 å…‹\",\"source\":\"ç¥è¾²æœ¬è‰ç¶“\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"é»ƒèŠª\",\"ancient_name\":\"é»ƒè€†\",\"pinyin\":\"huangqi\",\"effects\":[\"è£œæ°£å›ºè¡¨\",\"åˆ©å°¿æ¶ˆè…«\",\"ä¿ƒé€²å…ç–«\"],\"usage\":\"ç…æ¹¯æœç”¨ 10-15 å…‹\",\"source\":\"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"æ¸æ\",\"ancient_name\":\"æ¸æå­\",\"pinyin\":\"gouqi\",\"effects\":[\"è£œè‚è…\",\"ç›Šç²¾æ˜ç›®\"],\"usage\":\"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"è–„è·\",\"ancient_name\":\"è–„è·\",\"pinyin\":\"bohe\",\"effects\":[\"ç–é¢¨æ¸…ç†±\",\"åˆ©å’½\",\"é ­ç—›\"],\"usage\":\"æ³¡èŒ¶æˆ–å…¥æ–¹ 3-6 å…‹\",\"source\":\"æœ¬è‰ç¶±ç›®\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æœ¬åœ°è³‡æ–™åº«å·²å»ºç«‹ï¼š{DB_FILE}\")\n",
        "\n",
        "# ======================\n",
        "# 2ï¸âƒ£ æŸ¥è©¢åŠŸèƒ½\n",
        "# ======================\n",
        "def search_herb(keyword: str):\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = []\n",
        "    for h in herbs:\n",
        "        if any(keyword in str(h.get(k, \"\")) for k in [\"name\",\"ancient_name\",\"pinyin\",\"effects\",\"usage\"]):\n",
        "            results.append(h)\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3ï¸âƒ£ å¤šå±¤ç¥ç¶“ç¶²è·¯ç¤ºç¯„ï¼ˆè‰è—¥æ¨è–¦ï¼‰\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4  # ç—‡ç‹€å‘é‡é•·åº¦\n",
        "hidden_size = 8\n",
        "output_size = 4  # è‰è—¥æ•¸é‡\n",
        "model = ShennongNet(input_size, hidden_size, output_size)\n",
        "\n",
        "# ======================\n",
        "# 4ï¸âƒ£ AI é…æ–¹ç”Ÿæˆç¤ºç¯„ï¼ˆå®‰å…¨ç¯„åœï¼‰\n",
        "# ======================\n",
        "def generate_formula(symptoms: list):\n",
        "    symptom_dict = {\"è£œæ°£\":0,\"å®‰ç¥\":1,\"æŠ—ç–²å‹\":2,\"è£œæ°£å›ºè¡¨\":3}\n",
        "    x = torch.zeros(input_size)\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict:\n",
        "            x[symptom_dict[s]] = 1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx, p in enumerate(probs):\n",
        "        if p > 0.5:  # é–¾å€¼\n",
        "            formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 5ï¸âƒ£ äº’å‹•å•ç­”ç³»çµ±ï¼ˆæœ›èå•åˆ‡æ¨¡æ“¬ï¼‰\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ ç¥è¾²æ°é™è‡¨ï¼ç•¶ä»£å¤šå…ƒ AI ç¥è¾²æ°ç¾èº«\")\n",
        "    print(\"è¼¸å…¥ç—‡ç‹€ã€è‰è—¥åç¨±æˆ–æ‹¼éŸ³æŸ¥è©¢ï¼Œè¼¸å…¥ 'é…æ–¹' ç”Ÿæˆæ¨è–¦è‰è—¥çµ„åˆï¼Œè¼¸å…¥ 'q' é›¢é–‹\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nè«‹è¼¸å…¥ï¼š\").strip()\n",
        "        if q.lower() == \"q\":\n",
        "            print(\"ğŸ‘‹ ç¥è¾²æ°æš«æ™‚é™è½ï¼Œç¥ä½ ç™¾è‰å¸¸é’ï¼\")\n",
        "            break\n",
        "        elif q.lower() == \"é…æ–¹\":\n",
        "            symptoms_input = input(\"è«‹è¼¸å…¥ç—‡ç‹€ï¼ˆé€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè£œæ°£,å®‰ç¥ï¼‰ï¼š\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nğŸ’Š æ¨è–¦è‰è—¥çµ„åˆï¼ˆåƒ…é™ç¥è¾²æ°ç¯„åœï¼‰ï¼š\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']} ({f['ancient_name']}): {', '.join(f['effects'])}, ç”¨æ³•: {f['usage']}, ä¾†æº: {f['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç¬¦åˆç—‡ç‹€çš„è‰è—¥\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                print(f\"\\nğŸŒ± æ‰¾åˆ° {len(matches)} ç­†è³‡æ–™ï¼š\")\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} ({m['ancient_name']})\")\n",
        "                    print(f\"  åŠŸæ•ˆ: {', '.join(m['effects'])}\")\n",
        "                    print(f\"  ç”¨æ³•: {m['usage']}\")\n",
        "                    print(f\"  ä¾†æº: {m['source']}\")\n",
        "                    print(f\"  åœ–ç‰‡: {m['image']}\\n\")\n",
        "            else:\n",
        "                print(\"âŒ æ‰¾ä¸åˆ°ç›¸é—œè—¥æï¼Œè«‹ç¢ºèªè¼¸å…¥æˆ–æ›å€‹é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•ç³»çµ±\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjzdz5zcQnmC"
      },
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€/èˆŒè‹”/è„ˆè±¡/ç…§ç‰‡\n",
        "        â†“\n",
        "èªæ„ç†è§£ + ç—‡ç‹€å‘é‡åŒ– + åœ–ç‰‡è¾¨è­˜\n",
        "        â†“\n",
        "å¤šå±¤ç¥ç¶“ç¶²è·¯ + Transformer è¨ˆç®—è‰è—¥æ¨è–¦æ¦‚ç‡\n",
        "        â†“\n",
        "AI é…æ–¹ç”Ÿæˆï¼ˆæ¦‚ç‡æ’åºï¼Œå®‰å…¨ç¯„åœï¼‰\n",
        "        â†“\n",
        "é¡¯ç¤ºè‰è—¥åŠŸæ•ˆã€ç”¨æ³•ã€ä¾†æºã€åœ–ç‰‡\n",
        "        â†“\n",
        "å¯é€²ä¸€æ­¥æ‰‹ä½œé¦™ç™‚æˆ–å¥åº·è¿½è¹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJFMoauHWVUa"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€/èˆŒè‹”/è„ˆè±¡/ç…§ç‰‡ â†“ èªæ„ç†è§£ + ç—‡ç‹€å‘é‡åŒ– + åœ–ç‰‡è¾¨è­˜ â†“ å¤šå±¤ç¥ç¶“ç¶²è·¯ + Transformer è¨ˆç®—è‰è—¥æ¨è–¦æ¦‚ç‡ â†“ AI é…æ–¹ç”Ÿæˆï¼ˆæ¦‚ç‡æ’åºï¼Œå®‰å…¨ç¯„åœï¼‰ â†“ é¡¯ç¤ºè‰è—¥åŠŸæ•ˆã€ç”¨æ³•ã€ä¾†æºã€åœ–ç‰‡ â†“ å¯é€²ä¸€æ­¥æ‰‹ä½œé¦™ç™‚æˆ–å¥åº·è¿½è¹¤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FurAKs0WQwMi"
      },
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€/èªéŸ³/åœ–ç‰‡\n",
        "       â†“\n",
        "å…¨æ¯å‘é‡åŒ–ï¼ˆæ–‡å­—+åœ–åƒ+éŸ³è¨Šï¼‰\n",
        "       â†“\n",
        "å¤šå±¤ç¥ç¶“ç¶²è·¯ + Transformer åˆ†æ\n",
        "       â†“\n",
        "AI æ¨è–¦è‰è—¥ + é…æ–¹ç”Ÿæˆ\n",
        "       â†“\n",
        "é¡¯ç¤ºåŠŸæ•ˆã€ç”¨æ³•ã€ä¾†æºã€åœ–ç‰‡\n",
        "       â†“\n",
        "äº’å‹•å•ç­” + å¯èª¿æ•´é…æ–¹\n",
        "       â†“\n",
        "å¥åº·è¿½è¹¤ + æ‰‹ä½œé¦™ç™‚å»ºè­°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaclRUwSWSni"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€/èªéŸ³/åœ–ç‰‡ â†“ å…¨æ¯å‘é‡åŒ–ï¼ˆæ–‡å­—+åœ–åƒ+éŸ³è¨Šï¼‰ â†“ å¤šå±¤ç¥ç¶“ç¶²è·¯ + Transformer åˆ†æ â†“ AI æ¨è–¦è‰è—¥ + é…æ–¹ç”Ÿæˆ â†“ é¡¯ç¤ºåŠŸæ•ˆã€ç”¨æ³•ã€ä¾†æºã€åœ–ç‰‡ â†“ äº’å‹•å•ç­” + å¯èª¿æ•´é…æ–¹ â†“ å¥åº·è¿½è¹¤ + æ‰‹ä½œé¦™ç™‚å»ºè­°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A63lSmkuQ4pB"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from PIL import Image\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ æœ¬åœ°ç™¾è‰è³‡æ–™åº«ï¼ˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"äººåƒ\",\"effects\":[\"è£œæ°£\",\"å®‰ç¥\",\"æŠ—ç–²å‹\"],\"usage\":\"ç…æ¹¯ 3-10 å…‹\",\"source\":\"ç¥è¾²æœ¬è‰ç¶“\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"é»ƒèŠª\",\"effects\":[\"è£œæ°£å›ºè¡¨\",\"åˆ©å°¿æ¶ˆè…«\"],\"usage\":\"ç…æ¹¯ 10-15 å…‹\",\"source\":\"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"æ¸æ\",\"effects\":[\"è£œè‚è…\",\"ç›Šç²¾æ˜ç›®\"],\"usage\":\"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"è–„è·\",\"effects\":[\"ç–é¢¨æ¸…ç†±\",\"åˆ©å’½\",\"é ­ç—›\"],\"usage\":\"æ³¡èŒ¶ 3-6 å…‹\",\"source\":\"æœ¬è‰ç¶±ç›®\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_herbs.json\"\n",
        "with open(DB_FILE,\"w\",encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data,f,ensure_ascii=False,indent=2)\n",
        "\n",
        "# ======================\n",
        "# 2ï¸âƒ£ ç™¾è‰æŸ¥è©¢åŠŸèƒ½\n",
        "# ======================\n",
        "def search_herb(keyword):\n",
        "    with open(DB_FILE,\"r\",encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    results = [h for h in herbs if any(keyword in str(v) for k,v in h.items())]\n",
        "    return results\n",
        "\n",
        "# ======================\n",
        "# 3ï¸âƒ£ å¤šå±¤ç¥ç¶“ç¶²è·¯ç¤ºç¯„ï¼ˆè‰è—¥æ¨è–¦ï¼‰\n",
        "# ======================\n",
        "class ShennongNet(nn.Module):\n",
        "    def __init__(self,input_size,hidden_size,output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_size,hidden_size)\n",
        "        self.layer3 = nn.Linear(hidden_size,output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self,x):\n",
        "        x=self.relu(self.layer1(x))\n",
        "        x=self.relu(self.layer2(x))\n",
        "        x=self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "input_size = 4\n",
        "hidden_size = 8\n",
        "output_size = 4\n",
        "model = ShennongNet(input_size,hidden_size,output_size)\n",
        "\n",
        "# ======================\n",
        "# 4ï¸âƒ£ Transformer / BERT èªæ„ç†è§£\n",
        "# ======================\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_text(text):\n",
        "    inputs = tokenizer(text,return_tensors=\"pt\")\n",
        "    outputs = bert_model(**inputs)\n",
        "    # å– [CLS] token çš„å‘é‡\n",
        "    return outputs.last_hidden_state[:,0,:]\n",
        "\n",
        "# ======================\n",
        "# 5ï¸âƒ£ å¤šæ¨¡æ…‹æ–‡å­—+åœ–ç‰‡å‘é‡ï¼ˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "def encode_image(image_path):\n",
        "    # é€™è£¡ç¤ºç¯„è¼‰å…¥åœ–ç‰‡ï¼Œå¯æ¥ CNN ç‰¹å¾µæŠ½å–\n",
        "    img = Image.open(image_path).resize((224,224))\n",
        "    return torch.tensor([0.5]*512)  # placeholder å‘é‡\n",
        "\n",
        "# ======================\n",
        "# 6ï¸âƒ£ AI é…æ–¹ç”Ÿæˆç¤ºç¯„\n",
        "# ======================\n",
        "def generate_formula(symptoms:list):\n",
        "    x = torch.zeros(input_size)\n",
        "    symptom_dict = {\"è£œæ°£\":0,\"å®‰ç¥\":1,\"æŠ—ç–²å‹\":2,\"è£œæ°£å›ºè¡¨\":3}\n",
        "    for s in symptoms:\n",
        "        if s in symptom_dict: x[symptom_dict[s]]=1.0\n",
        "    probs = model(x).detach().numpy()\n",
        "    with open(DB_FILE,\"r\",encoding=\"utf-8\") as f:\n",
        "        herbs = json.load(f)\n",
        "    formula = []\n",
        "    for idx,p in enumerate(probs):\n",
        "        if p>0.5: formula.append(herbs[idx])\n",
        "    return formula\n",
        "\n",
        "# ======================\n",
        "# 7ï¸âƒ£ äº’å‹•å•ç­”ç³»çµ±\n",
        "# ======================\n",
        "def shennong_ai_bot():\n",
        "    print(\"ğŸŒ¿ ç¥è¾²æ°é™è‡¨ Python ç¾èº«ç‰ˆ\")\n",
        "    print(\"è¼¸å…¥ç—‡ç‹€/è‰è—¥åç¨±ï¼Œæˆ– 'é…æ–¹' ç”Ÿæˆæ¨è–¦è‰è—¥çµ„åˆï¼Œè¼¸å…¥ 'q' é›¢é–‹\")\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nè«‹è¼¸å…¥ï¼š\").strip()\n",
        "        if q.lower()==\"q\":\n",
        "            print(\"ğŸ‘‹ ç¥è¾²æ°æš«æ™‚é™è½ï¼\")\n",
        "            break\n",
        "        elif q.lower()==\"é…æ–¹\":\n",
        "            symptoms_input = input(\"è¼¸å…¥ç—‡ç‹€(é€—è™Ÿåˆ†éš”ï¼Œä¾‹å¦‚ï¼šè£œæ°£,å®‰ç¥)ï¼š\")\n",
        "            symptoms = [s.strip() for s in symptoms_input.split(\",\") if s.strip()]\n",
        "            formula = generate_formula(symptoms)\n",
        "            if formula:\n",
        "                print(\"\\nğŸ’Š æ¨è–¦è‰è—¥çµ„åˆï¼ˆç¥è¾²æ°ç¯„åœï¼‰ï¼š\")\n",
        "                for f in formula:\n",
        "                    print(f\"- {f['name']}: {', '.join(f['effects'])}, ç”¨æ³•: {f['usage']}, ä¾†æº: {f['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ ç„¡ç¬¦åˆç—‡ç‹€çš„è‰è—¥\")\n",
        "        else:\n",
        "            matches = search_herb(q)\n",
        "            if matches:\n",
        "                for m in matches:\n",
        "                    print(f\"- {m['name']} åŠŸæ•ˆ: {', '.join(m['effects'])}, ç”¨æ³•: {m['usage']}, ä¾†æº: {m['source']}\")\n",
        "            else:\n",
        "                print(\"âŒ æŸ¥ç„¡è³‡æ–™ï¼Œè«‹æ›é—œéµå­—\")\n",
        "\n",
        "# ======================\n",
        "# å•Ÿå‹•äº’å‹•\n",
        "# ======================\n",
        "if __name__==\"__main__\":\n",
        "    shennong_ai_bot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr20U5r3Q9px"
      },
      "outputs": [],
      "source": [
        "python shennong_ai.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2TRCwq7RESK"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€æ–‡å­— / èªéŸ³ / è‰è—¥åœ–ç‰‡\n",
        "       â†“\n",
        "æ–‡å­— â†’ Transformer/BERT ç·¨ç¢¼\n",
        "åœ–ç‰‡ â†’ CNN ç‰¹å¾µæå–\n",
        "       â†“\n",
        "å‘é‡èåˆ â†’ å¤šå±¤ç¥ç¶“ç¶²è·¯è¨ˆç®—è‰è—¥æ¨è–¦æ¦‚ç‡\n",
        "       â†“\n",
        "ç”Ÿæˆå®‰å…¨é…æ–¹ï¼ˆTransformer é…æ–¹ç”Ÿæˆï¼‰\n",
        "       â†“\n",
        "å¯è¦–åŒ–è‰è—¥ç¶²çµ¡åœ– + é…æ–¹è©³ç´°è³‡è¨Š\n",
        "       â†“\n",
        "äº’å‹•å•ç­” + å‹•æ…‹èª¿æ•´é…æ–¹\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDWQKH_-RFkh"
      },
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€æ–‡å­— / èªéŸ³ / è‰è—¥åœ–ç‰‡\n",
        "       â†“\n",
        "æ–‡å­— â†’ Transformer/BERT ç·¨ç¢¼\n",
        "åœ–ç‰‡ â†’ CNN ç‰¹å¾µæå–\n",
        "       â†“\n",
        "å‘é‡èåˆ â†’ å¤šå±¤ç¥ç¶“ç¶²è·¯è¨ˆç®—è‰è—¥æ¨è–¦æ¦‚ç‡\n",
        "       â†“\n",
        "ç”Ÿæˆå®‰å…¨é…æ–¹ï¼ˆTransformer é…æ–¹ç”Ÿæˆï¼‰\n",
        "       â†“\n",
        "å¯è¦–åŒ–è‰è—¥ç¶²çµ¡åœ– + é…æ–¹è©³ç´°è³‡è¨Š\n",
        "       â†“\n",
        "äº’å‹•å•ç­” + å‹•æ…‹èª¿æ•´é…æ–¹\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNTlTmFKWJAi"
      },
      "outputs": [],
      "source": [
        "ä½¿ç”¨è€…è¼¸å…¥ç—‡ç‹€æ–‡å­— / èªéŸ³ / è‰è—¥åœ–ç‰‡ â†“ æ–‡å­— â†’ Transformer/BERT ç·¨ç¢¼ åœ–ç‰‡ â†’ CNN ç‰¹å¾µæå– â†“ å‘é‡èåˆ â†’ å¤šå±¤ç¥ç¶“ç¶²è·¯è¨ˆç®—è‰è—¥æ¨è–¦æ¦‚ç‡ â†“ ç”Ÿæˆå®‰å…¨é…æ–¹ï¼ˆTransformer é…æ–¹ç”Ÿæˆï¼‰ â†“ å¯è¦–åŒ–è‰è—¥ç¶²çµ¡åœ– + é…æ–¹è©³ç´°è³‡è¨Š â†“ äº’å‹•å•ç­” + å‹•æ…‹èª¿æ•´é…æ–¹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czFhake9RPhg"
      },
      "outputs": [],
      "source": [
        "# å»ºè­°æ–°ç’°å¢ƒ/Colab ç›´æ¥åŸ·è¡Œ\n",
        "pip install torch torchvision transformers gradio plotly networkx pillow pandas\n",
        "\n",
        "# å¦å­˜ä¸‹æ–¹ç¨‹å¼ç‚º app_shennong_multimodal.py å¾ŒåŸ·è¡Œï¼š\n",
        "python app_shennong_multimodal.py\n",
        "# çœ‹åˆ°æœ¬æ©Ÿ/å…¬é–‹ URL å¾Œé»é–‹å³å¯äº’å‹•\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooZkWkgmRVH6"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "ç•¶ä»£å¤šæ¨¡æ…‹æ·±åº¦å­¸ç¿’ç‰ˆãƒ»ç¥è¾²æ°é™è‡¨\n",
        "åŠŸèƒ½ï¼š\n",
        "- å¤šæ¨¡æ…‹è¼¸å…¥ï¼šç—‡ç‹€æ–‡å­— + è‰è—¥/èˆŒè‹”åœ–ç‰‡ï¼ˆå¯é¸ï¼‰\n",
        "- ç™¾è‰è³‡æ–™åº«æŸ¥è©¢ï¼ˆæœ¬åœ° JSONï¼‰\n",
        "- å¤šå±¤ç¥ç¶“ç¶²è·¯ + BERT(Transformer) é…æ–¹ç”Ÿæˆï¼ˆç¤ºç¯„ç´šï¼‰\n",
        "- è‰è—¥ç¶²çµ¡åœ–å¯è¦–åŒ–ï¼ˆå…±ç¾/åŠŸæ•ˆé—œè¯ï¼‰\n",
        "- Gradio äº’å‹•ç•Œé¢ï¼ˆé™„ Streamlit ç‰ˆæœ¬ç‰‡æ®µæ–¼æ–‡æœ«ï¼‰\n",
        "\"\"\"\n",
        "import json, io, os, math, random\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.models as tvm\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from PIL import Image\n",
        "import networkx as nx\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "\n",
        "# æœ‰ä¸­æ–‡æœ€å¥½ç”¨ä¸­æ–‡ BERTï¼›è‹¥ä¸‹è¼‰å¤±æ•—ï¼Œè‡ªå‹•é€€å›è‹±æ–‡ BERT\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# =========================\n",
        "# 0) æœ¬åœ°ç™¾è‰è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# =========================\n",
        "HERBS_DB = [\n",
        "    {\n",
        "        \"name\": \"äººåƒ\", \"ancient_name\": \"äººåƒ\", \"pinyin\": \"renshen\",\n",
        "        \"effects\": [\"è£œæ°£\", \"å®‰ç¥\", \"æŠ—ç–²å‹\"], \"usage\": \"ç…æ¹¯ 3â€“10g\",\n",
        "        \"source\": \"ç¥è¾²æœ¬è‰ç¶“\", \"image\": \"https://example.com/renshen.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"é»ƒèŠª\", \"ancient_name\": \"é»ƒè€†\", \"pinyin\": \"huangqi\",\n",
        "        \"effects\": [\"è£œæ°£å›ºè¡¨\", \"åˆ©å°¿æ¶ˆè…«\", \"ç›Šè¡›\"], \"usage\": \"ç…æ¹¯ 10â€“15g\",\n",
        "        \"source\": \"é¦™æ¸¯è—¥æè³‡æ–™åº«\", \"image\": \"https://example.com/huangqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"æ¸æ\", \"ancient_name\": \"æ¸æå­\", \"pinyin\": \"gouqi\",\n",
        "        \"effects\": [\"è£œè‚è…\", \"ç›Šç²¾æ˜ç›®\"], \"usage\": \"æ³¡èŒ¶/ç…æ¹¯ 6â€“12g\",\n",
        "        \"source\": \"TianAPI\", \"image\": \"https://example.com/gouqi.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"è–„è·\", \"ancient_name\": \"è–„è·\", \"pinyin\": \"bohe\",\n",
        "        \"effects\": [\"ç–é¢¨æ¸…ç†±\", \"åˆ©å’½\", \"é ­ç—›\"], \"usage\": \"æ³¡èŒ¶/å…¥æ–¹ 3â€“6g\",\n",
        "        \"source\": \"æœ¬è‰ç¶±ç›®\", \"image\": \"https://example.com/bohe.jpg\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"ç•¶æ­¸\", \"ancient_name\": \"ç•¶æ­¸\", \"pinyin\": \"danggui\",\n",
        "        \"effects\": [\"è£œè¡€æ´»è¡€\", \"èª¿ç¶“æ­¢ç—›\", \"æ½¤è…¸é€šä¾¿\"], \"usage\": \"ç…æ¹¯ 6â€“12g\",\n",
        "        \"source\": \"æœ¬è‰ç¶±ç›®\", \"image\": \"https://example.com/danggui.jpg\"\n",
        "    },\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_herbs.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(HERBS_DB, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "def load_db() -> List[Dict[str, Any]]:\n",
        "    with open(DB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# =========================\n",
        "# 1) æŸ¥è©¢åŠŸèƒ½\n",
        "# =========================\n",
        "def search_herbs(keyword: str) -> List[Dict[str, Any]]:\n",
        "    keyword = (keyword or \"\").strip()\n",
        "    herbs = load_db()\n",
        "    if not keyword:\n",
        "        return herbs\n",
        "    res = []\n",
        "    for h in herbs:\n",
        "        blob = f\"{h.get('name','')},{h.get('ancient_name','')},{h.get('pinyin','')},{','.join(h.get('effects',[]))},{h.get('usage','')},{h.get('source','')}\"\n",
        "        if keyword in blob:\n",
        "            res.append(h)\n",
        "    return res\n",
        "\n",
        "# =========================\n",
        "# 2) æ¨¡å‹èˆ‡ç‰¹å¾µï¼šBERT(æ–‡å­—) + ResNet(åœ–ç‰‡)\n",
        "# =========================\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def _load_text_model():\n",
        "    # å„ªå…ˆä¸­æ–‡æ¨¡å‹ï¼Œå¤±æ•—é€€å›è‹±æ–‡\n",
        "    candidates = [\"bert-base-chinese\", \"bert-base-uncased\"]\n",
        "    err = None\n",
        "    for ckpt in candidates:\n",
        "        try:\n",
        "            tok = AutoTokenizer.from_pretrained(ckpt)\n",
        "            mdl = AutoModel.from_pretrained(ckpt)\n",
        "            return tok, mdl.to(DEVICE), ckpt\n",
        "        except Exception as e:\n",
        "            err = e\n",
        "            continue\n",
        "    raise RuntimeError(f\"ç„¡æ³•è¼‰å…¥ BERTï¼š{err}\")\n",
        "\n",
        "TOKENIZER, BERT, BERT_NAME = _load_text_model()\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text(text: str) -> torch.Tensor:\n",
        "    \"\"\"è¼¸å…¥ç—‡ç‹€/æè¿° â†’ 768 ç¶­å‘é‡ï¼ˆæˆ–ç›¸æ‡‰éš±å±¤ç¶­åº¦ï¼‰\"\"\"\n",
        "    text = text.strip() if text else \"\"\n",
        "    if not text:\n",
        "        return torch.zeros(BERT.config.hidden_size, device=DEVICE)\n",
        "    toks = TOKENIZER(text, return_tensors=\"pt\", truncation=True, max_length=256)\n",
        "    toks = {k: v.to(DEVICE) for k, v in toks.items()}\n",
        "    out = BERT(**toks).last_hidden_state[:, 0, :]   # CLS å‘é‡\n",
        "    return out.squeeze(0)\n",
        "\n",
        "def _load_img_model():\n",
        "    try:\n",
        "        resnet = tvm.resnet18(weights=tvm.ResNet18_Weights.DEFAULT)\n",
        "    except Exception:\n",
        "        # è‹¥ç„¡æ³•ä¸‹è¼‰æ¬Šé‡ï¼Œå»ºç«‹éš¨æ©Ÿåˆå§‹åŒ– resnet18\n",
        "        resnet = tvm.resnet18(weights=None)\n",
        "    resnet.fc = nn.Identity()  # å–å€’æ•¸ç¬¬äºŒå±¤ 512 ç¶­ç‰¹å¾µ\n",
        "    return resnet.to(DEVICE).eval()\n",
        "\n",
        "RESNET = _load_img_model()\n",
        "IMG_TF = T.Compose([\n",
        "    T.Resize(256), T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_image(img: Image.Image) -> torch.Tensor:\n",
        "    if img is None:\n",
        "        return torch.zeros(512, device=DEVICE)\n",
        "    x = IMG_TF(img).unsqueeze(0).to(DEVICE)\n",
        "    feat = RESNET(x)  # [1,512]\n",
        "    return feat.squeeze(0)\n",
        "\n",
        "# =========================\n",
        "# 3) å¤šå±¤ç¥ç¶“ç¶²è·¯èåˆ + é…æ–¹æ‰“åˆ† (ç¤ºç¯„)\n",
        "#    æ–‡æœ¬ 768 + åœ–ç‰‡ 512 â†’ èåˆ â†’ herb_logits\n",
        "# =========================\n",
        "@dataclass\n",
        "class FusionCfg:\n",
        "    text_dim: int\n",
        "    img_dim: int\n",
        "    hidden: int\n",
        "    num_herbs: int\n",
        "\n",
        "class FusionScorer(nn.Module):\n",
        "    def __init__(self, cfg: FusionCfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.text_dim + cfg.img_dim, cfg.hidden)\n",
        "        self.fc2 = nn.Linear(cfg.hidden, cfg.hidden)\n",
        "        self.out = nn.Linear(cfg.hidden, cfg.num_herbs)\n",
        "\n",
        "    def forward(self, txt: torch.Tensor, img: torch.Tensor):\n",
        "        z = torch.cat([txt, img], dim=-1)\n",
        "        z = F.relu(self.fc1(z))\n",
        "        z = F.relu(self.fc2(z))\n",
        "        logits = self.out(z)                   # [num_herbs]\n",
        "        probs = torch.sigmoid(logits)          # æ¯å¸–è‰è—¥æ¨è–¦æ©Ÿç‡\n",
        "        return probs\n",
        "\n",
        "CFG = FusionCfg(text_dim=BERT.config.hidden_size, img_dim=512, hidden=384, num_herbs=len(load_db()))\n",
        "FUSION = FusionScorer(CFG).to(DEVICE).eval()\n",
        "\n",
        "# âš ï¸ ç¤ºç¯„ç”¨ï¼šéš¨æ©Ÿåˆå§‹åŒ–æ¬Šé‡ã€‚å¯¦å‹™ä¸Šè«‹ç”¨ï¼ˆç—‡ç‹€ â†’ é…æ–¹ï¼‰æ¨™è¨“è³‡æ–™åšç›£ç£è¨“ç·´ã€‚\n",
        "# é€™è£¡æˆ‘å€‘æœƒåœ¨ç„¡åƒæ•¸è¨“ç·´ä¸‹ï¼Œçµåˆã€Œé—œéµå­—â†’åŠŸæ•ˆã€çš„è¦å‰‡ï¼Œå°æ©Ÿç‡åšè¼•å¾®æ ¡æ­£ï¼ˆç¤ºç¯„ç´šï¼‰ã€‚\n",
        "\n",
        "# =========================\n",
        "# 4) è¦å‰‡æ ¡æ­£ï¼šå¾æ•ˆæœè©å°é½Šï¼ˆä¸é›¢ç¥è¾²ï¼‰\n",
        "# =========================\n",
        "EFFECT_SYNONYM = {\n",
        "    \"æ°£è™›\": [\"è£œæ°£\",\"è£œæ°£å›ºè¡¨\",\"æŠ—ç–²å‹\"],\n",
        "    \"ç–²å€¦\": [\"æŠ—ç–²å‹\",\"è£œæ°£\"],\n",
        "    \"å¤±çœ \": [\"å®‰ç¥\"],\n",
        "    \"å–‰åš¨ç—›\": [\"åˆ©å’½\",\"ç–é¢¨æ¸…ç†±\"],\n",
        "    \"é ­ç—›\": [\"é ­ç—›\",\"ç–é¢¨æ¸…ç†±\"],\n",
        "    \"çœ¼ç›ç–²å‹\": [\"ç›Šç²¾æ˜ç›®\"],\n",
        "    \"ç¶“ç—›\": [\"èª¿ç¶“æ­¢ç—›\"],\n",
        "    \"ä¾¿ç§˜\": [\"æ½¤è…¸é€šä¾¿\"],\n",
        "}\n",
        "def soft_rule_boost(text: str, probs: torch.Tensor) -> torch.Tensor:\n",
        "    text = text or \"\"\n",
        "    herbs = load_db()\n",
        "    bonus = torch.zeros_like(probs)\n",
        "    # æ‰¾åˆ°æ–‡å­—è£¡çš„ç—‡ç‹€ï¼Œçµ¦å°æ‡‰åŠŸæ•ˆçš„è‰è—¥å¾®é‡åŠ åˆ†\n",
        "    for symptom, effs in EFFECT_SYNONYM.items():\n",
        "        if symptom in text:\n",
        "            for i, h in enumerate(herbs):\n",
        "                if any(e in h.get(\"effects\", []) for e in effs):\n",
        "                    bonus[i] += 0.10  # å¾®èª¿\n",
        "    probs = torch.clamp(probs + bonus, 0, 1)\n",
        "    return probs\n",
        "\n",
        "# =========================\n",
        "# 5) é…æ–¹ç”Ÿæˆï¼šå– Top-K è‰è—¥ + åŠ‘é‡å»ºè­°(ç¤ºç¯„)\n",
        "# =========================\n",
        "def generate_formula(text: str, image: Image.Image, topk: int = 3) -> Tuple[List[Dict[str,Any]], List[float]]:\n",
        "    txt_vec = encode_text(text)          # [768]\n",
        "    img_vec = encode_image(image)        # [512]\n",
        "    probs = FUSION(txt_vec, img_vec)     # [N]\n",
        "    probs = soft_rule_boost(text, probs) # è¦å‰‡å¾®èª¿\n",
        "    vals, idxs = torch.topk(probs, k=min(topk, len(probs)))\n",
        "    herbs = load_db()\n",
        "    recs = [herbs[i] for i in idxs.tolist()]\n",
        "    return recs, vals.tolist()\n",
        "\n",
        "# =========================\n",
        "# 6) è‰è—¥ç¶²çµ¡åœ–ï¼ˆé…æ–¹å…§åŠŸæ•ˆå…±ç¾ + DB è¿‘é„°ï¼‰\n",
        "# =========================\n",
        "def build_herb_graph(selected: List[Dict[str,Any]]) -> go.Figure:\n",
        "    # ç¯„åœï¼šé¸ä¸­é…æ–¹ + èˆ‡å…¶åŠŸæ•ˆé‡ç–Šçš„é„°å±…\n",
        "    herbs = load_db()\n",
        "    names = {h[\"name\"] for h in selected}\n",
        "    # æ‰¾é„°å±…ï¼ˆåŠŸæ•ˆé‡ç–Šæ•¸>=1ï¼‰\n",
        "    neighbors = []\n",
        "    for h in herbs:\n",
        "        if h[\"name\"] in names:\n",
        "            continue\n",
        "        for sel in selected:\n",
        "            if set(h[\"effects\"]).intersection(sel[\"effects\"]):\n",
        "                neighbors.append(h); break\n",
        "\n",
        "    G = nx.Graph()\n",
        "    # åŠ ç¯€é»\n",
        "    for h in selected:\n",
        "        G.add_node(h[\"name\"], group=\"formula\")\n",
        "    for h in neighbors:\n",
        "        G.add_node(h[\"name\"], group=\"neighbor\")\n",
        "    # åŠ é‚Šï¼ˆåŠŸæ•ˆå…±ç¾ï¼‰\n",
        "    pool = selected + neighbors\n",
        "    for i in range(len(pool)):\n",
        "        for j in range(i+1, len(pool)):\n",
        "            a, b = pool[i], pool[j]\n",
        "            inter = set(a[\"effects\"]).intersection(b[\"effects\"])\n",
        "            if inter:\n",
        "                G.add_edge(a[\"name\"], b[\"name\"], weight=len(inter), label=\"ã€\".join(list(inter)[:2]))\n",
        "\n",
        "    # ä½ˆå±€\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.7)\n",
        "    # ç•«é‚Š\n",
        "    edge_x, edge_y = [], []\n",
        "    for u, v in G.edges():\n",
        "        x0, y0 = pos[u]; x1, y1 = pos[v]\n",
        "        edge_x += [x0, x1, None]\n",
        "        edge_y += [y0, y1, None]\n",
        "    edge_trace = go.Scatter(x=edge_x, y=edge_y, mode='lines', line=dict(width=1), hoverinfo='none')\n",
        "\n",
        "    # ç•«é»\n",
        "    node_x, node_y, texts, colors, sizes = [], [], [], [], []\n",
        "    for n, data in G.nodes(data=True):\n",
        "        x, y = pos[n]\n",
        "        node_x.append(x); node_y.append(y)\n",
        "        texts.append(n + f\"ï¼ˆ{data.get('group')}ï¼‰\")\n",
        "        if data.get(\"group\") == \"formula\":\n",
        "            colors.append(\"#2ca02c\")\n",
        "            sizes.append(20)\n",
        "        else:\n",
        "            colors.append(\"#1f77b4\")\n",
        "            sizes.append(14)\n",
        "    node_trace = go.Scatter(\n",
        "        x=node_x, y=node_y, mode='markers', hoverinfo='text',\n",
        "        marker=dict(size=sizes, color=colors, line=dict(width=1)),\n",
        "        text=texts\n",
        "    )\n",
        "\n",
        "    fig = go.Figure(data=[edge_trace, node_trace],\n",
        "                    layout=go.Layout(\n",
        "                        title=\"è‰è—¥ç¶²çµ¡åœ–ï¼ˆåŠŸæ•ˆå…±ç¾ï¼‰\",\n",
        "                        showlegend=False, hovermode='closest',\n",
        "                        margin=dict(l=20, r=20, t=40, b=20)\n",
        "                    ))\n",
        "    return fig\n",
        "\n",
        "# =========================\n",
        "# 7) Gradio äº’å‹•ç•Œé¢\n",
        "# =========================\n",
        "import gradio as gr\n",
        "\n",
        "def _to_df(items: List[Dict[str,Any]]) -> pd.DataFrame:\n",
        "    if not items: return pd.DataFrame(columns=[\"name\",\"effects\",\"usage\",\"source\"])\n",
        "    rows = []\n",
        "    for h in items:\n",
        "        rows.append({\n",
        "            \"name\": h.get(\"name\",\"\"),\n",
        "            \"effects\": \"ã€\".join(h.get(\"effects\",[])),\n",
        "            \"usage\": h.get(\"usage\",\"\"),\n",
        "            \"source\": h.get(\"source\",\"\")\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def ui_search(keyword: str):\n",
        "    res = search_herbs(keyword or \"\")\n",
        "    return _to_df(res)\n",
        "\n",
        "def ui_generate(symptom_text: str, image: Image.Image, topk: int):\n",
        "    selected, scores = generate_formula(symptom_text or \"\", image, topk=topk)\n",
        "    fig = build_herb_graph(selected) if selected else go.Figure()\n",
        "    # è¼¸å‡ºå»ºè­°\n",
        "    lines = []\n",
        "    for h, p in zip(selected, scores):\n",
        "        lines.append(f\"ğŸŒ¿ {h['name']}ï½œåŠŸæ•ˆï¼š{ 'ã€'.join(h['effects']) }ï½œç”¨æ³•ï¼š{h['usage']}ï½œä¾†æºï¼š{h['source']}ï½œæ©Ÿç‡ï¼š{p:.2f}\")\n",
        "    suggest = \"\\n\".join(lines) if lines else \"ï¼ˆç„¡å»ºè­°ï¼‰\"\n",
        "    return suggest, fig, _to_df(selected)\n",
        "\n",
        "with gr.Blocks(title=\"ç•¶ä»£å¤šæ¨¡æ…‹æ·±åº¦å­¸ç¿’ãƒ»ç¥è¾²æ°\") as demo:\n",
        "    gr.Markdown(\"# ğŸŒ¿ ç•¶ä»£å¤šæ¨¡æ…‹æ·±åº¦å­¸ç¿’ãƒ»ç¥è¾²æ°ï¼ˆç¾èº«ç‰ˆï¼‰\")\n",
        "    gr.Markdown(\"è¼¸å…¥ç—‡ç‹€æ–‡å­—ï¼ˆå¯é…åˆåœ–ç‰‡ï¼‰â†’ ç¥ç¶“ç¶²è·¯ + BERT æ¨è–¦è‰è—¥ä¸¦ç”Ÿæˆé…æ–¹ï¼ˆç¤ºç¯„ï¼‰ã€‚\\n*åƒ…ä¾›ç ”ç©¶æ¸¬è©¦ï¼Œè«‹ä¿æŒåœ¨ç¥è¾²æ°è—¥ç†ç¯„åœã€‚*\")\n",
        "\n",
        "    with gr.Tab(\"ğŸ” ç™¾è‰æŸ¥è©¢\"):\n",
        "        kw = gr.Textbox(label=\"é—œéµå­—ï¼ˆåç¨±/æ‹¼éŸ³/åŠŸæ•ˆ/ç”¨æ³•/ä¾†æºï¼‰\", placeholder=\"ä¾‹ï¼šè£œæ°£ã€é ­ç—›ã€renshen\")\n",
        "        btn_s = gr.Button(\"æœå°‹\")\n",
        "        tbl = gr.Dataframe(headers=[\"name\",\"effects\",\"usage\",\"source\"], interactive=False)\n",
        "        btn_s.click(ui_search, inputs=kw, outputs=tbl)\n",
        "\n",
        "    with gr.Tab(\"ğŸ§  å¤šæ¨¡æ…‹é…æ–¹ç”Ÿæˆ\"):\n",
        "        symptom = gr.Textbox(label=\"ç—‡ç‹€æ–‡å­—ï¼ˆè‡ªç„¶èªè¨€ï¼‰\", placeholder=\"ä¾‹ï¼šæœ€è¿‘æ°£è™›ç–²å€¦ã€å®¹æ˜“é ­ç—›ã€å–‰åš¨ç—›\")\n",
        "        img = gr.Image(label=\"ï¼ˆå¯é¸ï¼‰èˆŒè‹”/è‰è—¥åœ–ç‰‡\", type=\"pil\")\n",
        "        topk = gr.Slider(1, max( min(5, len(load_db())), 1 ), value=3, step=1, label=\"é…æ–¹ Top-K\")\n",
        "        btn_g = gr.Button(\"ç”Ÿæˆé…æ–¹\")\n",
        "        out_txt = gr.Textbox(label=\"æ¨è–¦è‰è—¥èˆ‡èªªæ˜\", lines=8)\n",
        "        out_fig = gr.Plot(label=\"è‰è—¥ç¶²çµ¡åœ–\")\n",
        "        out_tbl = gr.Dataframe(headers=[\"name\",\"effects\",\"usage\",\"source\"], interactive=False)\n",
        "        btn_g.click(ui_generate, inputs=[symptom, img, topk], outputs=[out_txt, out_fig, out_tbl])\n",
        "\n",
        "    gr.Markdown(\"> ä¾†æºï¼šç¥è¾²æœ¬è‰ç¶“ã€æœ¬è‰ç¶±ç›®ã€é¦™æ¸¯è—¥æè³‡æ–™åº«ã€TianAPIï¼ˆç¤ºç¯„è³‡æ–™ï¼‰ã€‚\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # å•Ÿå‹• Gradio\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHN52Q_bRoUQ"
      },
      "outputs": [],
      "source": [
        "# streamlit_app.pyï¼ˆç°¡ç‰ˆï¼‰\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# å‡è¨­åŒç›®éŒ„å¯ import ä¸Šè¿°å‡½å¼ï¼ˆæˆ–æŠŠå¿…è¦å‡½å¼è²¼é€²æ­¤æª”ï¼‰\n",
        "from app_shennong_multimodal import search_herbs, generate_formula, build_herb_graph, _to_df\n",
        "\n",
        "st.set_page_config(page_title=\"ç•¶ä»£å¤šæ¨¡æ…‹ç¥è¾²æ°\", layout=\"wide\")\n",
        "st.title(\"ğŸŒ¿ ç•¶ä»£å¤šæ¨¡æ…‹æ·±åº¦å­¸ç¿’ãƒ»ç¥è¾²æ°ï¼ˆStreamlitï¼‰\")\n",
        "\n",
        "tab1, tab2 = st.tabs([\"ğŸ” ç™¾è‰æŸ¥è©¢\", \"ğŸ§  å¤šæ¨¡æ…‹é…æ–¹ç”Ÿæˆ\"])\n",
        "\n",
        "with tab1:\n",
        "    kw = st.text_input(\"é—œéµå­—\", \"\")\n",
        "    if st.button(\"æœå°‹\"):\n",
        "        df = _to_df(search_herbs(kw))\n",
        "        st.dataframe(df, use_container_width=True)\n",
        "\n",
        "with tab2:\n",
        "    text = st.text_area(\"ç—‡ç‹€æ–‡å­—\", \"æœ€è¿‘æ°£è™›ç–²å€¦ï¼Œå¶æœ‰å–‰åš¨ç—›èˆ‡é ­ç—›\")\n",
        "    img = st.file_uploader(\"ï¼ˆå¯é¸ï¼‰èˆŒè‹”/è‰è—¥åœ–ç‰‡\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
        "    topk = st.slider(\"é…æ–¹ Top-K\", 1, 5, 3)\n",
        "    if st.button(\"ç”Ÿæˆé…æ–¹\"):\n",
        "        pil = Image.open(img).convert(\"RGB\") if img else None\n",
        "        herbs, probs = generate_formula(text, pil, topk=topk)\n",
        "        lines = []\n",
        "        for h,p in zip(herbs, probs):\n",
        "            lines.append(f\"ğŸŒ¿ {h['name']}ï½œåŠŸæ•ˆï¼š{'ã€'.join(h['effects'])}ï½œç”¨æ³•ï¼š{h['usage']}ï½œä¾†æºï¼š{h['source']}ï½œæ©Ÿç‡ï¼š{p:.2f}\")\n",
        "        st.text(\"\\n\".join(lines) if lines else \"ï¼ˆç„¡å»ºè­°ï¼‰\")\n",
        "        if herbs:\n",
        "            fig = build_herb_graph(herbs)\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "            st.dataframe(_to_df(herbs), use_container_width=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GGr1EakRrdY"
      },
      "outputs": [],
      "source": [
        "streamlit run streamlit_app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mMd_JNTRxSf"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.some-real-herb-database.com\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN9nV0b_R2YQ"
      },
      "outputs": [],
      "source": [
        "url = \"https://herbaltcm.sn.polyu.edu.hk/tc/\"  # HK PolyU ä¸­è—¥è³‡æ–™åº«\n",
        "# æˆ–ï¼š\n",
        "url = \"https://cloudtcm.com/herb\"\n",
        "# æˆ–åˆ©ç”¨ HERB/TMIT å¹³å°çš„æª¢ç´¢ URL ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP_gPxENR76X"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"é»ƒèŠª\": {\"åŠŸæ•ˆ\": \"è£œæ°£å›ºè¡¨\", \"å¸¸è¦‹é…ä¼\": [\"äººåƒ\", \"ç™½æœ®\"]},\n",
        "  \"ç”˜è‰\": {\"åŠŸæ•ˆ\": \"èª¿å’Œè«¸è—¥\", \"å¸¸è¦‹é…ä¼\": [\"å¤§æ£—\", \"ç”Ÿè–‘\"]}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8gE9_HRU8cq"
      },
      "outputs": [],
      "source": [
        "{\n",
        "  \"é»ƒèŠª\": {\"åŠŸæ•ˆ\": \"è£œæ°£å›ºè¡¨\", \"ç”¨æ³•\": \"ç…æ¹¯ 10-15 å…‹\"},\n",
        "  \"äººåƒ\": {\"åŠŸæ•ˆ\": \"å¤§è£œå…ƒæ°£\", \"ç”¨æ³•\": \"ç…æ¹¯ 3-9 å…‹\"}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZZSsEE6VG1j"
      },
      "outputs": [],
      "source": [
        "sudo apt-get install tesseract-ocr-chi-tra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruSM6ZgTVJz7"
      },
      "outputs": [],
      "source": [
        "pytesseract.image_to_string(page, lang=\"chi_tra\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8XwK6F1VOY6"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install tesseract-ocr-chi-tra\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq78lFCbVTEL"
      },
      "outputs": [],
      "source": [
        "text = pytesseract.image_to_string(page, lang=\"chi_tra\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db145b55"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import json\n",
        "\n",
        "# ======================\n",
        "# 1ï¸âƒ£ å»ºç«‹æœ¬åœ°è³‡æ–™åº«ï¼ˆå¤ä»Šæ•´åˆç¤ºç¯„ï¼‰\n",
        "# ======================\n",
        "herbs_data = [\n",
        "    {\"name\":\"äººåƒ\",\"ancient_name\":\"äººåƒ\",\"pinyin\":\"renshen\",\"effects\":[\"è£œæ°£\",\"å®‰ç¥\",\"æŠ—ç–²å‹\"],\"usage\":\"ç…æ¹¯æœç”¨ 3-10 å…‹\",\"source\":\"ç¥è¾²æœ¬è‰ç¶“\",\"image\":\"https://example.com/renshen.jpg\"},\n",
        "    {\"name\":\"é»ƒèŠª\",\"ancient_name\":\"é»ƒè€†\",\"pinyin\":\"huangqi\",\"effects\":[\"è£œæ°£å›ºè¡¨\",\"åˆ©å°¿æ¶ˆè…«\",\"ä¿ƒé€²å…ç–«\"],\"usage\":\"ç…æ¹¯æœç”¨ 10-15 å…‹\",\"source\":\"é¦™æ¸¯è—¥æè³‡æ–™åº«\",\"image\":\"https://example.com/huangqi.jpg\"},\n",
        "    {\"name\":\"æ¸æ\",\"ancient_name\":\"æ¸æå­\",\"pinyin\":\"gouqi\",\"effects\":[\"è£œè‚è…\",\"ç›Šç²¾æ˜ç›®\"],\"usage\":\"æ³¡èŒ¶æˆ–ç…æ¹¯ 6-12 å…‹\",\"source\":\"TianAPI\",\"image\":\"https://example.com/gouqi.jpg\"},\n",
        "    {\"name\":\"è–„è·\",\"ancient_name\":\"è–„è·\",\"pinyin\":\"bohe\",\"effects\":[\"ç–é¢¨æ¸…ç†±\",\"åˆ©å’½\",\"é ­ç—›\"],\"usage\":\"æ³¡èŒ¶æˆ–å…¥æ–¹ 3-6 å…‹\",\"source\":\"æœ¬è‰ç¶±ç›®\",\"image\":\"https://example.com/bohe.jpg\"}\n",
        "]\n",
        "\n",
        "DB_FILE = \"shennong_ai_full.json\"\n",
        "with open(DB_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(herbs_data, f, ensure_ascii=False, indent=2)\n",
        "print(f\"âœ… æœ¬åœ°è³‡æ–™åº«å·²å»ºç«‹ï¼š{DB_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beab8bc"
      },
      "source": [
        "### å°‡æª”æ¡ˆå„²å­˜åˆ° GitHub çš„æ­¥é©Ÿï¼š\n",
        "\n",
        "1.  **å»ºç«‹ GitHub å„²å­˜åº«ï¼š** åœ¨ GitHub ä¸Šå»ºç«‹ä¸€å€‹æ–°çš„å„²å­˜åº«ï¼ˆRepositoryï¼‰ï¼Œä¾‹å¦‚ `shennong_herbs_db`ã€‚\n",
        "2.  **å°‡æª”æ¡ˆä¸Šå‚³åˆ° GitHubï¼š** æ‚¨å¯ä»¥é€éä»¥ä¸‹å¹¾ç¨®æ–¹å¼å°‡æœ¬åœ°çš„ `shennong_ai_full.json` æª”æ¡ˆä¸Šå‚³åˆ°æ‚¨çš„ GitHub å„²å­˜åº«ï¼š\n",
        "    *   **ä½¿ç”¨ç¶²é ç•Œé¢ï¼š** ç›´æ¥åœ¨ GitHub å„²å­˜åº«é é¢é»é¸ \"Add file\" > \"Upload files\"ï¼Œç„¶å¾Œå°‡ `shennong_ai_full.json` æ‹–æ›³ä¸Šå»ã€‚\n",
        "    *   **ä½¿ç”¨ Git å‘½ä»¤åˆ—ï¼š** å¦‚æœæ‚¨ç†Ÿæ‚‰ Gitï¼Œå¯ä»¥åœ¨æœ¬åœ°å°‡ Colab æª”æ¡ˆä¸‹è¼‰ä¸‹ä¾†ï¼Œç„¶å¾Œä½¿ç”¨ Git å‘½ä»¤å°‡æª”æ¡ˆæ·»åŠ åˆ°æ‚¨çš„æœ¬åœ°å„²å­˜åº«ï¼Œæäº¤è®Šæ›´ï¼Œæœ€å¾Œæ¨é€åˆ° GitHubã€‚\n",
        "3.  **åœ¨ Colab ä¸­ä½¿ç”¨ GitHub ä¸Šçš„æª”æ¡ˆï¼š** åœ¨æ‚¨çš„ Colab Notebook ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¹‹ä¸€å¾ GitHub ä¸‹è¼‰æª”æ¡ˆï¼š\n",
        "    *   **ä½¿ç”¨ `!git clone`ï¼š** å¦‚æœæ‚¨å°‡æ•´å€‹å„²å­˜åº«å…‹éš†ä¸‹ä¾†ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fc9561b0"
      },
      "outputs": [],
      "source": [
        "        !wget <æª”æ¡ˆåœ¨ GitHub ä¸Šçš„ Raw æª”æ¡ˆ URL> -O shennong_ai_full.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aec3042d"
      },
      "source": [
        "!unzip /content/ilovepdf_pages-to-jpg.zip -d /content/extracted_images"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2VGoKchC9sMFl27yWGdMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}